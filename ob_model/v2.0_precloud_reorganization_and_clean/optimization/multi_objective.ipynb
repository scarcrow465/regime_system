{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96799c84-711a-42f6-94fc-2a6fb93491b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-objective optimization module\n",
    "Handles regime threshold optimization with multiple objectives\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import differential_evolution, minimize\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from config.settings import (\n",
    "    OBJECTIVE_WEIGHTS, PARAMETER_BOUNDS, OPTIMIZATION_METHOD,\n",
    "    COMMISSION_RATE, SLIPPAGE_RATE\n",
    ")\n",
    "from core.regime_classifier import RollingRegimeClassifier\n",
    "from backtesting.strategies import EnhancedRegimeStrategyBacktester\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class OptimizationResults:\n",
    "    \"\"\"Container for optimization results\"\"\"\n",
    "    best_params: Dict[str, float]\n",
    "    best_score: float\n",
    "    sharpe_ratio: float\n",
    "    max_drawdown: float\n",
    "    regime_persistence: float\n",
    "    total_return: float\n",
    "    win_rate: float\n",
    "    calmar_ratio: float\n",
    "    sortino_ratio: float\n",
    "    strategy_returns: pd.Series\n",
    "    regime_classifications: pd.DataFrame\n",
    "    optimization_history: List[Dict]\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for saving\"\"\"\n",
    "        return {\n",
    "            'best_params': self.best_params,\n",
    "            'best_score': self.best_score,\n",
    "            'sharpe_ratio': self.sharpe_ratio,\n",
    "            'max_drawdown': self.max_drawdown,\n",
    "            'regime_persistence': self.regime_persistence,\n",
    "            'total_return': self.total_return,\n",
    "            'win_rate': self.win_rate,\n",
    "            'calmar_ratio': self.calmar_ratio,\n",
    "            'sortino_ratio': self.sortino_ratio,\n",
    "            'optimization_iterations': len(self.optimization_history)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-OBJECTIVE OPTIMIZER\n",
    "# =============================================================================\n",
    "\n",
    "class MultiObjectiveRegimeOptimizer:\n",
    "    \"\"\"\n",
    "    Multi-objective optimization for regime classification thresholds\n",
    "    Optimizes for Sharpe ratio, drawdown, and regime persistence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regime_classifier: RollingRegimeClassifier,\n",
    "                 data: pd.DataFrame,\n",
    "                 objective_weights: Optional[Dict[str, float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize optimizer\n",
    "        \n",
    "        Args:\n",
    "            regime_classifier: Configured regime classifier\n",
    "            data: DataFrame with indicators\n",
    "            objective_weights: Weights for objectives (sharpe, drawdown, persistence)\n",
    "        \"\"\"\n",
    "        self.classifier = regime_classifier\n",
    "        self.data = data\n",
    "        self.backtester = EnhancedRegimeStrategyBacktester()\n",
    "        \n",
    "        # Objective weights\n",
    "        self.objective_weights = objective_weights or OBJECTIVE_WEIGHTS\n",
    "        \n",
    "        # Parameter bounds\n",
    "        self.param_bounds = PARAMETER_BOUNDS\n",
    "        \n",
    "        # Optimization history\n",
    "        self.optimization_history = []\n",
    "        self.iteration_count = 0\n",
    "        self.max_iterations = 100  # Will be updated\n",
    "        \n",
    "        logger.info(f\"Initialized optimizer with {len(self.param_bounds)} parameters\")\n",
    "    \n",
    "    def update_classifier_thresholds(self, params: Dict[str, float]):\n",
    "        \"\"\"Update classifier thresholds with new parameters\"\"\"\n",
    "        try:\n",
    "            # Update direction thresholds\n",
    "            self.classifier.dimension_thresholds['direction']['strong_trend_threshold'] = params['direction_strong_trend']\n",
    "            self.classifier.dimension_thresholds['direction']['weak_trend_threshold'] = params['direction_weak_trend']\n",
    "            \n",
    "            # Update trend strength thresholds\n",
    "            self.classifier.dimension_thresholds['trend_strength']['strong_alignment'] = params['trend_strong_alignment']\n",
    "            self.classifier.dimension_thresholds['trend_strength']['moderate_alignment'] = params['trend_moderate_alignment']\n",
    "            \n",
    "            # Update velocity thresholds\n",
    "            self.classifier.dimension_thresholds['velocity']['acceleration_threshold'] = params['velocity_acceleration']\n",
    "            self.classifier.dimension_thresholds['velocity']['stable_range'] = params['velocity_stable_range']\n",
    "            \n",
    "            # Update volatility thresholds\n",
    "            self.classifier.dimension_thresholds['volatility']['high_vol_percentile'] = params['volatility_high_percentile']\n",
    "            self.classifier.dimension_thresholds['volatility']['low_vol_percentile'] = params['volatility_low_percentile']\n",
    "            \n",
    "            # Update microstructure thresholds\n",
    "            self.classifier.dimension_thresholds['microstructure']['institutional_volume_threshold'] = params['microstructure_institutional']\n",
    "            self.classifier.dimension_thresholds['microstructure']['retail_volume_threshold'] = params['microstructure_retail']\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating classifier thresholds: {e}\")\n",
    "    \n",
    "    def calculate_regime_persistence(self, regimes: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate regime persistence score (higher is better)\"\"\"\n",
    "        try:\n",
    "            persistence_scores = []\n",
    "            \n",
    "            # Calculate persistence for each dimension\n",
    "            for dimension in ['Direction', 'TrendStrength', 'Velocity', 'Volatility', 'Microstructure']:\n",
    "                col = f'{dimension}_Regime'\n",
    "                if col in regimes.columns:\n",
    "                    # Count regime changes\n",
    "                    regime_changes = (regimes[col] != regimes[col].shift()).sum()\n",
    "                    total_periods = len(regimes[regimes[col] != 'Undefined'])\n",
    "                    \n",
    "                    if total_periods > 0:\n",
    "                        # Persistence = 1 - (changes / total_periods)\n",
    "                        persistence = max(0, 1 - (regime_changes / total_periods))\n",
    "                        persistence_scores.append(persistence)\n",
    "            \n",
    "            # Return average persistence across all dimensions\n",
    "            return np.mean(persistence_scores) if persistence_scores else 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating regime persistence: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_performance(self, params_array: np.ndarray) -> Tuple[Dict[str, float], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Evaluate performance for given parameters\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (performance_metrics, regime_classifications)\n",
    "        \"\"\"\n",
    "        # Convert array to parameter dictionary\n",
    "        param_names = list(self.param_bounds.keys())\n",
    "        params = dict(zip(param_names, params_array))\n",
    "        \n",
    "        # Update classifier with new parameters\n",
    "        self.update_classifier_thresholds(params)\n",
    "        \n",
    "        # Classify regimes with new parameters\n",
    "        regimes = self.classifier.classify_regimes(self.data, show_progress=False)\n",
    "        \n",
    "        # Run backtesting\n",
    "        strategy_returns = self.backtester.adaptive_regime_strategy_enhanced(self.data, regimes)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        performance = self.backtester.calculate_performance_metrics(strategy_returns)\n",
    "        \n",
    "        # Calculate regime persistence\n",
    "        persistence = self.calculate_regime_persistence(regimes)\n",
    "        \n",
    "        # Add persistence to metrics\n",
    "        performance['regime_persistence'] = persistence\n",
    "        \n",
    "        return performance, regimes\n",
    "    \n",
    "    def objective_function(self, params_array: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for optimization (to minimize)\n",
    "        \n",
    "        Args:\n",
    "            params_array: Array of parameter values\n",
    "            \n",
    "        Returns:\n",
    "            Combined objective score (lower is better)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.iteration_count += 1\n",
    "            \n",
    "            # Log progress periodically\n",
    "            if self.iteration_count % 5 == 0:\n",
    "                print(f\"Function call {self.iteration_count}/{self.max_iterations}\")\n",
    "            \n",
    "            # Evaluate performance\n",
    "            performance, regimes = self.evaluate_performance(params_array)\n",
    "            \n",
    "            # Extract metrics\n",
    "            sharpe_ratio = performance['sharpe_ratio']\n",
    "            max_drawdown = performance['max_drawdown']\n",
    "            persistence = performance['regime_persistence']\n",
    "            \n",
    "            # Multi-objective score (convert to minimization)\n",
    "            sharpe_component = -sharpe_ratio  # Negative because we want to maximize\n",
    "            drawdown_component = -max_drawdown  # Already negative, but we want less negative\n",
    "            persistence_component = -persistence  # Negative because we want to maximize\n",
    "            \n",
    "            # Weighted combination\n",
    "            total_score = (\n",
    "                self.objective_weights['sharpe_ratio'] * sharpe_component +\n",
    "                self.objective_weights['max_drawdown'] * drawdown_component +\n",
    "                self.objective_weights['regime_persistence'] * persistence_component\n",
    "            )\n",
    "            \n",
    "            # Store in history\n",
    "            param_dict = dict(zip(self.param_bounds.keys(), params_array))\n",
    "            self.optimization_history.append({\n",
    "                'iteration': self.iteration_count,\n",
    "                'params': param_dict.copy(),\n",
    "                'sharpe_ratio': sharpe_ratio,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'regime_persistence': persistence,\n",
    "                'total_score': -total_score,  # Convert back for reporting\n",
    "                'total_return': performance['total_return'],\n",
    "                'win_rate': performance['win_rate']\n",
    "            })\n",
    "            \n",
    "            # Log best so far\n",
    "            if self.iteration_count % 10 == 0:\n",
    "                best_score = min(h['total_score'] for h in self.optimization_history)\n",
    "                logger.info(f\"Iteration {self.iteration_count}: Current score={-total_score:.4f}, \"\n",
    "                          f\"Best={-best_score:.4f}, Sharpe={sharpe_ratio:.4f}\")\n",
    "            \n",
    "            return total_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in objective function: {e}\")\n",
    "            return 1000.0  # High penalty for errors\n",
    "    \n",
    "    def optimize_regime_thresholds(self, \n",
    "                                  method: str = OPTIMIZATION_METHOD,\n",
    "                                  max_iterations: int = 100) -> OptimizationResults:\n",
    "        \"\"\"\n",
    "        Run multi-objective optimization\n",
    "        \n",
    "        Args:\n",
    "            method: Optimization method ('differential_evolution' or 'L-BFGS-B')\n",
    "            max_iterations: Maximum iterations\n",
    "            \n",
    "        Returns:\n",
    "            OptimizationResults object\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting multi-objective regime optimization...\")\n",
    "        logger.info(f\"Method: {method}, Max iterations: {max_iterations}\")\n",
    "        logger.info(f\"Objectives: {list(self.objective_weights.keys())}\")\n",
    "        logger.info(f\"Weights: {list(self.objective_weights.values())}\")\n",
    "        \n",
    "        # Reset counters\n",
    "        self.iteration_count = 0\n",
    "        self.max_iterations = max_iterations\n",
    "        self.optimization_history = []\n",
    "        \n",
    "        try:\n",
    "            # Prepare bounds\n",
    "            bounds = list(self.param_bounds.values())\n",
    "            param_names = list(self.param_bounds.keys())\n",
    "            \n",
    "            # Initial guess (midpoint of bounds)\n",
    "            initial_guess = [(b[0] + b[1]) / 2 for b in bounds]\n",
    "            \n",
    "            logger.info(f\"Optimizing {len(bounds)} parameters...\")\n",
    "            \n",
    "            # Run optimization\n",
    "            if method == 'differential_evolution':\n",
    "                result = differential_evolution(\n",
    "                    self.objective_function,\n",
    "                    bounds,\n",
    "                    maxiter=max_iterations,\n",
    "                    popsize=5,  # Small population for speed\n",
    "                    seed=42,\n",
    "                    disp=True,\n",
    "                    workers=1,\n",
    "                    updating='immediate',\n",
    "                    strategy='best1bin',\n",
    "                    recombination=0.7,\n",
    "                    mutation=(0.5, 1.0)\n",
    "                )\n",
    "            else:\n",
    "                # L-BFGS-B\n",
    "                result = minimize(\n",
    "                    self.objective_function,\n",
    "                    initial_guess,\n",
    "                    method='L-BFGS-B',\n",
    "                    bounds=bounds,\n",
    "                    options={'maxiter': max_iterations, 'disp': True}\n",
    "                )\n",
    "            \n",
    "            # Extract best parameters\n",
    "            best_params = dict(zip(param_names, result.x))\n",
    "            \n",
    "            # Get final performance with best parameters\n",
    "            final_performance, final_regimes = self.evaluate_performance(result.x)\n",
    "            \n",
    "            logger.info(\"Optimization completed!\")\n",
    "            logger.info(f\"Best Score: {-result.fun:.4f}\")\n",
    "            logger.info(f\"Best Sharpe: {final_performance['sharpe_ratio']:.4f}\")\n",
    "            logger.info(f\"Best Drawdown: {final_performance['max_drawdown']:.4f}\")\n",
    "            logger.info(f\"Best Persistence: {final_performance['regime_persistence']:.4f}\")\n",
    "            \n",
    "            # Create results object\n",
    "            results = OptimizationResults(\n",
    "                best_params=best_params,\n",
    "                best_score=-result.fun,\n",
    "                sharpe_ratio=final_performance['sharpe_ratio'],\n",
    "                max_drawdown=final_performance['max_drawdown'],\n",
    "                regime_persistence=final_performance['regime_persistence'],\n",
    "                total_return=final_performance['total_return'],\n",
    "                win_rate=final_performance['win_rate'],\n",
    "                calmar_ratio=final_performance['calmar_ratio'],\n",
    "                sortino_ratio=final_performance['sortino_ratio'],\n",
    "                strategy_returns=final_performance['returns'],\n",
    "                regime_classifications=final_regimes,\n",
    "                optimization_history=self.optimization_history\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Optimization failed: {e}\")\n",
    "            # Return default results\n",
    "            return self._create_default_results()\n",
    "    \n",
    "    def _create_default_results(self) -> OptimizationResults:\n",
    "        \"\"\"Create default results for failed optimization\"\"\"\n",
    "        return OptimizationResults(\n",
    "            best_params={},\n",
    "            best_score=0.0,\n",
    "            sharpe_ratio=0.0,\n",
    "            max_drawdown=-1.0,\n",
    "            regime_persistence=0.0,\n",
    "            total_return=-1.0,\n",
    "            win_rate=0.0,\n",
    "            calmar_ratio=0.0,\n",
    "            sortino_ratio=0.0,\n",
    "            strategy_returns=pd.Series(dtype=float),\n",
    "            regime_classifications=pd.DataFrame(),\n",
    "            optimization_history=self.optimization_history\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZATION UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def print_optimization_results(results: OptimizationResults):\n",
    "    \"\"\"Pretty print optimization results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Best Score: {results.best_score:.4f}\")\n",
    "    print(f\"  Sharpe Ratio: {results.sharpe_ratio:.4f}\")\n",
    "    print(f\"  Max Drawdown: {results.max_drawdown:.2%}\")\n",
    "    print(f\"  Total Return: {results.total_return:.2%}\")\n",
    "    print(f\"  Win Rate: {results.win_rate:.2%}\")\n",
    "    print(f\"  Calmar Ratio: {results.calmar_ratio:.4f}\")\n",
    "    print(f\"  Sortino Ratio: {results.sortino_ratio:.4f}\")\n",
    "    print(f\"  Regime Persistence: {results.regime_persistence:.4f}\")\n",
    "    \n",
    "    print(\"\\nOptimized Parameters:\")\n",
    "    for param, value in results.best_params.items():\n",
    "        print(f\"  {param}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nOptimization completed in {len(results.optimization_history)} iterations\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def compare_optimizations(results_list: List[OptimizationResults], \n",
    "                         labels: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Compare multiple optimization results\"\"\"\n",
    "    if labels is None:\n",
    "        labels = [f\"Opt_{i+1}\" for i in range(len(results_list))]\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for i, (result, label) in enumerate(zip(results_list, labels)):\n",
    "        comparison_data.append({\n",
    "            'Label': label,\n",
    "            'Score': result.best_score,\n",
    "            'Sharpe': result.sharpe_ratio,\n",
    "            'Drawdown': result.max_drawdown,\n",
    "            'Return': result.total_return,\n",
    "            'Win_Rate': result.win_rate,\n",
    "            'Calmar': result.calmar_ratio,\n",
    "            'Sortino': result.sortino_ratio,\n",
    "            'Persistence': result.regime_persistence\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.set_index('Label')\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "def save_optimization_history(history: List[Dict], filepath: str):\n",
    "    \"\"\"Save optimization history to CSV\"\"\"\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(filepath, index=False)\n",
    "    logger.info(f\"Optimization history saved to: {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
