{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performance attribution module\n",
    "Analyzes which regimes and factors contribute to strategy performance\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from config.settings import (\n",
    "    REGIME_DIMENSIONS, RESULTS_DIR, \n",
    "    FIGURE_SIZE, FIGURE_DPI, PLOT_STYLE\n",
    ")\n",
    "from utils.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use(PLOT_STYLE)\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE ATTRIBUTION ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "class PerformanceAttributionAnalyzer:\n",
    "    \"\"\"Analyze performance attribution across regimes and factors\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.attribution_results = {}\n",
    "        self.regime_performance = {}\n",
    "        \n",
    "    def analyze_regime_performance(self, \n",
    "                                 returns: pd.Series,\n",
    "                                 regimes: pd.DataFrame,\n",
    "                                 positions: Optional[pd.Series] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze performance by regime\n",
    "        \n",
    "        Args:\n",
    "            returns: Strategy returns\n",
    "            regimes: Regime classifications\n",
    "            positions: Optional position sizes\n",
    "            \n",
    "        Returns:\n",
    "            Performance attribution by regime\n",
    "        \"\"\"\n",
    "        logger.info(\"Analyzing performance attribution by regime\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for dimension in REGIME_DIMENSIONS:\n",
    "            regime_col = f'{dimension}_Regime'\n",
    "            \n",
    "            if regime_col not in regimes.columns:\n",
    "                continue\n",
    "            \n",
    "            # Calculate performance by regime\n",
    "            regime_perf = self._calculate_regime_performance(\n",
    "                returns, regimes[regime_col], positions\n",
    "            )\n",
    "            \n",
    "            # Calculate contribution to total return\n",
    "            contributions = self._calculate_regime_contributions(\n",
    "                returns, regimes[regime_col]\n",
    "            )\n",
    "            \n",
    "            # Analyze risk by regime\n",
    "            risk_metrics = self._calculate_regime_risk(\n",
    "                returns, regimes[regime_col]\n",
    "            )\n",
    "            \n",
    "            results[dimension] = {\n",
    "                'performance': regime_perf,\n",
    "                'contributions': contributions,\n",
    "                'risk_metrics': risk_metrics,\n",
    "                'best_regime': max(regime_perf.items(), key=lambda x: x[1]['sharpe_ratio'])[0]\n",
    "                              if regime_perf else None,\n",
    "                'worst_regime': min(regime_perf.items(), key=lambda x: x[1]['sharpe_ratio'])[0]\n",
    "                               if regime_perf else None\n",
    "            }\n",
    "            \n",
    "            self.regime_performance[dimension] = results[dimension]\n",
    "        \n",
    "        # Overall attribution summary\n",
    "        results['overall'] = self._calculate_overall_attribution(results, returns)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_regime_performance(self, \n",
    "                                    returns: pd.Series,\n",
    "                                    regime_series: pd.Series,\n",
    "                                    positions: Optional[pd.Series]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Calculate performance metrics for each regime\"\"\"\n",
    "        regime_metrics = {}\n",
    "        \n",
    "        for regime in regime_series.unique():\n",
    "            if regime == 'Undefined':\n",
    "                continue\n",
    "            \n",
    "            # Get returns for this regime\n",
    "            regime_mask = regime_series == regime\n",
    "            regime_returns = returns[regime_mask]\n",
    "            \n",
    "            if len(regime_returns) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'total_return': (1 + regime_returns).prod() - 1,\n",
    "                'avg_return': regime_returns.mean(),\n",
    "                'volatility': regime_returns.std(),\n",
    "                'sharpe_ratio': regime_returns.mean() / regime_returns.std() * np.sqrt(252 * 26)\n",
    "                               if regime_returns.std() > 0 else 0,\n",
    "                'max_drawdown': self._calculate_max_drawdown(regime_returns),\n",
    "                'win_rate': (regime_returns > 0).sum() / len(regime_returns)\n",
    "                           if len(regime_returns) > 0 else 0,\n",
    "                'num_periods': len(regime_returns),\n",
    "                'pct_time': len(regime_returns) / len(returns) * 100\n",
    "            }\n",
    "            \n",
    "            # Add position analysis if available\n",
    "            if positions is not None:\n",
    "                regime_positions = positions[regime_mask]\n",
    "                metrics['avg_position'] = regime_positions.mean()\n",
    "                metrics['position_utilization'] = (regime_positions != 0).sum() / len(regime_positions)\n",
    "                                                  if len(regime_positions) > 0 else 0\n",
    "            \n",
    "            regime_metrics[regime] = metrics\n",
    "        \n",
    "        return regime_metrics\n",
    "    \n",
    "    def _calculate_max_drawdown(self, returns: pd.Series) -> float:\n",
    "        \"\"\"Calculate maximum drawdown\"\"\"\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        return drawdown.min()\n",
    "    \n",
    "    def _calculate_regime_contributions(self,\n",
    "                                      returns: pd.Series,\n",
    "                                      regime_series: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"Calculate contribution of each regime to total return\"\"\"\n",
    "        total_return = (1 + returns).prod() - 1\n",
    "        \n",
    "        contributions = {}\n",
    "        \n",
    "        for regime in regime_series.unique():\n",
    "            if regime == 'Undefined':\n",
    "                continue\n",
    "            \n",
    "            regime_mask = regime_series == regime\n",
    "            regime_returns = returns[regime_mask]\n",
    "            \n",
    "            # Calculate cumulative return contribution\n",
    "            regime_cum_return = (1 + regime_returns).prod() - 1\n",
    "            \n",
    "            # Calculate percentage contribution\n",
    "            if total_return != 0:\n",
    "                contribution_pct = (regime_cum_return / total_return) * 100\n",
    "            else:\n",
    "                contribution_pct = 0\n",
    "            \n",
    "            contributions[regime] = {\n",
    "                'return_contribution': regime_cum_return,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'periods': len(regime_returns)\n",
    "            }\n",
    "        \n",
    "        return contributions\n",
    "    \n",
    "    def _calculate_regime_risk(self,\n",
    "                             returns: pd.Series,\n",
    "                             regime_series: pd.Series) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Calculate risk metrics by regime\"\"\"\n",
    "        risk_metrics = {}\n",
    "        \n",
    "        for regime in regime_series.unique():\n",
    "            if regime == 'Undefined':\n",
    "                continue\n",
    "            \n",
    "            regime_mask = regime_series == regime\n",
    "            regime_returns = returns[regime_mask]\n",
    "            \n",
    "            if len(regime_returns) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Calculate risk metrics\n",
    "            metrics = {\n",
    "                'volatility': regime_returns.std() * np.sqrt(252 * 26),\n",
    "                'downside_volatility': regime_returns[regime_returns < 0].std() * np.sqrt(252 * 26)\n",
    "                                      if len(regime_returns[regime_returns < 0]) > 0 else 0,\n",
    "                'var_95': np.percentile(regime_returns, 5),\n",
    "                'cvar_95': regime_returns[regime_returns <= np.percentile(regime_returns, 5)].mean()\n",
    "                          if len(regime_returns[regime_returns <= np.percentile(regime_returns, 5)]) > 0 else 0,\n",
    "                'skewness': stats.skew(regime_returns),\n",
    "                'kurtosis': stats.kurtosis(regime_returns)\n",
    "            }\n",
    "            \n",
    "            risk_metrics[regime] = metrics\n",
    "        \n",
    "        return risk_metrics\n",
    "    \n",
    "    def _calculate_overall_attribution(self, \n",
    "                                     dimension_results: Dict[str, Any],\n",
    "                                     returns: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate overall attribution summary\"\"\"\n",
    "        # Find most impactful dimension\n",
    "        dimension_impacts = {}\n",
    "        \n",
    "        for dimension, results in dimension_results.items():\n",
    "            if dimension == 'overall' or 'performance' not in results:\n",
    "                continue\n",
    "            \n",
    "            # Calculate dispersion of performance across regimes\n",
    "            performances = [r['sharpe_ratio'] for r in results['performance'].values()]\n",
    "            if performances:\n",
    "                dispersion = np.std(performances)\n",
    "                dimension_impacts[dimension] = dispersion\n",
    "        \n",
    "        most_impactful = max(dimension_impacts.items(), key=lambda x: x[1])[0] if dimension_impacts else None\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_return = (1 + returns).prod() - 1\n",
    "        sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252 * 26) if returns.std() > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'most_impactful_dimension': most_impactful,\n",
    "            'dimension_impacts': dimension_impacts,\n",
    "            'recommendation': self._generate_attribution_recommendations(dimension_results)\n",
    "        }\n",
    "    \n",
    "    def _generate_attribution_recommendations(self, \n",
    "                                            dimension_results: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on attribution analysis\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Find dimensions with poor performing regimes\n",
    "        for dimension, results in dimension_results.items():\n",
    "            if dimension == 'overall' or 'performance' not in results:\n",
    "                continue\n",
    "            \n",
    "            # Check for consistently poor regimes\n",
    "            poor_regimes = [regime for regime, metrics in results['performance'].items()\n",
    "                           if metrics['sharpe_ratio'] < -0.5]\n",
    "            \n",
    "            if poor_regimes:\n",
    "                recommendations.append(\n",
    "                    f\"Consider avoiding or reducing exposure during \"\n",
    "                    f\"{dimension} regimes: {', '.join(poor_regimes)}\"\n",
    "                )\n",
    "            \n",
    "            # Check for unutilized regimes\n",
    "            low_utilization = [regime for regime, metrics in results['performance'].items()\n",
    "                              if metrics.get('position_utilization', 1) < 0.2]\n",
    "            \n",
    "            if low_utilization:\n",
    "                recommendations.append(\n",
    "                    f\"Low position utilization in {dimension} regimes: \"\n",
    "                    f\"{', '.join(low_utilization)}. Consider improving entry signals.\"\n",
    "                )\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# =============================================================================\n",
    "# FACTOR ATTRIBUTION\n",
    "# =============================================================================\n",
    "\n",
    "class FactorAttributionAnalyzer:\n",
    "    \"\"\"Analyze performance attribution by factors\"\"\"\n",
    "    \n",
    "    def analyze_factor_attribution(self,\n",
    "                                 returns: pd.Series,\n",
    "                                 data: pd.DataFrame,\n",
    "                                 regimes: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Decompose returns by various factors\n",
    "        \n",
    "        Args:\n",
    "            returns: Strategy returns\n",
    "            data: DataFrame with indicators\n",
    "            regimes: Regime classifications\n",
    "            \n",
    "        Returns:\n",
    "            Factor attribution analysis\n",
    "        \"\"\"\n",
    "        logger.info(\"Analyzing factor attribution\")\n",
    "        \n",
    "        results = {\n",
    "            'timing': self._analyze_timing_attribution(returns, regimes),\n",
    "            'selection': self._analyze_selection_attribution(returns, regimes),\n",
    "            'volatility': self._analyze_volatility_attribution(returns, data),\n",
    "            'indicator': self._analyze_indicator_attribution(returns, data, regimes)\n",
    "        }\n",
    "        \n",
    "        # Calculate total attribution\n",
    "        results['total'] = self._calculate_total_attribution(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _analyze_timing_attribution(self,\n",
    "                                  returns: pd.Series,\n",
    "                                  regimes: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Analyze timing contribution to returns\"\"\"\n",
    "        # Calculate regime timing effectiveness\n",
    "        timing_scores = {}\n",
    "        \n",
    "        for dimension in REGIME_DIMENSIONS:\n",
    "            regime_col = f'{dimension}_Regime'\n",
    "            if regime_col not in regimes.columns:\n",
    "                continue\n",
    "            \n",
    "            # Calculate how well we time regime transitions\n",
    "            regime_changes = regimes[regime_col] != regimes[regime_col].shift()\n",
    "            \n",
    "            # Returns around regime changes\n",
    "            pre_change_returns = returns.shift(1)[regime_changes].mean()\n",
    "            post_change_returns = returns[regime_changes].mean()\n",
    "            \n",
    "            timing_scores[dimension] = {\n",
    "                'pre_transition_return': pre_change_returns,\n",
    "                'post_transition_return': post_change_returns,\n",
    "                'timing_effectiveness': post_change_returns - pre_change_returns\n",
    "            }\n",
    "        \n",
    "        return timing_scores\n",
    "    \n",
    "    def _analyze_selection_attribution(self,\n",
    "                                     returns: pd.Series,\n",
    "                                     regimes: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Analyze selection contribution (regime identification)\"\"\"\n",
    "        selection_scores = {}\n",
    "        \n",
    "        # Calculate returns in correctly identified vs misidentified regimes\n",
    "        # This would require known \"true\" regimes, so we use confidence as proxy\n",
    "        \n",
    "        for dimension in REGIME_DIMENSIONS:\n",
    "            confidence_col = f'{dimension}_Confidence'\n",
    "            if confidence_col not in regimes.columns:\n",
    "                continue\n",
    "            \n",
    "            # High vs low confidence returns\n",
    "            high_conf_mask = regimes[confidence_col] > regimes[confidence_col].median()\n",
    "            \n",
    "            high_conf_returns = returns[high_conf_mask].mean()\n",
    "            low_conf_returns = returns[~high_conf_mask].mean()\n",
    "            \n",
    "            selection_scores[dimension] = {\n",
    "                'high_confidence_return': high_conf_returns,\n",
    "                'low_confidence_return': low_conf_returns,\n",
    "                'selection_alpha': high_conf_returns - low_conf_returns\n",
    "            }\n",
    "        \n",
    "        return selection_scores\n",
    "    \n",
    "    def _analyze_volatility_attribution(self,\n",
    "                                      returns: pd.Series,\n",
    "                                      data: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Analyze volatility timing contribution\"\"\"\n",
    "        if 'ATR' not in data.columns:\n",
    "            return {}\n",
    "        \n",
    "        # Quartile analysis of returns by volatility\n",
    "        volatility_quartiles = pd.qcut(data['ATR'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "        \n",
    "        vol_attribution = {}\n",
    "        for quartile in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "            mask = volatility_quartiles == quartile\n",
    "            if mask.sum() > 0:\n",
    "                vol_attribution[quartile] = {\n",
    "                    'avg_return': returns[mask].mean(),\n",
    "                    'sharpe': returns[mask].mean() / returns[mask].std() * np.sqrt(252 * 26)\n",
    "                             if returns[mask].std() > 0 else 0,\n",
    "                    'periods': mask.sum()\n",
    "                }\n",
    "        \n",
    "        return vol_attribution\n",
    "    \n",
    "    def _analyze_indicator_attribution(self,\n",
    "                                     returns: pd.Series,\n",
    "                                     data: pd.DataFrame,\n",
    "                                     regimes: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Analyze which indicators contribute most to returns\"\"\"\n",
    "        indicator_scores = {}\n",
    "        \n",
    "        # Test key indicators\n",
    "        test_indicators = ['RSI', 'MACD_Signal', 'ADX', 'ATR', 'MFI']\n",
    "        \n",
    "        for indicator in test_indicators:\n",
    "            if indicator not in data.columns:\n",
    "                continue\n",
    "            \n",
    "            # Simple analysis: returns when indicator is extreme\n",
    "            indicator_values = data[indicator]\n",
    "            \n",
    "            # Define extreme thresholds\n",
    "            high_threshold = indicator_values.quantile(0.8)\n",
    "            low_threshold = indicator_values.quantile(0.2)\n",
    "            \n",
    "            high_returns = returns[indicator_values > high_threshold].mean()\n",
    "            low_returns = returns[indicator_values < low_threshold].mean()\n",
    "            neutral_returns = returns[(indicator_values >= low_threshold) & \n",
    "                                    (indicator_values <= high_threshold)].mean()\n",
    "            \n",
    "            indicator_scores[indicator] = {\n",
    "                'high_value_return': high_returns,\n",
    "                'low_value_return': low_returns,\n",
    "                'neutral_return': neutral_returns,\n",
    "                'indicator_edge': max(abs(high_returns - neutral_returns),\n",
    "                                     abs(low_returns - neutral_returns))\n",
    "            }\n",
    "        \n",
    "        return indicator_scores\n",
    "    \n",
    "    def _calculate_total_attribution(self, \n",
    "                                   attribution_results: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate total attribution summary\"\"\"\n",
    "        # Aggregate attribution scores\n",
    "        total_timing_effect = np.mean([\n",
    "            score['timing_effectiveness'] \n",
    "            for score in attribution_results['timing'].values()\n",
    "            if 'timing_effectiveness' in score\n",
    "        ])\n",
    "        \n",
    "        total_selection_effect = np.mean([\n",
    "            score['selection_alpha']\n",
    "            for score in attribution_results['selection'].values()\n",
    "            if 'selection_alpha' in score\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'timing_contribution': total_timing_effect,\n",
    "            'selection_contribution': total_selection_effect,\n",
    "            'total_alpha': total_timing_effect + total_selection_effect\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_regime_performance(regime_performance: Dict[str, Any],\n",
    "                          save_path: Optional[str] = None):\n",
    "    \"\"\"Plot performance by regime\"\"\"\n",
    "    n_dims = len([d for d in regime_performance.keys() if d != 'overall'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (dimension, results) in enumerate(regime_performance.items()):\n",
    "        if dimension == 'overall' or i >= len(axes):\n",
    "            continue\n",
    "        \n",
    "        ax = axes[i]\n",
    "        \n",
    "        if 'performance' not in results or not results['performance']:\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "            ax.set_title(f'{dimension} - No data')\n",
    "            continue\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        regimes = list(results['performance'].keys())\n",
    "        sharpe_ratios = [results['performance'][r]['sharpe_ratio'] for r in regimes]\n",
    "        returns = [results['performance'][r]['total_return'] * 100 for r in regimes]\n",
    "        \n",
    "        # Create bar plot\n",
    "        x = np.arange(len(regimes))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, sharpe_ratios, width, label='Sharpe Ratio', alpha=0.8)\n",
    "        \n",
    "        # Use secondary y-axis for returns\n",
    "        ax2 = ax.twinx()\n",
    "        bars2 = ax2.bar(x + width/2, returns, width, label='Total Return (%)', \n",
    "                       alpha=0.8, color='orange')\n",
    "        \n",
    "        # Color bars based on performance\n",
    "        for bar, sharpe in zip(bars1, sharpe_ratios):\n",
    "            if sharpe < 0:\n",
    "                bar.set_color('red')\n",
    "            elif sharpe > 1:\n",
    "                bar.set_color('green')\n",
    "            else:\n",
    "                bar.set_color('steelblue')\n",
    "        \n",
    "        ax.set_xlabel('Regime')\n",
    "        ax.set_ylabel('Sharpe Ratio', color='steelblue')\n",
    "        ax2.set_ylabel('Total Return (%)', color='orange')\n",
    "        ax.set_title(f'{dimension} Regime Performance')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(regimes, rotation=45, ha='right')\n",
    "        \n",
    "        # Add legend\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_dims, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Performance Attribution by Regime', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        logger.info(f\"Performance attribution plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def plot_contribution_breakdown(contributions: Dict[str, Dict[str, Any]],\n",
    "                              save_path: Optional[str] = None):\n",
    "    \"\"\"Plot contribution breakdown by regime\"\"\"\n",
    "    # Aggregate contributions across dimensions\n",
    "    all_contributions = {}\n",
    "    \n",
    "    for dimension, dim_contributions in contributions.items():\n",
    "        if dimension == 'overall' or 'contributions' not in dim_contributions:\n",
    "            continue\n",
    "        \n",
    "        for regime, contrib in dim_contributions['contributions'].items():\n",
    "            key = f\"{dimension}_{regime}\"\n",
    "            all_contributions[key] = contrib['return_contribution']\n",
    "    \n",
    "    # Sort by absolute contribution\n",
    "    sorted_contrib = sorted(all_contributions.items(), \n",
    "                          key=lambda x: abs(x[1]), \n",
    "                          reverse=True)[:15]  # Top 15\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    labels = [k for k, _ in sorted_contrib]\n",
    "    values = [v * 100 for _, v in sorted_contrib]  # Convert to percentage\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = ax.barh(labels, values)\n",
    "    \n",
    "    # Color positive/negative differently\n",
    "    for bar, val in zip(bars, values):\n",
    "        if val < 0:\n",
    "            bar.set_color('red')\n",
    "        else:\n",
    "            bar.set_color('green')\n",
    "    \n",
    "    ax.set_xlabel('Return Contribution (%)')\n",
    "    ax.set_title('Top 15 Regime Contributions to Total Return')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (label, value) in enumerate(zip(labels, values)):\n",
    "        ax.text(value + (0.1 if value > 0 else -0.1), i, f'{value:.1f}%', \n",
    "               ha='left' if value > 0 else 'right', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        logger.info(f\"Contribution breakdown plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ATTRIBUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_performance_attribution(returns: pd.Series,\n",
    "                              regimes: pd.DataFrame,\n",
    "                              data: pd.DataFrame,\n",
    "                              positions: Optional[pd.Series] = None,\n",
    "                              save_report: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete performance attribution analysis\n",
    "    \n",
    "    Args:\n",
    "        returns: Strategy returns\n",
    "        regimes: Regime classifications\n",
    "        data: DataFrame with indicators\n",
    "        positions: Optional position sizes\n",
    "        save_report: Whether to save attribution report\n",
    "        \n",
    "    Returns:\n",
    "        Complete attribution analysis\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting performance attribution analysis\")\n",
    "    \n",
    "    # Initialize analyzers\n",
    "    perf_analyzer = PerformanceAttributionAnalyzer()\n",
    "    factor_analyzer = FactorAttributionAnalyzer()\n",
    "    \n",
    "    # Run analyses\n",
    "    results = {\n",
    "        'regime_attribution': perf_analyzer.analyze_regime_performance(\n",
    "            returns, regimes, positions\n",
    "        ),\n",
    "        'factor_attribution': factor_analyzer.analyze_factor_attribution(\n",
    "            returns, data, regimes\n",
    "        ),\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    # Create summary\n",
    "    overall_attribution = results['regime_attribution'].get('overall', {})\n",
    "    \n",
    "    results['summary'] = {\n",
    "        'total_return': overall_attribution.get('total_return', 0),\n",
    "        'sharpe_ratio': overall_attribution.get('sharpe_ratio', 0),\n",
    "        'most_impactful_dimension': overall_attribution.get('most_impactful_dimension'),\n",
    "        'recommendations': overall_attribution.get('recommendation', [])\n",
    "    }\n",
    "    \n",
    "    # Save report if requested\n",
    "    if save_report:\n",
    "        save_attribution_report(results, perf_analyzer.regime_performance)\n",
    "    \n",
    "    logger.info(\"Performance attribution analysis complete\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_attribution_report(results: Dict[str, Any],\n",
    "                          regime_performance: Dict[str, Any]):\n",
    "    \"\"\"Save performance attribution report\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create attribution directory\n",
    "    attribution_dir = os.path.join(RESULTS_DIR, 'performance_attribution')\n",
    "    os.makedirs(attribution_dir, exist_ok=True)\n",
    "    \n",
    "    # Save plots\n",
    "    plot_regime_performance(\n",
    "        regime_performance,\n",
    "        save_path=os.path.join(attribution_dir, f'regime_performance_{timestamp}.png')\n",
    "    )\n",
    "    \n",
    "    plot_contribution_breakdown(\n",
    "        regime_performance,\n",
    "        save_path=os.path.join(attribution_dir, f'contribution_breakdown_{timestamp}.png')\n",
    "    )\n",
    "    \n",
    "    # Save text report\n",
    "    report_path = os.path.join(attribution_dir, f'attribution_report_{timestamp}.txt')\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"PERFORMANCE ATTRIBUTION REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        summary = results['summary']\n",
    "        f.write(f\"Total Return: {summary['total_return']:.2%}\\n\")\n",
    "        f.write(f\"Sharpe Ratio: {summary['sharpe_ratio']:.4f}\\n\")\n",
    "        f.write(f\"Most Impactful Dimension: {summary['most_impactful_dimension']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"1. REGIME ATTRIBUTION\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        \n",
    "        for dimension in REGIME_DIMENSIONS:\n",
    "            if dimension in results['regime_attribution']:\n",
    "                dim_results = results['regime_attribution'][dimension]\n",
    "                f.write(f\"\\n{dimension} Dimension:\\n\")\n",
    "                f.write(f\"  Best Regime: {dim_results.get('best_regime', 'N/A')}\\n\")\n",
    "                f.write(f\"  Worst Regime: {dim_results.get('worst_regime', 'N/A')}\\n\")\n",
    "                \n",
    "                if 'performance' in dim_results:\n",
    "                    f.write(\"\\n  Regime Performance:\\n\")\n",
    "                    for regime, metrics in dim_results['performance'].items():\n",
    "                        f.write(f\"    {regime}:\\n\")\n",
    "                        f.write(f\"      Sharpe: {metrics['sharpe_ratio']:.3f}\\n\")\n",
    "                        f.write(f\"      Return: {metrics['total_return']:.2%}\\n\")\n",
    "                        f.write(f\"      Time %: {metrics['pct_time']:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\n2. FACTOR ATTRIBUTION\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        \n",
    "        factor_attr = results.get('factor_attribution', {})\n",
    "        \n",
    "        if 'total' in factor_attr:\n",
    "            f.write(f\"\\nTotal Attribution:\\n\")\n",
    "            f.write(f\"  Timing Contribution: {factor_attr['total']['timing_contribution']:.4f}\\n\")\n",
    "            f.write(f\"  Selection Contribution: {factor_attr['total']['selection_contribution']:.4f}\\n\")\n",
    "            f.write(f\"  Total Alpha: {factor_attr['total']['total_alpha']:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n3. RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for i, rec in enumerate(summary.get('recommendations', []), 1):\n",
    "            f.write(f\"{i}. {rec}\\n\")\n",
    "    \n",
    "    logger.info(f\"Attribution report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
