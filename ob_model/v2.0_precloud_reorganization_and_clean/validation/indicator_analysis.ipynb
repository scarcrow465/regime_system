{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d41f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Indicator analysis and validation module\n",
    "Analyzes indicator correlations, contributions, and redundancies\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import os\n",
    "import sys\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from config.settings import (\n",
    "    INDICATOR_WEIGHTS, REGIME_DIMENSIONS, RESULTS_DIR,\n",
    "    FIGURE_SIZE, FIGURE_DPI\n",
    ")\n",
    "from utils.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# INDICATOR CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "class IndicatorCorrelationAnalyzer:\n",
    "    \"\"\"Analyze correlations between indicators\"\"\"\n",
    "    \n",
    "    def __init__(self, correlation_threshold: float = 0.90):\n",
    "        \"\"\"\n",
    "        Initialize analyzer\n",
    "        \n",
    "        Args:\n",
    "            correlation_threshold: Threshold for identifying redundant indicators\n",
    "        \"\"\"\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.correlation_matrix = None\n",
    "        self.redundant_pairs = []\n",
    "        \n",
    "    def analyze_correlations(self, data: pd.DataFrame, \n",
    "                           indicators: Optional[List[str]] = None,\n",
    "                           method: str = 'pearson') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate correlation matrix for indicators\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with indicators\n",
    "            indicators: List of indicator columns to analyze\n",
    "            method: Correlation method ('pearson' or 'spearman')\n",
    "            \n",
    "        Returns:\n",
    "            Correlation matrix\n",
    "        \"\"\"\n",
    "        if indicators is None:\n",
    "            # Get all indicator columns (exclude basic price/volume)\n",
    "            exclude = ['open', 'high', 'low', 'close', 'volume', 'returns', \n",
    "                      'log_returns', 'date', 'timestamp']\n",
    "            indicators = [col for col in data.columns if col not in exclude]\n",
    "        \n",
    "        # Filter to existing columns\n",
    "        indicators = [col for col in indicators if col in data.columns]\n",
    "        \n",
    "        logger.info(f\"Analyzing correlations for {len(indicators)} indicators using {method} method\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        if method == 'pearson':\n",
    "            self.correlation_matrix = data[indicators].corr(method='pearson')\n",
    "        elif method == 'spearman':\n",
    "            self.correlation_matrix = data[indicators].corr(method='spearman')\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown correlation method: {method}\")\n",
    "        \n",
    "        # Find redundant pairs\n",
    "        self._find_redundant_pairs()\n",
    "        \n",
    "        return self.correlation_matrix\n",
    "    \n",
    "    def _find_redundant_pairs(self):\n",
    "        \"\"\"Find highly correlated indicator pairs\"\"\"\n",
    "        self.redundant_pairs = []\n",
    "        \n",
    "        if self.correlation_matrix is None:\n",
    "            return\n",
    "        \n",
    "        # Get upper triangle of correlation matrix\n",
    "        upper_triangle = np.triu(self.correlation_matrix.values, k=1)\n",
    "        \n",
    "        # Find pairs with high correlation\n",
    "        for i in range(len(upper_triangle)):\n",
    "            for j in range(i+1, len(upper_triangle)):\n",
    "                corr_value = upper_triangle[i, j]\n",
    "                \n",
    "                if abs(corr_value) >= self.correlation_threshold:\n",
    "                    self.redundant_pairs.append({\n",
    "                        'indicator1': self.correlation_matrix.index[i],\n",
    "                        'indicator2': self.correlation_matrix.columns[j],\n",
    "                        'correlation': corr_value\n",
    "                    })\n",
    "        \n",
    "        logger.info(f\"Found {len(self.redundant_pairs)} redundant indicator pairs\")\n",
    "    \n",
    "    def get_redundancy_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate redundancy analysis report\"\"\"\n",
    "        if not self.redundant_pairs:\n",
    "            return {'redundant_count': 0, 'pairs': []}\n",
    "        \n",
    "        # Sort by absolute correlation\n",
    "        sorted_pairs = sorted(self.redundant_pairs, \n",
    "                            key=lambda x: abs(x['correlation']), \n",
    "                            reverse=True)\n",
    "        \n",
    "        # Group by indicator\n",
    "        indicator_redundancy = {}\n",
    "        for pair in sorted_pairs:\n",
    "            for indicator in [pair['indicator1'], pair['indicator2']]:\n",
    "                if indicator not in indicator_redundancy:\n",
    "                    indicator_redundancy[indicator] = []\n",
    "                other = pair['indicator2'] if indicator == pair['indicator1'] else pair['indicator1']\n",
    "                indicator_redundancy[indicator].append(other)\n",
    "        \n",
    "        return {\n",
    "            'redundant_count': len(self.redundant_pairs),\n",
    "            'pairs': sorted_pairs[:10],  # Top 10 most correlated\n",
    "            'indicators_with_redundancy': indicator_redundancy,\n",
    "            'recommendation': self._generate_redundancy_recommendations()\n",
    "        }\n",
    "    \n",
    "    def _generate_redundancy_recommendations(self) -> List[str]:\n",
    "        \"\"\"Generate recommendations for handling redundant indicators\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Count redundancies per indicator\n",
    "        redundancy_count = {}\n",
    "        for pair in self.redundant_pairs:\n",
    "            for indicator in [pair['indicator1'], pair['indicator2']]:\n",
    "                redundancy_count[indicator] = redundancy_count.get(indicator, 0) + 1\n",
    "        \n",
    "        # Sort by redundancy count\n",
    "        most_redundant = sorted(redundancy_count.items(), \n",
    "                               key=lambda x: x[1], \n",
    "                               reverse=True)[:5]\n",
    "        \n",
    "        for indicator, count in most_redundant:\n",
    "            recommendations.append(\n",
    "                f\"Consider removing {indicator} - correlated with {count} other indicators\"\n",
    "            )\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def plot_correlation_heatmap(self, figsize: Tuple[int, int] = FIGURE_SIZE,\n",
    "                                save_path: Optional[str] = None):\n",
    "        \"\"\"Plot correlation heatmap\"\"\"\n",
    "        if self.correlation_matrix is None:\n",
    "            logger.warning(\"No correlation matrix to plot\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create mask for upper triangle\n",
    "        mask = np.triu(np.ones_like(self.correlation_matrix), k=1)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(self.correlation_matrix, \n",
    "                   mask=mask,\n",
    "                   cmap='coolwarm',\n",
    "                   center=0,\n",
    "                   vmin=-1,\n",
    "                   vmax=1,\n",
    "                   square=True,\n",
    "                   linewidths=0.5,\n",
    "                   cbar_kws={\"shrink\": 0.8})\n",
    "        \n",
    "        plt.title('Indicator Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "            logger.info(f\"Correlation heatmap saved to: {save_path}\")\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# INDICATOR IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "class IndicatorImportanceAnalyzer:\n",
    "    \"\"\"Analyze indicator importance and contribution\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.importance_scores = {}\n",
    "        self.pca_results = None\n",
    "        \n",
    "    def analyze_regime_contribution(self, data: pd.DataFrame, \n",
    "                                  regimes: pd.DataFrame,\n",
    "                                  dimension: str = 'Direction') -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze how indicators contribute to regime classification\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with indicators\n",
    "            regimes: DataFrame with regime classifications\n",
    "            dimension: Regime dimension to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of indicator importance scores\n",
    "        \"\"\"\n",
    "        logger.info(f\"Analyzing indicator contribution to {dimension} regime\")\n",
    "        \n",
    "        # Get regime column\n",
    "        regime_col = f'{dimension}_Regime'\n",
    "        if regime_col not in regimes.columns:\n",
    "            logger.error(f\"Regime column {regime_col} not found\")\n",
    "            return {}\n",
    "        \n",
    "        # Convert regime to numeric\n",
    "        regime_numeric = pd.Categorical(regimes[regime_col]).codes\n",
    "        \n",
    "        # Get indicators for this dimension\n",
    "        dim_indicators = self._get_dimension_indicators(dimension)\n",
    "        available_indicators = [ind for ind in dim_indicators if ind in data.columns]\n",
    "        \n",
    "        if not available_indicators:\n",
    "            logger.warning(f\"No indicators found for dimension {dimension}\")\n",
    "            return {}\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        mi_scores = mutual_info_regression(\n",
    "            data[available_indicators].fillna(0),\n",
    "            regime_numeric\n",
    "        )\n",
    "        \n",
    "        # Normalize scores\n",
    "        if mi_scores.max() > 0:\n",
    "            mi_scores = mi_scores / mi_scores.max()\n",
    "        \n",
    "        # Create importance dictionary\n",
    "        importance = dict(zip(available_indicators, mi_scores))\n",
    "        \n",
    "        # Sort by importance\n",
    "        importance = dict(sorted(importance.items(), \n",
    "                               key=lambda x: x[1], \n",
    "                               reverse=True))\n",
    "        \n",
    "        self.importance_scores[dimension] = importance\n",
    "        \n",
    "        return importance\n",
    "    \n",
    "    def _get_dimension_indicators(self, dimension: str) -> List[str]:\n",
    "        \"\"\"Get indicators for a specific dimension\"\"\"\n",
    "        dimension_map = {\n",
    "            'Direction': ['SMA_Signal', 'EMA_Signal', 'MACD_Signal', 'ADX', \n",
    "                         'Aroon_Oscillator', 'CCI', 'PSAR', 'Vortex_Pos', \n",
    "                         'SuperTrend_Direction', 'DPO', 'KST'],\n",
    "            'TrendStrength': ['ADX', 'Aroon_Up', 'CCI', 'MACD_Histogram', \n",
    "                            'RSI', 'TSI', 'LinearReg_Slope', 'Correlation'],\n",
    "            'Velocity': ['ROC', 'RSI', 'TSI', 'MACD_Histogram', \n",
    "                        'Acceleration', 'Jerk'],\n",
    "            'Volatility': ['ATR', 'BB_Width', 'KC_Width', 'DC_Width', 'NATR', \n",
    "                         'UI', 'Historical_Vol', 'Parkinson', 'GarmanKlass', \n",
    "                         'RogersSatchell', 'YangZhang'],\n",
    "            'Microstructure': ['Volume', 'OBV', 'CMF', 'MFI', 'ADI', 'EOM', \n",
    "                             'FI', 'VPT', 'VWAP', 'CVD', 'Delta']\n",
    "        }\n",
    "        \n",
    "        return dimension_map.get(dimension, [])\n",
    "    \n",
    "    def run_pca_analysis(self, data: pd.DataFrame, \n",
    "                        n_components: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run PCA analysis to understand indicator relationships\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with indicators\n",
    "            n_components: Number of PCA components\n",
    "            \n",
    "        Returns:\n",
    "            PCA analysis results\n",
    "        \"\"\"\n",
    "        # Get indicator columns\n",
    "        exclude = ['open', 'high', 'low', 'close', 'volume', 'returns', \n",
    "                  'log_returns', 'date', 'timestamp']\n",
    "        indicators = [col for col in data.columns if col not in exclude]\n",
    "        \n",
    "        # Prepare data\n",
    "        X = data[indicators].fillna(0)\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Run PCA\n",
    "        pca = PCA(n_components=min(n_components, len(indicators)))\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Store results\n",
    "        self.pca_results = {\n",
    "            'explained_variance': pca.explained_variance_ratio_,\n",
    "            'cumulative_variance': np.cumsum(pca.explained_variance_ratio_),\n",
    "            'components': pd.DataFrame(\n",
    "                pca.components_,\n",
    "                columns=indicators,\n",
    "                index=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
    "            ),\n",
    "            'loadings': self._calculate_loadings(pca, indicators)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"PCA complete: {self.pca_results['cumulative_variance'][4]:.1%} \"\n",
    "                   f\"variance explained by first 5 components\")\n",
    "        \n",
    "        return self.pca_results\n",
    "    \n",
    "    def _calculate_loadings(self, pca, feature_names: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Calculate PCA loadings\"\"\"\n",
    "        loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "        \n",
    "        return pd.DataFrame(\n",
    "            loadings,\n",
    "            index=feature_names,\n",
    "            columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
    "        )\n",
    "    \n",
    "    def get_importance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of indicator importance across dimensions\"\"\"\n",
    "        if not self.importance_scores:\n",
    "            return {}\n",
    "        \n",
    "        # Aggregate importance across dimensions\n",
    "        all_indicators = {}\n",
    "        \n",
    "        for dimension, scores in self.importance_scores.items():\n",
    "            for indicator, score in scores.items():\n",
    "                if indicator not in all_indicators:\n",
    "                    all_indicators[indicator] = []\n",
    "                all_indicators[indicator].append(score)\n",
    "        \n",
    "        # Calculate average importance\n",
    "        avg_importance = {\n",
    "            ind: np.mean(scores) for ind, scores in all_indicators.items()\n",
    "        }\n",
    "        \n",
    "        # Sort by importance\n",
    "        sorted_importance = dict(sorted(avg_importance.items(), \n",
    "                                      key=lambda x: x[1], \n",
    "                                      reverse=True))\n",
    "        \n",
    "        return {\n",
    "            'top_indicators': list(sorted_importance.keys())[:10],\n",
    "            'importance_scores': sorted_importance,\n",
    "            'dimension_specific': self.importance_scores,\n",
    "            'recommendations': self._generate_importance_recommendations(sorted_importance)\n",
    "        }\n",
    "    \n",
    "    def _generate_importance_recommendations(self, \n",
    "                                           importance_scores: Dict[str, float]) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on importance analysis\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Find low importance indicators\n",
    "        low_importance = [ind for ind, score in importance_scores.items() \n",
    "                         if score < 0.1]\n",
    "        \n",
    "        if low_importance:\n",
    "            recommendations.append(\n",
    "                f\"Consider removing {len(low_importance)} low-importance indicators: \"\n",
    "                f\"{', '.join(low_importance[:5])}\"\n",
    "            )\n",
    "        \n",
    "        # Check dimension balance\n",
    "        for dimension in REGIME_DIMENSIONS:\n",
    "            if dimension in self.importance_scores:\n",
    "                dim_scores = self.importance_scores[dimension]\n",
    "                if len(dim_scores) < 3:\n",
    "                    recommendations.append(\n",
    "                        f\"Warning: {dimension} dimension has only {len(dim_scores)} \"\n",
    "                        f\"active indicators\"\n",
    "                    )\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# =============================================================================\n",
    "# INDICATOR VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "class IndicatorValidator:\n",
    "    \"\"\"Validate indicator calculations and data quality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = {}\n",
    "        \n",
    "    def validate_indicators(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run comprehensive indicator validation\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with indicators\n",
    "            \n",
    "        Returns:\n",
    "            Validation results\n",
    "        \"\"\"\n",
    "        logger.info(\"Running indicator validation\")\n",
    "        \n",
    "        results = {\n",
    "            'missing_data': self._check_missing_data(data),\n",
    "            'data_quality': self._check_data_quality(data),\n",
    "            'indicator_ranges': self._check_indicator_ranges(data),\n",
    "            'calculation_consistency': self._check_calculation_consistency(data),\n",
    "            'overall_score': 0.0\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        scores = []\n",
    "        \n",
    "        # Missing data score (lower is better)\n",
    "        missing_pct = results['missing_data']['overall_missing_pct']\n",
    "        scores.append(max(0, 1 - missing_pct / 50))  # 50% missing = 0 score\n",
    "        \n",
    "        # Quality issues score\n",
    "        quality_score = results['data_quality']['quality_score']\n",
    "        scores.append(quality_score)\n",
    "        \n",
    "        # Range violations score\n",
    "        range_issues = len(results['indicator_ranges']['out_of_range'])\n",
    "        scores.append(max(0, 1 - range_issues / 10))  # 10 issues = 0 score\n",
    "        \n",
    "        results['overall_score'] = np.mean(scores)\n",
    "        \n",
    "        self.validation_results = results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _check_missing_data(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Check for missing data in indicators\"\"\"\n",
    "        missing_counts = data.isnull().sum()\n",
    "        missing_pct = (missing_counts / len(data)) * 100\n",
    "        \n",
    "        # Find problematic indicators\n",
    "        problematic = missing_pct[missing_pct > 10].to_dict()\n",
    "        \n",
    "        return {\n",
    "            'total_missing': missing_counts.sum(),\n",
    "            'overall_missing_pct': missing_pct.mean(),\n",
    "            'problematic_indicators': problematic,\n",
    "            'recommendation': \"Consider imputation for indicators with >10% missing\"\n",
    "                            if problematic else \"Missing data levels acceptable\"\n",
    "        }\n",
    "    \n",
    "    def _check_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Check data quality issues\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check for infinite values\n",
    "        inf_columns = []\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype in ['float64', 'int64']:\n",
    "                if np.isinf(data[col]).any():\n",
    "                    inf_columns.append(col)\n",
    "                    issues.append(f\"{col} contains infinite values\")\n",
    "        \n",
    "        # Check for constant columns\n",
    "        constant_columns = []\n",
    "        for col in data.columns:\n",
    "            if data[col].nunique() == 1:\n",
    "                constant_columns.append(col)\n",
    "                issues.append(f\"{col} is constant\")\n",
    "        \n",
    "        # Check for duplicate columns\n",
    "        duplicate_columns = []\n",
    "        for i, col1 in enumerate(data.columns):\n",
    "            for col2 in data.columns[i+1:]:\n",
    "                if data[col1].equals(data[col2]):\n",
    "                    duplicate_columns.append((col1, col2))\n",
    "                    issues.append(f\"{col1} and {col2} are identical\")\n",
    "        \n",
    "        quality_score = max(0, 1 - len(issues) / 20)  # 20 issues = 0 score\n",
    "        \n",
    "        return {\n",
    "            'issues': issues[:10],  # Top 10 issues\n",
    "            'infinite_values': inf_columns,\n",
    "            'constant_columns': constant_columns,\n",
    "            'duplicate_columns': duplicate_columns,\n",
    "            'quality_score': quality_score\n",
    "        }\n",
    "    \n",
    "    def _check_indicator_ranges(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Check if indicators are within expected ranges\"\"\"\n",
    "        out_of_range = []\n",
    "        \n",
    "        # Define expected ranges for common indicators\n",
    "        expected_ranges = {\n",
    "            'RSI': (0, 100),\n",
    "            'MFI': (0, 100),\n",
    "            'Stoch_K': (0, 100),\n",
    "            'Stoch_D': (0, 100),\n",
    "            'BB_Percent': (0, 1),\n",
    "            'ADX': (0, 100),\n",
    "            'Aroon_Up': (0, 100),\n",
    "            'Aroon_Down': (0, 100),\n",
    "            'CCI': (-200, 200),  # Typical range\n",
    "            'CMF': (-1, 1)\n",
    "        }\n",
    "        \n",
    "        for indicator, (min_val, max_val) in expected_ranges.items():\n",
    "            if indicator in data.columns:\n",
    "                actual_min = data[indicator].min()\n",
    "                actual_max = data[indicator].max()\n",
    "                \n",
    "                if actual_min < min_val or actual_max > max_val:\n",
    "                    out_of_range.append({\n",
    "                        'indicator': indicator,\n",
    "                        'expected_range': (min_val, max_val),\n",
    "                        'actual_range': (actual_min, actual_max)\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'out_of_range': out_of_range,\n",
    "            'recommendation': f\"Review {len(out_of_range)} indicators with \"\n",
    "                            f\"unexpected ranges\" if out_of_range else \n",
    "                            \"All indicators within expected ranges\"\n",
    "        }\n",
    "    \n",
    "    def _check_calculation_consistency(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Check calculation consistency between related indicators\"\"\"\n",
    "        inconsistencies = []\n",
    "        \n",
    "        # Check MACD components\n",
    "        if all(col in data.columns for col in ['MACD', 'MACD_Signal_Line']):\n",
    "            # MACD signal should be smoother than MACD\n",
    "            macd_std = data['MACD'].std()\n",
    "            signal_std = data['MACD_Signal_Line'].std()\n",
    "            \n",
    "            if signal_std > macd_std:\n",
    "                inconsistencies.append(\n",
    "                    \"MACD Signal Line more volatile than MACD line\"\n",
    "                )\n",
    "        \n",
    "        # Check Bollinger Bands\n",
    "        if all(col in data.columns for col in ['BB_Upper', 'BB_Lower', 'close']):\n",
    "            # Close should occasionally touch bands\n",
    "            touches_upper = (data['close'] >= data['BB_Upper']).sum()\n",
    "            touches_lower = (data['close'] <= data['BB_Lower']).sum()\n",
    "            \n",
    "            touch_pct = (touches_upper + touches_lower) / len(data) * 100\n",
    "            \n",
    "            if touch_pct < 1:\n",
    "                inconsistencies.append(\n",
    "                    f\"Price rarely touches Bollinger Bands ({touch_pct:.1f}%)\"\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            'inconsistencies': inconsistencies,\n",
    "            'recommendation': \"Review indicator calculations\" if inconsistencies\n",
    "                            else \"Indicator calculations appear consistent\"\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ANALYSIS FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_indicator_analysis(data: pd.DataFrame, \n",
    "                         regimes: pd.DataFrame,\n",
    "                         save_report: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete indicator analysis\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with indicators\n",
    "        regimes: DataFrame with regime classifications\n",
    "        save_report: Whether to save analysis report\n",
    "        \n",
    "    Returns:\n",
    "        Complete analysis results\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting comprehensive indicator analysis\")\n",
    "    \n",
    "    # Initialize analyzers\n",
    "    correlation_analyzer = IndicatorCorrelationAnalyzer()\n",
    "    importance_analyzer = IndicatorImportanceAnalyzer()\n",
    "    validator = IndicatorValidator()\n",
    "    \n",
    "    # Run analyses\n",
    "    results = {\n",
    "        'correlation_analysis': {},\n",
    "        'importance_analysis': {},\n",
    "        'validation': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # 1. Correlation analysis\n",
    "    correlation_matrix = correlation_analyzer.analyze_correlations(data)\n",
    "    results['correlation_analysis'] = correlation_analyzer.get_redundancy_report()\n",
    "    \n",
    "    # 2. Importance analysis\n",
    "    for dimension in REGIME_DIMENSIONS:\n",
    "        importance_analyzer.analyze_regime_contribution(data, regimes, dimension)\n",
    "    \n",
    "    results['importance_analysis'] = importance_analyzer.get_importance_summary()\n",
    "    \n",
    "    # 3. PCA analysis\n",
    "    pca_results = importance_analyzer.run_pca_analysis(data)\n",
    "    results['pca_analysis'] = {\n",
    "        'variance_explained_5pc': pca_results['cumulative_variance'][4],\n",
    "        'variance_explained_10pc': pca_results['cumulative_variance'][9] \n",
    "                                   if len(pca_results['cumulative_variance']) > 9 else None\n",
    "    }\n",
    "    \n",
    "    # 4. Validation\n",
    "    results['validation'] = validator.validate_indicators(data)\n",
    "    \n",
    "    # 5. Generate overall recommendations\n",
    "    all_recommendations = []\n",
    "    \n",
    "    # From correlation analysis\n",
    "    all_recommendations.extend(\n",
    "        results['correlation_analysis'].get('recommendation', [])\n",
    "    )\n",
    "    \n",
    "    # From importance analysis\n",
    "    all_recommendations.extend(\n",
    "        results['importance_analysis'].get('recommendations', [])\n",
    "    )\n",
    "    \n",
    "    # From validation\n",
    "    all_recommendations.append(\n",
    "        results['validation']['missing_data']['recommendation']\n",
    "    )\n",
    "    \n",
    "    results['recommendations'] = all_recommendations\n",
    "    \n",
    "    # 6. Save report if requested\n",
    "    if save_report:\n",
    "        save_indicator_analysis_report(results, correlation_analyzer, importance_analyzer)\n",
    "    \n",
    "    logger.info(\"Indicator analysis complete\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_indicator_analysis_report(results: Dict[str, Any],\n",
    "                                 correlation_analyzer: IndicatorCorrelationAnalyzer,\n",
    "                                 importance_analyzer: IndicatorImportanceAnalyzer):\n",
    "    \"\"\"Save indicator analysis report\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save plots\n",
    "    plot_dir = os.path.join(RESULTS_DIR, 'indicator_analysis')\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    correlation_analyzer.plot_correlation_heatmap(\n",
    "        save_path=os.path.join(plot_dir, f'correlation_heatmap_{timestamp}.png')\n",
    "    )\n",
    "    \n",
    "    # Save text report\n",
    "    report_path = os.path.join(plot_dir, f'indicator_analysis_report_{timestamp}.txt')\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"INDICATOR ANALYSIS REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"1. CORRELATION ANALYSIS\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        f.write(f\"Redundant pairs found: {results['correlation_analysis']['redundant_count']}\\n\")\n",
    "        \n",
    "        if results['correlation_analysis']['pairs']:\n",
    "            f.write(\"\\nTop correlated pairs:\\n\")\n",
    "            for pair in results['correlation_analysis']['pairs'][:5]:\n",
    "                f.write(f\"  {pair['indicator1']} <-> {pair['indicator2']}: \"\n",
    "                       f\"{pair['correlation']:.3f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n2. IMPORTANCE ANALYSIS\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        f.write(\"Top 10 most important indicators:\\n\")\n",
    "        for i, indicator in enumerate(results['importance_analysis']['top_indicators'], 1):\n",
    "            score = results['importance_analysis']['importance_scores'][indicator]\n",
    "            f.write(f\"  {i}. {indicator}: {score:.3f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n3. VALIDATION RESULTS\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        f.write(f\"Overall validation score: {results['validation']['overall_score']:.2f}/1.0\\n\")\n",
    "        f.write(f\"Missing data: {results['validation']['missing_data']['overall_missing_pct']:.1f}%\\n\")\n",
    "        f.write(f\"Quality issues: {len(results['validation']['data_quality']['issues'])}\\n\")\n",
    "        \n",
    "        f.write(\"\\n4. RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for i, rec in enumerate(results['recommendations'], 1):\n",
    "            f.write(f\"{i}. {rec}\\n\")\n",
    "    \n",
    "    logger.info(f\"Indicator analysis report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
