{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425560fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py:percent\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.17.2\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3 (ipykernel)\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80c982-0f90-4d27-8ed1-8cb218c992b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Add parent directory to path for imports\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     DATA_DIR, TIMEFRAMES, DEFAULT_SYMBOLS,\n\u001b[0;32m     19\u001b[0m     LOG_LEVEL, LOG_FORMAT\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# VS CODE TEST TEXT 2\n",
    "\n",
    "\"\"\"\n",
    "Data loading and preprocessing functionality\n",
    "Handles CSV loading, data validation, and preprocessing\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from config.settings import (\n",
    "    DATA_DIR, TIMEFRAMES, DEFAULT_SYMBOLS,\n",
    "    LOG_LEVEL, LOG_FORMAT\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=getattr(logging, LOG_LEVEL), format=LOG_FORMAT)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_csv_data(filepath: str, \n",
    "                  parse_dates: bool = True,\n",
    "                  date_column: str = 'Date',\n",
    "                  timeframe: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV data with automatic date parsing and column standardization\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file\n",
    "        parse_dates: Whether to parse date column\n",
    "        date_column: Name of date column\n",
    "        timeframe: Optional timeframe hint (e.g., '15min', '1H', 'Daily')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with standardized columns and datetime index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading data from: {filepath}\")\n",
    "        \n",
    "        # Load the CSV\n",
    "        if parse_dates:\n",
    "            df = pd.read_csv(filepath, parse_dates=[date_column])\n",
    "        else:\n",
    "            df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Standardize column names\n",
    "        df.columns = [col.strip().lower() for col in df.columns]\n",
    "        \n",
    "        # Handle date column\n",
    "        date_col = date_column.lower()\n",
    "        if date_col in df.columns:\n",
    "            df['date'] = pd.to_datetime(df[date_col])\n",
    "            df.set_index('date', inplace=True)\n",
    "            if date_col != 'date':\n",
    "                df.drop(columns=[date_col], inplace=True)\n",
    "        \n",
    "        # Sort by date\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        # Check required columns\n",
    "        required_columns = ['open', 'high', 'low', 'close']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            logger.warning(f\"Missing columns: {missing_columns}\")\n",
    "            # Try to handle missing high/low\n",
    "            if 'high' not in df.columns and 'close' in df.columns:\n",
    "                df['high'] = df['close']\n",
    "            if 'low' not in df.columns and 'close' in df.columns:\n",
    "                df['low'] = df['close']\n",
    "        \n",
    "        # Infer timeframe if not provided\n",
    "        if timeframe is None and len(df) > 1:\n",
    "            timeframe = infer_timeframe(df)\n",
    "            logger.info(f\"Inferred timeframe: {timeframe}\")\n",
    "        \n",
    "        # Add timeframe info\n",
    "        df.attrs['timeframe'] = timeframe\n",
    "        \n",
    "        # Basic data quality checks\n",
    "        df = validate_and_clean_data(df)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df)} rows from {df.index[0]} to {df.index[-1]}\")\n",
    "        logger.info(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV data: {e}\")\n",
    "        raise\n",
    "\n",
    "def infer_timeframe(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Infer timeframe from data frequency\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with datetime index\n",
    "        \n",
    "    Returns:\n",
    "        Timeframe string (e.g., '15min', '1H', 'Daily')\n",
    "    \"\"\"\n",
    "    if len(df) < 2:\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Calculate median time difference\n",
    "    time_diffs = df.index[1:] - df.index[:-1]\n",
    "    median_diff = pd.Timedelta(np.median(time_diffs.total_seconds()), unit='s')\n",
    "    \n",
    "    # Map to standard timeframes\n",
    "    if median_diff <= pd.Timedelta(minutes=6):\n",
    "        return '5min'\n",
    "    elif median_diff <= pd.Timedelta(minutes=20):\n",
    "        return '15min'\n",
    "    elif median_diff <= pd.Timedelta(minutes=35):\n",
    "        return '30min'\n",
    "    elif median_diff <= pd.Timedelta(hours=1.5):\n",
    "        return '1H'\n",
    "    elif median_diff <= pd.Timedelta(hours=5):\n",
    "        return '4H'\n",
    "    elif median_diff <= pd.Timedelta(days=2):\n",
    "        return 'Daily'\n",
    "    else:\n",
    "        return 'Weekly'\n",
    "\n",
    "def validate_and_clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate and clean price data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLC data\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Remove rows with any NaN in OHLC\n",
    "    ohlc_cols = ['open', 'high', 'low', 'close']\n",
    "    existing_ohlc = [col for col in ohlc_cols if col in df.columns]\n",
    "    df = df.dropna(subset=existing_ohlc)\n",
    "    \n",
    "    # Validate price relationships\n",
    "    if all(col in df.columns for col in ['open', 'high', 'low', 'close']):\n",
    "        # High should be >= Low\n",
    "        invalid_hl = df['high'] < df['low']\n",
    "        if invalid_hl.any():\n",
    "            logger.warning(f\"Found {invalid_hl.sum()} rows with high < low, fixing...\")\n",
    "            df.loc[invalid_hl, 'high'] = df.loc[invalid_hl, ['open', 'close']].max(axis=1)\n",
    "            df.loc[invalid_hl, 'low'] = df.loc[invalid_hl, ['open', 'close']].min(axis=1)\n",
    "    \n",
    "    # Remove rows with zero or negative prices\n",
    "    for col in existing_ohlc:\n",
    "        df = df[df[col] > 0]\n",
    "    \n",
    "    final_rows = len(df)\n",
    "    if final_rows < initial_rows:\n",
    "        logger.info(f\"Data cleaning removed {initial_rows - final_rows} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_multiple_files(filepaths: List[str], \n",
    "                       concat: bool = True) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Load multiple CSV files\n",
    "    \n",
    "    Args:\n",
    "        filepaths: List of file paths\n",
    "        concat: Whether to concatenate into single DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Single concatenated DataFrame or dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        filename = os.path.basename(filepath).split('.')[0]\n",
    "        try:\n",
    "            df = load_csv_data(filepath)\n",
    "            data_dict[filename] = df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {filepath}: {e}\")\n",
    "    \n",
    "    if concat and len(data_dict) > 1:\n",
    "        # Concatenate all DataFrames\n",
    "        all_data = pd.concat(data_dict.values(), sort=True)\n",
    "        all_data.sort_index(inplace=True)\n",
    "        return all_data\n",
    "    else:\n",
    "        return data_dict\n",
    "\n",
    "def resample_data(df: pd.DataFrame, \n",
    "                  target_timeframe: str,\n",
    "                  method: str = 'ohlc') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resample data to different timeframe\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLC data\n",
    "        target_timeframe: Target timeframe (e.g., '1H', '4H', 'D')\n",
    "        method: Resampling method ('ohlc' or 'last')\n",
    "        \n",
    "    Returns:\n",
    "        Resampled DataFrame\n",
    "    \"\"\"\n",
    "    # Convert timeframe to pandas frequency\n",
    "    freq_map = {\n",
    "        '5min': '5T',\n",
    "        '15min': '15T',\n",
    "        '30min': '30T',\n",
    "        '1H': '1H',\n",
    "        '4H': '4H',\n",
    "        'Daily': 'D',\n",
    "        'Weekly': 'W'\n",
    "    }\n",
    "    \n",
    "    if target_timeframe not in freq_map:\n",
    "        raise ValueError(f\"Unknown timeframe: {target_timeframe}\")\n",
    "    \n",
    "    freq = freq_map[target_timeframe]\n",
    "    \n",
    "    if method == 'ohlc':\n",
    "        # Resample OHLC properly\n",
    "        resampled = df.resample(freq).agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum' if 'volume' in df.columns else None\n",
    "        }).dropna()\n",
    "    else:\n",
    "        # Simple last value resampling\n",
    "        resampled = df.resample(freq).last().dropna()\n",
    "    \n",
    "    return resampled\n",
    "\n",
    "def prepare_data_for_analysis(df: pd.DataFrame,\n",
    "                            min_periods: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare data for regime analysis\n",
    "    \n",
    "    Args:\n",
    "        df: Raw OHLC DataFrame\n",
    "        min_periods: Minimum periods required\n",
    "        \n",
    "    Returns:\n",
    "        Prepared DataFrame\n",
    "    \"\"\"\n",
    "    if len(df) < min_periods:\n",
    "        raise ValueError(f\"Insufficient data: {len(df)} rows, need at least {min_periods}\")\n",
    "    \n",
    "    # Ensure we have all required columns\n",
    "    if 'high' not in df.columns:\n",
    "        df['high'] = df['close']\n",
    "    if 'low' not in df.columns:\n",
    "        df['low'] = df['close']\n",
    "    \n",
    "    # Add basic calculated fields\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # Add session info if intraday\n",
    "    if df.attrs.get('timeframe') in ['5min', '15min', '30min', '1H']:\n",
    "        df['hour'] = df.index.hour\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_processed_data(df: pd.DataFrame, \n",
    "                       filepath: str,\n",
    "                       format: str = 'csv') -> None:\n",
    "    \"\"\"\n",
    "    Save processed data to file\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        filepath: Output file path\n",
    "        format: Output format ('csv', 'parquet', 'pickle')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if format == 'csv':\n",
    "            df.to_csv(filepath)\n",
    "        elif format == 'parquet':\n",
    "            df.to_parquet(filepath)\n",
    "        elif format == 'pickle':\n",
    "            df.to_pickle(filepath)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "        \n",
    "        logger.info(f\"Saved {len(df)} rows to {filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Utility functions for data information\n",
    "def get_data_info(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Get summary information about the data\"\"\"\n",
    "    info = {\n",
    "        'rows': len(df),\n",
    "        'columns': list(df.columns),\n",
    "        'start_date': df.index[0],\n",
    "        'end_date': df.index[-1],\n",
    "        'timeframe': df.attrs.get('timeframe', 'Unknown'),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'memory_usage': df.memory_usage().sum() / 1024**2  # MB\n",
    "    }\n",
    "    return info\n",
    "\n",
    "def check_data_quality(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Run data quality checks\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for gaps\n",
    "    if df.attrs.get('timeframe') in ['5min', '15min', '30min', '1H']:\n",
    "        expected_freq = df.index.to_series().diff().mode()[0]\n",
    "        gaps = df.index.to_series().diff()[1:] > expected_freq * 2\n",
    "        if gaps.any():\n",
    "            issues.append(f\"Found {gaps.sum()} time gaps in data\")\n",
    "    \n",
    "    # Check for outliers\n",
    "    for col in ['open', 'high', 'low', 'close']:\n",
    "        if col in df.columns:\n",
    "            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "            outliers = z_scores > 5\n",
    "            if outliers.any():\n",
    "                issues.append(f\"Found {outliers.sum()} outliers in {col}\")\n",
    "    \n",
    "    return {\n",
    "        'issues': issues,\n",
    "        'quality_score': 1.0 - len(issues) / 10.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f7e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
