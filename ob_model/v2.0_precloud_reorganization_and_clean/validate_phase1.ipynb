{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to validate indicators using REAL market data\n",
    "Phase 1 - Testing with actual futures data\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Add regime_system to path\n",
    "sys.path.insert(0, r'C:\\Users\\rs\\GitProjects\\regime_system\\ob_model\\v2.0_precloud_reorganization_and_clean')\n",
    "\n",
    "# Import required modules\n",
    "from validation.indicator_analysis import IndicatorValidator\n",
    "from core.data_loader import load_csv_data\n",
    "from core.indicators import calculate_all_indicators\n",
    "from core.regime_classifier import RollingRegimeClassifier\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: VALIDATION WITH REAL MARKET DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def run_indicator_validation_real_data():\n",
    "    \"\"\"Run validation with real market data\"\"\"\n",
    "    \n",
    "    # Path to your actual data file\n",
    "    # Update this path to your actual CSV file location\n",
    "    data_files = [\n",
    "        r\"C:\\Users\\rs\\GitProjects\\regime_system\\ob_model\\v2.0_precloud_reorganization_and_clean\\combined_NQ_15m_data.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Find the first existing file\n",
    "    data_file = None\n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            data_file = file_path\n",
    "            break\n",
    "    \n",
    "    if data_file is None:\n",
    "        print(\"\\n⚠️ No data file found! Please update the path to your actual data file.\")\n",
    "        print(\"Looking for files in these locations:\")\n",
    "        for fp in data_files:\n",
    "            print(f\"  - {fp}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\n✓ Found data file: {data_file}\")\n",
    "    \n",
    "    # Load the data\n",
    "    print(\"\\nLoading real market data...\")\n",
    "    try:\n",
    "        data = load_csv_data(data_file, timeframe='15min')\n",
    "        print(f\"Loaded {len(data)} rows of data\")\n",
    "        print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "        \n",
    "        # Use a subset for faster testing (last 10,000 rows)\n",
    "        if len(data) > 10000:\n",
    "            print(f\"\\nUsing last 10,000 rows for validation (full dataset has {len(data)} rows)\")\n",
    "            data = data.tail(10000)\n",
    "        \n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"Data types: {data.dtypes.value_counts()}\")\n",
    "        \n",
    "        # Show any non-numeric columns\n",
    "        non_numeric = data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        if non_numeric:\n",
    "            print(f\"Non-numeric columns found: {non_numeric}\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required = ['open', 'high', 'low', 'close']\n",
    "        missing = [col for col in required if col not in data.columns]\n",
    "        if missing:\n",
    "            print(f\"\\n⚠️ Missing required columns: {missing}\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error loading data: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Calculate indicators\n",
    "    print(\"\\nCalculating indicators on real data...\")\n",
    "    try:\n",
    "        data_with_indicators = calculate_all_indicators(data, verbose=True)\n",
    "        indicator_count = len(data_with_indicators.columns) - len(data.columns)\n",
    "        print(f\"\\n✓ Successfully calculated {indicator_count} indicators\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error calculating indicators: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame(), {'error': str(e)}\n",
    "    \n",
    "    # Validate indicators\n",
    "    print(\"\\nValidating indicators...\")\n",
    "    validator = IndicatorValidator()\n",
    "    validation_results = validator.validate_indicators(data_with_indicators)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"  Overall missing %: {validation_results['missing_data']['overall_missing_pct']:.2f}%\")\n",
    "    print(f\"  Quality score: {validation_results['data_quality']['quality_score']:.2f}\")\n",
    "    print(f\"  Range issues: {len(validation_results['indicator_ranges']['out_of_range'])}\")\n",
    "    print(f\"  Overall score: {validation_results['overall_score']:.2f}\")\n",
    "    \n",
    "    # Show indicator count by type\n",
    "    numeric_indicators = data_with_indicators.select_dtypes(include=[np.number]).columns\n",
    "    non_base_numeric = [col for col in numeric_indicators if col not in ['open', 'high', 'low', 'close', 'volume']]\n",
    "    print(f\"\\nIndicator Summary:\")\n",
    "    print(f\"  Numeric indicators: {len(non_base_numeric)}\")\n",
    "    print(f\"  Total columns: {len(data_with_indicators.columns)}\")\n",
    "    \n",
    "    return data_with_indicators, validation_results\n",
    "\n",
    "def check_confidence_scores_real_data(data_with_indicators):\n",
    "    \"\"\"Check confidence scores with real market data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REGIME CLASSIFICATION WITH REAL DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create classifier with 36-hour window (optimal for 15-min NQ)\n",
    "    classifier = RollingRegimeClassifier(window_hours=36, timeframe='15min')\n",
    "    \n",
    "    # Classify regimes\n",
    "    print(\"Classifying regimes...\")\n",
    "    regimes = classifier.classify_regimes(data_with_indicators, show_progress=True)\n",
    "    \n",
    "    # Get regime statistics\n",
    "    stats = classifier.get_regime_statistics(regimes)\n",
    "    \n",
    "    # Print regime distributions\n",
    "    print(\"\\nREGIME DISTRIBUTIONS:\")\n",
    "    for dimension, dim_stats in stats.items():\n",
    "        if dimension != 'Composite' and 'percentages' in dim_stats:\n",
    "            print(f\"\\n{dimension} Dimension:\")\n",
    "            for regime, pct in sorted(dim_stats['percentages'].items()):\n",
    "                if regime != 'Undefined':\n",
    "                    print(f\"  {regime}: {pct:.1f}%\")\n",
    "    \n",
    "    # Check confidence scores\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFIDENCE SCORE ANALYSIS:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    confidence_summary = {}\n",
    "    \n",
    "    for dim in ['Direction', 'TrendStrength', 'Velocity', 'Volatility', 'Microstructure']:\n",
    "        conf_col = f'{dim}_Confidence'\n",
    "        if conf_col in regimes.columns:\n",
    "            conf_values = regimes[conf_col].dropna()\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats_dict = {\n",
    "                'mean': conf_values.mean(),\n",
    "                'std': conf_values.std(),\n",
    "                'min': conf_values.min(),\n",
    "                'max': conf_values.max(),\n",
    "                'unique': len(conf_values.unique()),\n",
    "                'zeros': (conf_values == 0).sum(),\n",
    "                'ones': (conf_values == 1).sum()\n",
    "            }\n",
    "            \n",
    "            confidence_summary[dim] = stats_dict\n",
    "            \n",
    "            print(f\"\\n{dim} Confidence:\")\n",
    "            print(f\"  Mean: {stats_dict['mean']:.3f}\")\n",
    "            print(f\"  Std: {stats_dict['std']:.3f}\")\n",
    "            print(f\"  Min: {stats_dict['min']:.3f}\")\n",
    "            print(f\"  Max: {stats_dict['max']:.3f}\")\n",
    "            print(f\"  Unique values: {stats_dict['unique']}\")\n",
    "            \n",
    "            # Check for issues\n",
    "            if stats_dict['mean'] == 0:\n",
    "                print(f\"  ⚠️ WARNING: All zeros - no confident classifications!\")\n",
    "            elif stats_dict['mean'] == 1:\n",
    "                print(f\"  ⚠️ WARNING: All ones - perfect agreement (suspicious)!\")\n",
    "            elif stats_dict['std'] < 0.01:\n",
    "                print(f\"  ⚠️ WARNING: Very low variance!\")\n",
    "            else:\n",
    "                print(f\"  ✓ Confidence scores show proper variation\")\n",
    "            \n",
    "            # Show distribution\n",
    "            if stats_dict['unique'] <= 5:\n",
    "                print(f\"  Distribution: {conf_values.value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    return regimes, confidence_summary\n",
    "\n",
    "def analyze_correlations_focused(data_with_indicators):\n",
    "    \"\"\"Analyze correlations with focus on problematic pairs\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Select only indicator columns (numeric only)\n",
    "    base_cols = ['open', 'high', 'low', 'close', 'volume', 'Symbol', 'Date']\n",
    "    numeric_cols = data_with_indicators.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    indicator_cols = [col for col in numeric_cols if col not in base_cols]\n",
    "    \n",
    "    # Additional check for any string columns that might have slipped through\n",
    "    indicator_cols = [col for col in indicator_cols if data_with_indicators[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "    \n",
    "    print(f\"Analyzing correlations for {len(indicator_cols)} numeric indicators...\")\n",
    "    \n",
    "    # Debug: Show first few indicators\n",
    "    print(f\"First 5 indicators: {indicator_cols[:5]}\")\n",
    "    \n",
    "    try:\n",
    "        corr_matrix = data_with_indicators[indicator_cols].corr()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error calculating correlations: {e}\")\n",
    "        print(\"Checking for problematic columns...\")\n",
    "        for col in indicator_cols:\n",
    "            try:\n",
    "                test = data_with_indicators[col].astype(float)\n",
    "            except:\n",
    "                print(f\"  Problem column: {col} - {data_with_indicators[col].dtype}\")\n",
    "        return None, [], []\n",
    "    \n",
    "    # Find perfect correlations (>0.99)\n",
    "    perfect_corr_pairs = []\n",
    "    high_corr_pairs = []\n",
    "    seen_pairs = set()  # Track unique pairs\n",
    "    \n",
    "    for i in range(len(indicator_cols)):\n",
    "        for j in range(i+1, len(indicator_cols)):\n",
    "            pair = tuple(sorted([indicator_cols[i], indicator_cols[j]]))  # Ensure consistent ordering\n",
    "            if pair not in seen_pairs:\n",
    "                corr_value = abs(corr_matrix.iloc[i, j])\n",
    "                if corr_value > 0.99 and not pd.isna(corr_value):\n",
    "                    perfect_corr_pairs.append({\n",
    "                        'indicator1': indicator_cols[i],\n",
    "                        'indicator2': indicator_cols[j],\n",
    "                        'correlation': corr_value\n",
    "                    })\n",
    "                elif corr_value > 0.90 and not pd.isna(corr_value):\n",
    "                    high_corr_pairs.append({\n",
    "                        'indicator1': indicator_cols[i],\n",
    "                        'indicator2': indicator_cols[j],\n",
    "                        'correlation': corr_value\n",
    "                    })\n",
    "                seen_pairs.add(pair)\n",
    "    \n",
    "    # Ensure unique pairs in output by sorting and deduplicating\n",
    "    unique_perfect_pairs = []\n",
    "    seen_output_pairs = set()\n",
    "    for pair in sorted(perfect_corr_pairs, key=lambda x: x['correlation'], reverse=True):\n",
    "        output_pair = tuple(sorted([pair['indicator1'], pair['indicator2']]))\n",
    "        if output_pair not in seen_output_pairs:\n",
    "            unique_perfect_pairs.append(pair)\n",
    "            seen_output_pairs.add(output_pair)\n",
    "    \n",
    "    print(f\"\\nPerfect correlations (>0.99): {len(unique_perfect_pairs)}\")\n",
    "    print(f\"High correlations (0.90-0.99): {len(high_corr_pairs)}\")\n",
    "    \n",
    "    # Show perfect correlations\n",
    "    if unique_perfect_pairs:\n",
    "        print(\"\\nPERFECT CORRELATIONS (Consider removing one from each pair):\")\n",
    "        for pair in unique_perfect_pairs[:10]:\n",
    "            print(f\"  - {pair['indicator1']} = {pair['indicator2']} ({pair['correlation']:.4f})\")\n",
    "    \n",
    "    return corr_matrix, unique_perfect_pairs, high_corr_pairs\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nRunning validation with REAL market data...\")\n",
    "    \n",
    "    # Step 1: Validate indicators with real data\n",
    "    data_with_indicators, validation_results = run_indicator_validation_real_data()\n",
    "    \n",
    "    if data_with_indicators is not None and not data_with_indicators.empty:\n",
    "        # Step 2: Check confidence scores\n",
    "        regimes, confidence_summary = check_confidence_scores_real_data(data_with_indicators)\n",
    "        \n",
    "        # Step 3: Analyze correlations\n",
    "        corr_matrix, perfect_corr_pairs, high_corr_pairs = analyze_correlations_focused(data_with_indicators)\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY - REAL DATA VALIDATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Check if Direction and Velocity improved\n",
    "        if 'Direction' in confidence_summary and confidence_summary['Direction']['mean'] > 0:\n",
    "            print(\"✓ Direction confidence improved with real data!\")\n",
    "        else:\n",
    "            print(\"✗ Direction confidence still showing issues\")\n",
    "            \n",
    "        if 'Velocity' in confidence_summary and confidence_summary['Velocity']['mean'] > 0:\n",
    "            print(\"✓ Velocity confidence improved with real data!\")\n",
    "        else:\n",
    "            print(\"✗ Velocity confidence still showing issues\")\n",
    "        \n",
    "        if corr_matrix is not None:\n",
    "            print(f\"\\n⚠️ Found {len(perfect_corr_pairs)} perfect correlations to remove\")\n",
    "            print(f\"⚠️ Found {len(high_corr_pairs)} high correlations to review\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ Correlation analysis failed - check for non-numeric columns\")\n",
    "        \n",
    "        print(\"\\nNEXT STEPS:\")\n",
    "        print(\"1. Review confidence score distributions\")\n",
    "        print(\"2. Remove perfectly correlated indicators\")\n",
    "        print(\"3. Test on different time periods\")\n",
    "        print(\"4. Proceed with regime distribution validation\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n✗ Validation failed - please check data file path and format\")\n",
    "    \n",
    "    print(f\"\\n\\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
