{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to update indicators with full calculations and run validation\n",
    "Phase 1 - Step 1: Fix indicator calculations and validate\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Add regime_system to path\n",
    "sys.path.insert(0, 'regime_system')\n",
    "\n",
    "# First, backup the existing indicators.py\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: INDICATOR RESTORATION AND VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Step 1: Backup existing file\n",
    "print(\"\\nStep 1: Creating backup of existing indicators.py...\")\n",
    "backup_path = f\"regime_system/core/indicators_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py\"\n",
    "try:\n",
    "    import shutil\n",
    "    shutil.copy2('regime_system/core/indicators.py', backup_path)\n",
    "    print(f\"✓ Backup created: {backup_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Backup failed: {e}\")\n",
    "\n",
    "# Step 2: Import the restored indicators from our artifact\n",
    "print(\"\\nStep 2: Loading restored indicator calculations...\")\n",
    "# In practice, you would copy the restored_indicators content to indicators.py\n",
    "# For now, let's validate what we have\n",
    "\n",
    "from regime_system.core.data_loader import load_csv_data\n",
    "from regime_system.core.indicators import calculate_all_indicators, validate_indicators\n",
    "from regime_system.core.regime_classifier import RollingRegimeClassifier\n",
    "\n",
    "# Step 3: Run indicator validation\n",
    "print(\"\\nStep 3: Running indicator validation...\")\n",
    "\n",
    "def run_indicator_validation(data_file: str = None):\n",
    "    \"\"\"Run comprehensive indicator validation\"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    if data_file and os.path.exists(data_file):\n",
    "        print(f\"Loading data from: {data_file}\")\n",
    "        data = load_csv_data(data_file, timeframe='15min')\n",
    "    else:\n",
    "        print(\"Generating synthetic test data...\")\n",
    "        # Generate synthetic data for testing\n",
    "        dates = pd.date_range(start='2023-01-01', periods=1000, freq='15min')\n",
    "        data = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'open': 100 + np.cumsum(np.random.randn(1000) * 0.5),\n",
    "            'high': 0,\n",
    "            'low': 0,\n",
    "            'close': 0,\n",
    "            'volume': np.random.randint(1000, 10000, 1000)\n",
    "        })\n",
    "        data['high'] = data['open'] + np.abs(np.random.randn(1000) * 0.3)\n",
    "        data['low'] = data['open'] - np.abs(np.random.randn(1000) * 0.3)\n",
    "        data['close'] = data['low'] + (data['high'] - data['low']) * np.random.rand(1000)\n",
    "        data.set_index('Date', inplace=True)\n",
    "    \n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    \n",
    "    # Calculate indicators\n",
    "    print(\"\\nCalculating indicators...\")\n",
    "    try:\n",
    "        data_with_indicators = calculate_all_indicators(data, verbose=True)\n",
    "        indicator_count = len(data_with_indicators.columns) - len(data.columns)\n",
    "        print(f\"\\n✓ Successfully calculated {indicator_count} indicators\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error calculating indicators: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Validate indicators\n",
    "    print(\"\\nValidating indicators...\")\n",
    "    validation_results = validate_indicators(data_with_indicators)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"  Valid indicators: {len(validation_results['valid'])}\")\n",
    "    print(f\"  Missing indicators: {len(validation_results['missing'])}\")\n",
    "    print(f\"  All NaN indicators: {len(validation_results['all_nan'])}\")\n",
    "    print(f\"  Mostly NaN indicators: {len(validation_results['mostly_nan'])}\")\n",
    "    print(f\"  Constant indicators: {len(validation_results['constant'])}\")\n",
    "    \n",
    "    if validation_results['missing']:\n",
    "        print(f\"\\n  Missing: {validation_results['missing'][:5]}{'...' if len(validation_results['missing']) > 5 else ''}\")\n",
    "    \n",
    "    if validation_results['all_nan']:\n",
    "        print(f\"\\n  All NaN: {validation_results['all_nan'][:5]}{'...' if len(validation_results['all_nan']) > 5 else ''}\")\n",
    "    \n",
    "    return data_with_indicators, validation_results\n",
    "\n",
    "def check_confidence_scores(data_with_indicators):\n",
    "    \"\"\"Check if confidence scores are properly varied\"\"\"\n",
    "    print(\"\\nStep 4: Testing regime classification confidence scores...\")\n",
    "    \n",
    "    # Create classifier\n",
    "    classifier = RollingRegimeClassifier(window_hours=36, timeframe='15min')\n",
    "    \n",
    "    # Classify regimes\n",
    "    print(\"Classifying regimes...\")\n",
    "    regimes = classifier.classify_regimes(data_with_indicators, show_progress=True)\n",
    "    \n",
    "    # Check confidence scores\n",
    "    print(\"\\nConfidence Score Analysis:\")\n",
    "    for dim in ['Direction', 'TrendStrength', 'Velocity', 'Volatility', 'Microstructure']:\n",
    "        conf_col = f'{dim}_Confidence'\n",
    "        if conf_col in regimes.columns:\n",
    "            conf_values = regimes[conf_col].dropna()\n",
    "            print(f\"\\n{dim} Confidence:\")\n",
    "            print(f\"  Mean: {conf_values.mean():.3f}\")\n",
    "            print(f\"  Std: {conf_values.std():.3f}\")\n",
    "            print(f\"  Min: {conf_values.min():.3f}\")\n",
    "            print(f\"  Max: {conf_values.max():.3f}\")\n",
    "            print(f\"  Unique values: {len(conf_values.unique())}\")\n",
    "            \n",
    "            # Check if all values are 1.0\n",
    "            if (conf_values == 1.0).all():\n",
    "                print(f\"  ⚠️ WARNING: All confidence scores are 1.0!\")\n",
    "            elif conf_values.std() < 0.01:\n",
    "                print(f\"  ⚠️ WARNING: Very low variance in confidence scores!\")\n",
    "            else:\n",
    "                print(f\"  ✓ Confidence scores show proper variation\")\n",
    "    \n",
    "    return regimes\n",
    "\n",
    "def analyze_indicator_correlations(data_with_indicators):\n",
    "    \"\"\"Analyze correlations between indicators\"\"\"\n",
    "    print(\"\\nStep 5: Analyzing indicator correlations...\")\n",
    "    \n",
    "    # Select only indicator columns (exclude OHLCV)\n",
    "    base_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    indicator_cols = [col for col in data_with_indicators.columns if col not in base_cols]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    print(f\"Calculating correlations for {len(indicator_cols)} indicators...\")\n",
    "    corr_matrix = data_with_indicators[indicator_cols].corr()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    high_corr_threshold = 0.90\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(indicator_cols)):\n",
    "        for j in range(i+1, len(indicator_cols)):\n",
    "            corr_value = abs(corr_matrix.iloc[i, j])\n",
    "            if corr_value > high_corr_threshold and not pd.isna(corr_value):\n",
    "                high_corr_pairs.append({\n",
    "                    'indicator1': indicator_cols[i],\n",
    "                    'indicator2': indicator_cols[j],\n",
    "                    'correlation': corr_value\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nFound {len(high_corr_pairs)} indicator pairs with correlation > {high_corr_threshold}\")\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        # Sort by correlation\n",
    "        high_corr_pairs.sort(key=lambda x: x['correlation'], reverse=True)\n",
    "        print(\"\\nTop 10 highly correlated pairs:\")\n",
    "        for i, pair in enumerate(high_corr_pairs[:10]):\n",
    "            print(f\"  {i+1}. {pair['indicator1']} <-> {pair['indicator2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # Group by indicator\n",
    "        redundant_indicators = {}\n",
    "        for pair in high_corr_pairs:\n",
    "            for ind in [pair['indicator1'], pair['indicator2']]:\n",
    "                if ind not in redundant_indicators:\n",
    "                    redundant_indicators[ind] = 0\n",
    "                redundant_indicators[ind] += 1\n",
    "        \n",
    "        # Find most redundant indicators\n",
    "        most_redundant = sorted(redundant_indicators.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print(\"\\nMost redundant indicators (appear in many high-correlation pairs):\")\n",
    "        for ind, count in most_redundant:\n",
    "            print(f\"  {ind}: appears in {count} high-correlation pairs\")\n",
    "    \n",
    "    return corr_matrix, high_corr_pairs\n",
    "\n",
    "def create_validation_report(validation_results, regimes, high_corr_pairs):\n",
    "    \"\"\"Create a comprehensive validation report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'indicators': {\n",
    "            'total_calculated': len(validation_results['valid']) + len(validation_results['all_nan']) + \n",
    "                              len(validation_results['mostly_nan']) + len(validation_results['constant']),\n",
    "            'valid': len(validation_results['valid']),\n",
    "            'problematic': len(validation_results['all_nan']) + len(validation_results['mostly_nan']) + \n",
    "                          len(validation_results['constant']),\n",
    "            'missing_expected': len(validation_results['missing'])\n",
    "        },\n",
    "        'confidence_scores': {\n",
    "            'all_ones_issue': False,\n",
    "            'low_variance_issue': False,\n",
    "            'dimensions_checked': 5\n",
    "        },\n",
    "        'correlations': {\n",
    "            'high_correlation_pairs': len(high_corr_pairs),\n",
    "            'threshold_used': 0.90\n",
    "        },\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Check confidence score issues\n",
    "    for dim in ['Direction', 'TrendStrength', 'Velocity', 'Volatility', 'Microstructure']:\n",
    "        conf_col = f'{dim}_Confidence'\n",
    "        if conf_col in regimes.columns:\n",
    "            conf_values = regimes[conf_col].dropna()\n",
    "            if (conf_values == 1.0).all():\n",
    "                report['confidence_scores']['all_ones_issue'] = True\n",
    "            elif conf_values.std() < 0.01:\n",
    "                report['confidence_scores']['low_variance_issue'] = True\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if report['confidence_scores']['all_ones_issue']:\n",
    "        report['recommendations'].append(\"CRITICAL: Update indicator calculations to use full formulas instead of simplified versions\")\n",
    "    \n",
    "    if report['indicators']['problematic'] > 10:\n",
    "        report['recommendations'].append(\"HIGH: Review and fix indicators that are returning NaN or constant values\")\n",
    "    \n",
    "    if report['correlations']['high_correlation_pairs'] > 20:\n",
    "        report['recommendations'].append(\"MEDIUM: Consider removing redundant indicators with correlation > 0.90\")\n",
    "    \n",
    "    if report['indicators']['missing_expected'] > 0:\n",
    "        report['recommendations'].append(\"LOW: Some expected indicators are missing from calculations\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSUMMARY:\")\n",
    "    print(f\"✓ Total indicators calculated: {report['indicators']['total_calculated']}\")\n",
    "    print(f\"{'✓' if not report['confidence_scores']['all_ones_issue'] else '✗'} Confidence scores: {'Properly varied' if not report['confidence_scores']['all_ones_issue'] else 'All showing 1.0 (needs fix)'}\")\n",
    "    print(f\"⚠️  High correlation pairs: {report['correlations']['high_correlation_pairs']}\")\n",
    "    \n",
    "    print(\"\\nRECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(report['recommendations'], 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Save report\n",
    "    report_file = f\"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(\"REGIME SYSTEM VALIDATION REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"Generated: {report['timestamp']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"INDICATOR ANALYSIS:\\n\")\n",
    "        f.write(f\"  Total Calculated: {report['indicators']['total_calculated']}\\n\")\n",
    "        f.write(f\"  Valid: {report['indicators']['valid']}\\n\")\n",
    "        f.write(f\"  Problematic: {report['indicators']['problematic']}\\n\")\n",
    "        f.write(f\"  Missing Expected: {report['indicators']['missing_expected']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"CONFIDENCE SCORES:\\n\")\n",
    "        f.write(f\"  All 1.0 Issue: {'Yes' if report['confidence_scores']['all_ones_issue'] else 'No'}\\n\")\n",
    "        f.write(f\"  Low Variance Issue: {'Yes' if report['confidence_scores']['low_variance_issue'] else 'No'}\\n\\n\")\n",
    "        \n",
    "        f.write(\"CORRELATIONS:\\n\")\n",
    "        f.write(f\"  High Correlation Pairs (>{report['correlations']['threshold_used']}): {report['correlations']['high_correlation_pairs']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"RECOMMENDATIONS:\\n\")\n",
    "        for rec in report['recommendations']:\n",
    "            f.write(f\"  - {rec}\\n\")\n",
    "    \n",
    "    print(f\"\\n✓ Report saved to: {report_file}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nRunning comprehensive validation...\")\n",
    "    \n",
    "    # Step 1: Validate indicators\n",
    "    data_with_indicators, validation_results = run_indicator_validation()\n",
    "    \n",
    "    if data_with_indicators is not None:\n",
    "        # Step 2: Check confidence scores\n",
    "        regimes = check_confidence_scores(data_with_indicators)\n",
    "        \n",
    "        # Step 3: Analyze correlations\n",
    "        corr_matrix, high_corr_pairs = analyze_indicator_correlations(data_with_indicators)\n",
    "        \n",
    "        # Step 4: Create report\n",
    "        report = create_validation_report(validation_results, regimes, high_corr_pairs)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"NEXT STEPS:\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"1. Replace regime_system/core/indicators.py with the full implementation\")\n",
    "        print(\"2. Re-run this validation to confirm confidence scores are fixed\")\n",
    "        print(\"3. Remove highly correlated indicators based on the analysis\")\n",
    "        print(\"4. Proceed with regime distribution validation\")\n",
    "        print(\"5. Run performance attribution analysis\")\n",
    "    else:\n",
    "        print(\"\\n✗ Validation failed - please check indicator calculations\")\n",
    "    \n",
    "    print(f\"\\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
