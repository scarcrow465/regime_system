{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import argparse  # For parameterization\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "if not args.verbose:\n",
    "    logger.setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add paths (make configurable)\n",
    "sys.path.insert(0, r'C:\\Users\\rs\\GitProjects\\regime_system\\ob_model\\v2.0_precloud_reorganization_and_clean')\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from core.data_loader import load_csv_data\n",
    "from core.indicators import calculate_all_indicators\n",
    "from daily_regime_classifier import NQDailyRegimeClassifier\n",
    "from hourly_early_warning_system_grok import LowerTimeframeEarlyWarningSystem  # Updated import\n",
    "\n",
    "# Argument parser for configs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--daily_path', default='combined_NQ_daily_data.csv')\n",
    "parser.add_argument('--ltf_path', default='combined_NQ_1h_data.csv')  # Assume 1H input\n",
    "parser.add_argument('--timeframes', nargs='+', default=['1H', '4H', '8H'])  # Multiple TFs\n",
    "parser.add_argument('--lookback_days', type=int, default=252)\n",
    "parser.add_argument('--walk_forward', action='store_true', default=False)  # Toggle walk-forward mode\n",
    "args = parser.parse_args()\n",
    "parser.add_argument('--verbose', action='store_true', default=False)  # Toggle verbose logs\n",
    "parser.add_argument('--tuning_verbose', action='store_true', default=False)  # Toggle tuning diagnostics\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LTF EARLY WARNING SYSTEM TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading daily data...\")\n",
    "daily_data = load_csv_data(args.daily_path, timeframe='1d').tail(args.lookback_days * 2)\n",
    "print(f\"Loaded {len(daily_data)} daily bars\")\n",
    "\n",
    "print(\"\\nLoading LTF (1H) data...\")\n",
    "ltf_data = load_csv_data(args.ltf_path, timeframe='60min')\n",
    "ltf_data = ltf_data[ltf_data.index.date >= daily_data.index[0].date()]\n",
    "ltf_data = ltf_data[ltf_data.index.date <= daily_data.index[-1].date()]\n",
    "print(f\"Loaded {len(ltf_data)} 1H bars\")\n",
    "\n",
    "# Calculate indicators\n",
    "print(\"\\nCalculating daily indicators...\")\n",
    "daily_with_indicators = calculate_all_indicators(daily_data, verbose=False)\n",
    "\n",
    "# Initialize classifier\n",
    "daily_classifier = NQDailyRegimeClassifier(lookback_days=args.lookback_days)\n",
    "daily_regimes = daily_classifier.classify_regimes(daily_with_indicators)\n",
    "\n",
    "thresh = daily_classifier.config['thresholds']  # Assuming thresh is defined in classifier config; if not, add this line if needed\n",
    "\n",
    "if args.tuning_verbose:\n",
    "    print(\"\\n============================================================\\nTHRESHOLD DIAGNOSTICS FOR TUNING\\n============================================================\")\n",
    "\n",
    "    # Direction Scores Distribution\n",
    "    print(\"\\nDirection Scores Distribution:\")\n",
    "    print(f\"  Mean: {daily_with_indicators['direction_score'].mean():.3f}\")\n",
    "    print(f\"  Std: {daily_with_indicators['direction_score'].std():.3f}\")\n",
    "    print(f\"  Min: {daily_with_indicators['direction_score'].min():.3f}\")\n",
    "    print(f\"  Max: {daily_with_indicators['direction_score'].max():.3f}\")\n",
    "    percentiles = daily_with_indicators['direction_score'].quantile([0.1, 0.25, 0.75, 0.9])\n",
    "    print(f\"  Percentiles: 10%={percentiles[0.1]:.3f}, 25%={percentiles[0.25]:.3f}, 75%={percentiles[0.75]:.3f}, 90%={percentiles[0.9]:.3f}\")\n",
    "    print(f\"  Current thresholds: strong={thresh['direction_strong']}, neutral={thresh['direction_neutral']}\")\n",
    "\n",
    "    for strong_thresh in [0.05, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]:\n",
    "        up_pct = (daily_with_indicators['direction_score'] > strong_thresh).mean() * 100\n",
    "        down_pct = (daily_with_indicators['direction_score'] < -strong_thresh).mean() * 100\n",
    "        sideways_pct = 100 - up_pct - down_pct\n",
    "        print(f\"    If strong={strong_thresh}: Up={up_pct:.1f}%, Down={down_pct:.1f}%, Sideways={sideways_pct:.1f}%\")\n",
    "\n",
    "    # Strength Scores Distribution\n",
    "    print(\"\\nStrength Scores Distribution:\")\n",
    "    print(f\"  Mean: {daily_with_indicators['strength_score'].mean():.3f}\")\n",
    "    print(f\"  Std: {daily_with_indicators['strength_score'].std():.3f}\")\n",
    "    percentiles = daily_with_indicators['strength_score'].quantile([0.25, 0.5, 0.75, 0.9])\n",
    "    print(f\"  Percentiles: 25%={percentiles[0.25]:.3f}, 50%={percentiles[0.5]:.3f}, 75%={percentiles[0.75]:.3f}, 90%={percentiles[0.9]:.3f}\")\n",
    "    print(f\"  Current thresholds: strong={thresh['strength_strong']}, moderate={thresh['strength_moderate']}\")\n",
    "\n",
    "    # Efficiency Ratio Distribution\n",
    "    print(\"\\nEfficiency Ratio Distribution:\")\n",
    "    print(f\"  Mean: {daily_with_indicators['efficiency_ratio'].mean():.3f}\")\n",
    "    print(f\"  Std: {daily_with_indicators['efficiency_ratio'].std():.3f}\")\n",
    "    percentiles = daily_with_indicators['efficiency_ratio'].quantile([0.25, 0.5, 0.75])\n",
    "    print(f\"  Percentiles: 25%={percentiles[0.25]:.3f}, 50%={percentiles[0.5]:.3f}, 75%={percentiles[0.75]:.3f}\")\n",
    "    print(f\"  Current thresholds: trending={thresh['efficiency_trending']}, ranging={thresh['efficiency_ranging']}\")\n",
    "\n",
    "    for trending_thresh in [0.05, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]:\n",
    "        trending_pct = (daily_with_indicators['efficiency_ratio'] > trending_thresh).mean() * 100\n",
    "        ranging_pct = (daily_with_indicators['efficiency_ratio'] < thresh['efficiency_ranging']).mean() * 100\n",
    "        print(f\"    If trending={trending_thresh}: Trending={trending_pct:.1f}%, Ranging={ranging_pct:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nTuning diagnostics skipped (use --tuning_verbose to enable)\")\n",
    "\n",
    "if not args.walk_forward:\n",
    "    # Batch mode\n",
    "    ensemble_divergences = []\n",
    "    divergences_dict = {}  # Store per TF\n",
    "    for tf in tqdm(args.timeframes, desc=\"Processing timeframes\", unit=\"TF\"):\n",
    "        print(f\"\\nProcessing {tf} timeframe...\")\n",
    "        ews = LowerTimeframeEarlyWarningSystem(daily_classifier, timeframe=tf)\n",
    "        \n",
    "        print(f\"Calculating {tf} indicators...\")\n",
    "        ltf_with_indicators = calculate_all_indicators(ltf_data, verbose=False)\n",
    "        \n",
    "        divergences = ews.detect_divergences(daily_regimes, ltf_with_indicators)\n",
    "        ensemble_divergences.append(divergences['divergence_score'])\n",
    "        divergences_dict[tf] = divergences\n",
    "        \n",
    "        # Save per TF\n",
    "        divergences.to_csv(f'{tf}_divergences_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "\n",
    "    # Ensemble average\n",
    "    avg_divergence = pd.concat(ensemble_divergences, axis=1).mean(axis=1)\n",
    "    print(\"\\nEnsemble Average Divergence Score (across TFs):\")\n",
    "    print(avg_divergence.tail())\n",
    "\n",
    "    # Use last TF for detailed analysis\n",
    "    divergences = divergences_dict[args.timeframes[-1]]\n",
    "\n",
    "    # DIVERGENCE ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIVERGENCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Overall divergence statistics\n",
    "    total_periods = len(divergences)\n",
    "    direction_div_pct = divergences['direction_divergence'].mean() * 100\n",
    "    strength_div_pct = divergences['strength_divergence'].mean() * 100\n",
    "    volatility_div_pct = divergences['volatility_divergence'].mean() * 100\n",
    "    character_div_pct = divergences['character_divergence'].mean() * 100\n",
    "\n",
    "    print(f\"\\nOverall Divergence Rates:\")\n",
    "    print(f\"  Direction: {direction_div_pct:.1f}% of periods\")\n",
    "    print(f\"  Strength: {strength_div_pct:.1f}% of periods\")\n",
    "    print(f\"  Volatility: {volatility_div_pct:.1f}% of periods\")\n",
    "    print(f\"  Character: {character_div_pct:.1f}% of periods\")\n",
    "\n",
    "    # Summarize high divergence periods\n",
    "    high_div_threshold = 0.5  # 50% divergence\n",
    "    recent_window = 24 * 7  # Last week\n",
    "\n",
    "    print(f\"\\nSummary of High Divergence Periods (>{high_div_threshold*100}% in {recent_window} period window):\")\n",
    "    divergences['rolling_div_score'] = divergences['divergence_score'].rolling(recent_window).mean()\n",
    "    high_div_periods = divergences[divergences['rolling_div_score'] > high_div_threshold]\n",
    "\n",
    "    if len(high_div_periods) > 0:\n",
    "        # Group consecutive periods\n",
    "        high_div_periods['group'] = (high_div_periods.index.to_series().diff() > pd.Timedelta(hours=1)).cumsum()\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        num_periods = high_div_periods['group'].nunique()\n",
    "        avg_div_score = high_div_periods['divergence_score'].mean() * 100\n",
    "        div_score_std = high_div_periods['divergence_score'].std() * 100\n",
    "        div_score_percentiles = high_div_periods['divergence_score'].quantile([0.25, 0.5, 0.75]) * 100\n",
    "        \n",
    "        # Count successful predictions\n",
    "        successful_predictions = 0\n",
    "        for group_id, group in high_div_periods.groupby('group'):\n",
    "            start = group.index[0]\n",
    "            daily_date = start.date()\n",
    "            if daily_date in daily_regimes.index.date:\n",
    "                daily_idx = daily_regimes.index.get_loc(pd.Timestamp(daily_date))\n",
    "                if daily_idx < len(daily_regimes) - 1:\n",
    "                    current_regime = daily_regimes.iloc[daily_idx]['composite_regime']\n",
    "                    next_regime = daily_regimes.iloc[daily_idx + 1]['composite_regime']\n",
    "                    if current_regime != next_regime:\n",
    "                        successful_predictions += 1\n",
    "        \n",
    "        success_rate = (successful_predictions / num_periods * 100) if num_periods > 0 else 0\n",
    "        \n",
    "        print(f\"  Total High Divergence Periods: {num_periods}\")\n",
    "        print(f\"  Average Divergence Score: {avg_div_score:.1f}%\")\n",
    "        print(f\"  Divergence Score Std Dev: {div_score_std:.1f}%\")\n",
    "        print(f\"  Divergence Score Percentiles: 25%={div_score_percentiles[0.25]:.1f}%, 50%={div_score_percentiles[0.5]:.1f}%, 75%={div_score_percentiles[0.75]:.1f}%\")\n",
    "        print(f\"  Periods Leading to Regime Change: {successful_predictions} ({success_rate:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  No high divergence periods found.\")\n",
    "\n",
    "    # REGIME CHANGE PREDICTION ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"REGIME CHANGE PREDICTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Find all daily regime changes\n",
    "    daily_regime_changes = daily_regimes['composite_regime'] != daily_regimes['composite_regime'].shift(1)\n",
    "    change_dates = daily_regimes[daily_regime_changes].index[1:]  # Skip first\n",
    "\n",
    "    print(f\"\\nFound {len(change_dates)} daily regime changes\")\n",
    "\n",
    "    # Check if LTF divergences preceded each change\n",
    "    lead_times = []\n",
    "    prediction_success = []\n",
    "\n",
    "    for change_date in change_dates[-10:]:  # Last 10 changes\n",
    "        # Look at 48 hours before the change\n",
    "        start_check = change_date - pd.Timedelta(hours=48)\n",
    "        end_check = change_date\n",
    "        \n",
    "        # Get divergences in this window\n",
    "        window_div = divergences[(divergences.index >= start_check) & (divergences.index < end_check)]\n",
    "        \n",
    "        if len(window_div) > 0:\n",
    "            # Calculate average divergence in windows\n",
    "            div_24h = window_div.iloc[-24:]['divergence_score'].mean() if len(window_div) >= 24 else 0\n",
    "            div_48h = window_div['divergence_score'].mean()\n",
    "            \n",
    "            # Find first significant divergence\n",
    "            significant_div = window_div[window_div['divergence_score'] > 0.4]\n",
    "            if len(significant_div) > 0:\n",
    "                first_warning = significant_div.index[0]\n",
    "                lead_time = (change_date - first_warning).total_seconds() / 3600\n",
    "                lead_times.append(lead_time)\n",
    "                prediction_success.append(True)\n",
    "                \n",
    "                print(f\"\\n  {change_date.strftime('%Y-%m-%d')}:\")\n",
    "                print(f\"    Lead time: {lead_time:.1f} hours\")\n",
    "                print(f\"    24h divergence: {div_24h*100:.0f}%\")\n",
    "                print(f\"    48h divergence: {div_48h*100:.0f}%\")\n",
    "            else:\n",
    "                prediction_success.append(False)\n",
    "                print(f\"\\n  {change_date.strftime('%Y-%m-%d')}: No significant warning\")\n",
    "\n",
    "    if lead_times:\n",
    "        print(f\"\\nPrediction Statistics:\")\n",
    "        print(f\"  Success rate: {sum(prediction_success)/len(prediction_success)*100:.0f}%\")\n",
    "        print(f\"  Average lead time: {np.mean(lead_times):.1f} hours\")\n",
    "        print(f\"  Median lead time: {np.median(lead_times):.1f} hours\")\n",
    "\n",
    "    # CURRENT WARNINGS with escalation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CURRENT WARNINGS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    current_warnings = ews.generate_warnings(divergences, lookback_periods=24)\n",
    "\n",
    "    # Escalate levels if multiple TFs and avg_divergence >0.6\n",
    "    escalate = len(args.timeframes) > 1 and avg_divergence.mean() > 0.6\n",
    "    level_map = {'WEAK': 'MODERATE', 'MODERATE': 'STRONG', 'STRONG': 'CRITICAL', 'CRITICAL': 'CRITICAL'}  # No higher than CRITICAL\n",
    "\n",
    "    if escalate:\n",
    "        for warning in current_warnings:\n",
    "            if warning['level'] in level_map:\n",
    "                warning['level'] = level_map[warning['level']]\n",
    "                warning['message'] += \" (Escalated due to multi-TF consensus)\"\n",
    "\n",
    "    if current_warnings:\n",
    "        for warning in current_warnings:\n",
    "            print(f\"\\n{warning['level']} WARNING - {warning['type'].upper()}:\")\n",
    "            print(f\"  {warning['message']}\")\n",
    "            if 'divergence_pct' in warning:\n",
    "                print(f\"  Divergence: {warning['divergence_pct']:.0f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo significant warnings at this time\")\n",
    "\n",
    "    # Create visualization\n",
    "    print(\"\\nCreating visualization...\")\n",
    "\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "    # Plot 1: Price with regime changes\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(ltf_data.index, ltf_data['close'], 'k-', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "    # Mark daily regime changes\n",
    "    for change_date in change_dates:\n",
    "        ax1.axvline(x=change_date, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.set_title('NQ LTF Price with Daily Regime Changes (Red Lines)')\n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "    # Plot 2: Direction divergence\n",
    "    ax2 = axes[1]\n",
    "    ax2.fill_between(divergences.index, 0, divergences['direction_divergence'], \n",
    "                     alpha=0.5, color='blue', label='Direction Divergence')\n",
    "    ax2.set_ylabel('Divergence')\n",
    "    ax2.set_title('Direction Regime Divergence (LTF vs Daily)')\n",
    "    ax2.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    # Plot 3: Strength divergence\n",
    "    ax3 = axes[2]\n",
    "    ax3.fill_between(divergences.index, 0, divergences['strength_divergence'], \n",
    "                     alpha=0.5, color='orange', label='Strength Divergence')\n",
    "    ax3.set_ylabel('Divergence')\n",
    "    ax3.set_title('Strength Regime Divergence')\n",
    "    ax3.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    # Plot 4: Volatility divergence\n",
    "    ax4 = axes[3]\n",
    "    ax4.fill_between(divergences.index, 0, divergences['volatility_divergence'], \n",
    "                     alpha=0.5, color='red', label='Volatility Divergence')\n",
    "    ax4.set_ylabel('Divergence')\n",
    "    ax4.set_title('Volatility Regime Divergence')\n",
    "    ax4.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    # Plot 5: Composite divergence score\n",
    "    ax5 = axes[4]\n",
    "    ax5.plot(divergences.index, divergences['divergence_score'], 'purple', linewidth=1)\n",
    "    ax5.fill_between(divergences.index, 0, divergences['divergence_score'], \n",
    "                     alpha=0.3, color='purple')\n",
    "\n",
    "    # Add warning level lines\n",
    "    ax5.axhline(y=0.3, color='yellow', linestyle='--', alpha=0.5, label='Weak Warning')\n",
    "    ax5.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate Warning')\n",
    "    ax5.axhline(y=0.7, color='red', linestyle='--', alpha=0.5, label='Strong Warning')\n",
    "\n",
    "    ax5.set_ylabel('Score')\n",
    "    ax5.set_xlabel('Date')\n",
    "    ax5.set_title('Composite Divergence Score')\n",
    "    ax5.set_ylim(0, 1)\n",
    "    ax5.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'ltf_early_warning_{datetime.now().strftime(\"%Y%m%d\")}.png', dpi=150)\n",
    "    print(\"✓ Saved divergence chart\")\n",
    "\n",
    "    # Save divergence data\n",
    "    output_file = f'ltf_divergences_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    divergences.to_csv(output_file)\n",
    "    print(f\"\\n✓ Divergence data saved to: {output_file}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EARLY WARNING SYSTEM INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\n1. DIVERGENCE PATTERNS:\")\n",
    "    print(\"   - Normal divergence rate: 20-30% is healthy\")\n",
    "    print(\"   - >50% sustained divergence often precedes regime change\")\n",
    "    print(\"   - Direction divergence is most predictive\")\n",
    "\n",
    "    print(\"\\n2. TYPICAL LEAD TIMES:\")\n",
    "    print(\"   - Minor regime adjustments: 6-12 hours warning\")\n",
    "    print(\"   - Major regime changes: 24-48 hours warning\")\n",
    "    print(\"   - Crisis/volatile transitions: Can be sudden (<6 hours)\")\n",
    "\n",
    "    print(\"\\n3. USAGE RECOMMENDATIONS:\")\n",
    "    print(\"   - Monitor composite score >0.5 for potential changes\")\n",
    "    print(\"   - Direction divergence >70% = high probability of trend change\")\n",
    "    print(\"   - Multiple divergences = higher confidence signal\")\n",
    "\n",
    "\n",
    "else:\n",
    "    # Walk-forward mode (standalone)\n",
    "    print(\"\\nRunning Walk-Forward Testing (Standalone)...\")\n",
    "    # Split data: Train on data up to 2024-01-01, test forward\n",
    "    train_end = pd.Timestamp('2024-01-01')\n",
    "    train_ltf = ltf_data[ltf_data.index < train_end]\n",
    "    test_ltf = ltf_data[ltf_data.index >= train_end].sort_index()\n",
    "    \n",
    "    # To avoid look-ahead, recalculate daily_regimes on train only initially\n",
    "    train_daily = daily_data[daily_data.index < train_end]\n",
    "    train_daily_with_indicators = calculate_all_indicators(train_daily, verbose=False)\n",
    "    daily_classifier = NQDailyRegimeClassifier(lookback_days=args.lookback_days)\n",
    "    current_daily_regimes = daily_classifier.classify_regimes(train_daily_with_indicators)\n",
    "    \n",
    "    # Initialize ews for each TF\n",
    "    ews_list = [LowerTimeframeEarlyWarningSystem(daily_classifier, timeframe=tf) for tf in args.timeframes]\n",
    "    \n",
    "    # Incremental loop: Process test data hour by hour\n",
    "    predictions = []  # Log (time, predicted_shift, actual_shift)\n",
    "    last_daily_close = train_end - pd.Timedelta(days=1)  # Start from last train day\n",
    "    cumulative_daily = train_daily.copy()\n",
    "    for i in tqdm(range(len(test_ltf)), desc=\"Processing test bars\", unit=\"bar\"):\n",
    "        new_bar = test_ltf.iloc[i:i+1]\n",
    "        current_time = new_bar.index[0]\n",
    "        \n",
    "        # Update daily regimes if a new day has closed\n",
    "        if current_time.hour >= 16 and current_time.date() > last_daily_close.date():\n",
    "            new_daily_date = current_time.date()\n",
    "            if new_daily_date in daily_data.index.date:\n",
    "                new_daily_bar = daily_data.loc[pd.Timestamp(new_daily_date)]\n",
    "                cumulative_daily = pd.concat([cumulative_daily, new_daily_bar.to_frame().T])\n",
    "                if len(cumulative_daily) >= 14:  # Min for ADX\n",
    "                    cumulative_daily = cumulative_daily.fillna(method='ffill').dropna(subset=['high', 'low', 'close'])\n",
    "                    cumulative_daily_with_indicators = calculate_all_indicators(cumulative_daily, verbose=False)\n",
    "                    current_daily_regimes = daily_classifier.classify_regimes(cumulative_daily_with_indicators)\n",
    "                last_daily_close = current_time\n",
    "        \n",
    "        # Only trigger LTF update if at session-aligned bar close (your times for 4H/8H)\n",
    "        close_hours = [22, 2, 6, 10, 14, 16]  # Your example closes; adjust as needed\n",
    "        if current_time.hour in close_hours:\n",
    "            # Update each LTF ews and collect divergences\n",
    "            divergences_list = []\n",
    "            warnings_list = []\n",
    "            new_daily_bar = current_daily_regimes.iloc[-1] if len(current_daily_regimes) > 0 else None\n",
    "            for ews in ews_list:\n",
    "                warnings, divergences = ews.update(new_bar.iloc[0], new_daily_bar)\n",
    "                divergences_list.append(divergences)  # Store full for later\n",
    "                warnings_list.extend(warnings)\n",
    "            \n",
    "            # Calculate alignment weight (1.0 full agreement, lower for conflicts)\n",
    "            alignment_weight = 0.0\n",
    "            ltf_regimes = [d['ltf_regime'].iloc[-1] if len(d) > 0 else None for d in divergences_list]  # Get latest LTF regimes\n",
    "            daily_regime = new_daily_bar['composite_regime'] if new_daily_bar is not None else None\n",
    "            if daily_regime is not None:\n",
    "                agreement_count = sum(1 for ltf_reg in ltf_regimes if ltf_reg == daily_regime) / len(ltf_regimes)  # % agreement with D\n",
    "                alignment_weight = 0.5 + (0.5 * agreement_count)  # Base 0.5, +up to 0.5 for full alignment\n",
    "                if agreement_count < 0.5:  # If <50% agree (LTFs conflicting with D/HTF)\n",
    "                    alignment_weight *= 1.5  # Bonus for strong LTF cluster against D (your reversal case)\n",
    "                else:\n",
    "                    alignment_weight *= 0.8  # Penalty for isolated LTF divergence\n",
    "            \n",
    "            # Predicted shift with weighted consensus\n",
    "            avg_score = np.mean([d['divergence_score'].iloc[-1] if len(d) > 0 else 0 for d in divergences_list])\n",
    "            predicted_shift = (avg_score * alignment_weight) > 0.8\n",
    "            \n",
    "            # Actual shift: Check if tomorrow's regime differs (simulation uses full for \"actual\")\n",
    "            daily_date = current_time.date()\n",
    "            if daily_date in daily_regimes.index.date:\n",
    "                daily_idx = daily_regimes.index.get_loc(pd.Timestamp(daily_date))\n",
    "                if daily_idx < len(daily_regimes) - 1:\n",
    "                    current_regime = daily_regimes.iloc[daily_idx]['composite_regime']\n",
    "                    next_regime = daily_regimes.iloc[daily_idx + 1]['composite_regime']\n",
    "                    actual_shift = current_regime != next_regime\n",
    "                    predictions.append((current_time, predicted_shift, actual_shift))\n",
    "        else:\n",
    "            continue  # Skip prediction if not at close time\n",
    "    \n",
    "    # Analyze\n",
    "    df_pred = pd.DataFrame(predictions, columns=['time', 'predicted', 'actual'])\n",
    "    success_rate = (df_pred['predicted'] == df_pred['actual']).mean() * 100 if len(df_pred) > 0 else 0\n",
    "    print(f\"\\nWalk-Forward Success Rate: {success_rate:.1f}%\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(pd.crosstab(df_pred['predicted'], df_pred['actual'], rownames=['Predicted'], colnames=['Actual']))\n",
    "    \n",
    "    # Save predictions\n",
    "    df_pred.to_csv(f'walk_forward_predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "    print(\"\\n✓ Walk-forward predictions saved\")\n",
    "\n",
    "print(f\"\\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
