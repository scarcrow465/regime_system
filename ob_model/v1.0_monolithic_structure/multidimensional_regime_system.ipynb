{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e62fc7-df4b-48e9-bde5-5ebbc9ad0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTI-DIMENSIONAL REGIME CLASSIFICATION SYSTEM - 5 DIMENSIONS\n",
    "# Comprehensive institutional-level regime analysis as originally intended\n",
    "# Save this as: multidimensional_regime_system.py\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Union, Dict, Tuple, Any\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optimization Code\n",
    "from multi_objective_optimizer import (\n",
    "    run_regime_optimization,\n",
    "    OptimizationResults,\n",
    "    MultiObjectiveRegimeOptimizer,\n",
    "    print_optimization_results,\n",
    "    optimize_window_size,\n",
    "    WalkForwardOptimizer\n",
    ")\n",
    "\n",
    "# Technical Analysis Libraries\n",
    "from ta.trend import SMAIndicator, EMAIndicator, MACD, ADXIndicator, AroonIndicator, CCIIndicator, IchimokuIndicator, PSARIndicator, VortexIndicator\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator, ROCIndicator, TSIIndicator, UltimateOscillator, StochRSIIndicator\n",
    "from ta.volatility import AverageTrueRange, BollingerBands, KeltnerChannel, DonchianChannel, UlcerIndex\n",
    "from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator, MFIIndicator, AccDistIndexIndicator, EaseOfMovementIndicator, ForceIndexIndicator\n",
    "from ta.others import DailyReturnIndicator, CumulativeReturnIndicator\n",
    "\n",
    "# Technical Analysis Libraries - Compatible with ta 0.11.0\n",
    "from ta.trend import SMAIndicator, EMAIndicator, MACD, ADXIndicator, AroonIndicator, CCIIndicator, IchimokuIndicator, PSARIndicator, VortexIndicator\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator, ROCIndicator, TSIIndicator, UltimateOscillator, StochRSIIndicator\n",
    "from ta.volatility import AverageTrueRange, BollingerBands, KeltnerChannel, DonchianChannel, UlcerIndex\n",
    "from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator, MFIIndicator, AccDistIndexIndicator, EaseOfMovementIndicator, ForceIndexIndicator\n",
    "from ta.others import DailyReturnIndicator, CumulativeReturnIndicator\n",
    "\n",
    "# Configure enhanced logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('multidimensional_regime_debug.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=== MULTI-DIMENSIONAL REGIME CLASSIFICATION SYSTEM ===\")\n",
    "print(\"5 Separate Regime Dimensions with Democratic Voting\")\n",
    "print(\"Direction • Trend Strength • Velocity • Volatility • Microstructure\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CSV_FILES = [\n",
    "    r\"C:\\Users\\rs\\OneDrive\\Desktop\\Excel\\Data\\New Data\\7.1 Master 15m Data - Updated - Nearest Unadjusted - 2014_01_01 - 2025_04_01 .csv\",\n",
    "    r\"C:\\Users\\rs\\OneDrive\\Desktop\\Excel\\Data\\New Data\\7.2 Master 15m Data - Updated - Nearest Unadjusted - 2000_01_01 - 2013_12_31 .csv\"\n",
    "]\n",
    "\n",
    "SYMBOLS = [\"NQ\"]  \n",
    "\n",
    "# Training Period\n",
    "TRAIN_START_DATE = \"2022-01-01 00:00:00-05:00\"  \n",
    "TRAIN_END_DATE = \"2023-01-01 00:00:00-05:00\"    \n",
    "\n",
    "# Testing Period  \n",
    "TEST_START_DATE = \"2023-01-01 00:00:00-05:00\"\n",
    "TEST_END_DATE = \"2024-01-01 00:00:00-05:00\"\n",
    "\n",
    "# For initial development/testing, use training data\n",
    "START_DATE = TRAIN_START_DATE\n",
    "END_DATE = TRAIN_END_DATE\n",
    "\n",
    "TIMEFRAME = \"15min\"\n",
    "OUTPUT_DIR = \"multidimensional_regime_results\"\n",
    "\n",
    "# OPTIMIZATION SETTINGS\n",
    "RUN_OPTIMIZATION = True  \n",
    "OPTIMIZATION_ITERATIONS = 10\n",
    "\n",
    "# =============================================================================\n",
    "# WINDOW CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "WALK_FORWARD = True  # Enable walk-forward validation\n",
    "OPTIMIZE_WINDOW = False  # Enable window size optimization\n",
    "\n",
    "# For 15-minute data, use hours instead of days\n",
    "if TIMEFRAME == \"15min\":\n",
    "    ROLLING_WINDOW_HOURS = 36    # 8 hours of data (32 bars)\n",
    "    MIN_WINDOW_HOURS = 2        # 2 hours minimum (8 bars)\n",
    "    BARS_PER_HOUR = 4          # 4 bars per hour for 15-min\n",
    "elif TIMEFRAME == \"5min\":\n",
    "    ROLLING_WINDOW_HOURS = 4    # 4 hours of data (48 bars)\n",
    "    MIN_WINDOW_HOURS = 1        # 1 hour minimum (12 bars)\n",
    "    BARS_PER_HOUR = 12         # 12 bars per hour for 5-min\n",
    "elif TIMEFRAME == \"1H\":\n",
    "    ROLLING_WINDOW_HOURS = 48   # 2 days of data (48 bars)\n",
    "    MIN_WINDOW_HOURS = 12       # 12 hours minimum\n",
    "    BARS_PER_HOUR = 1          # 1 bar per hour\n",
    "else:  # Daily\n",
    "    ROLLING_WINDOW_DAYS = 60    # Use days for daily\n",
    "    MIN_WINDOW_DAYS = 20        # Use days for daily\n",
    "    BARS_PER_HOUR = None       # Not applicable for daily\n",
    "\n",
    "SMOOTHING_PERIODS = 2           # 2-period smoothing for intraday\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTION TO CALCULATE WINDOW SIZES\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_window_sizes(timeframe: str):\n",
    "    \"\"\"Calculate window sizes in bars based on timeframe\"\"\"\n",
    "    \n",
    "    if timeframe in [\"5min\", \"15min\", \"1H\"]:\n",
    "        # For intraday, use hours\n",
    "        rolling_bars = int(ROLLING_WINDOW_HOURS * BARS_PER_HOUR)\n",
    "        min_bars = int(MIN_WINDOW_HOURS * BARS_PER_HOUR)\n",
    "    else:\n",
    "        # For daily, use days directly\n",
    "        rolling_bars = int(ROLLING_WINDOW_DAYS)\n",
    "        min_bars = int(MIN_WINDOW_DAYS)\n",
    "    \n",
    "    return rolling_bars, min_bars\n",
    "\n",
    "# =============================================================================\n",
    "# REGIME TYPE DEFINITIONS - 5 DIMENSIONS\n",
    "# =============================================================================\n",
    "\n",
    "class DirectionRegime:\n",
    "    \"\"\"Direction regime classifications\"\"\"\n",
    "    UP_TRENDING = \"Up_Trending\"\n",
    "    DOWN_TRENDING = \"Down_Trending\"\n",
    "    SIDEWAYS = \"Sideways\"\n",
    "    UNDEFINED = \"Undefined\"\n",
    "\n",
    "class TrendStrengthRegime:\n",
    "    \"\"\"Trend strength regime classifications\"\"\"\n",
    "    STRONG = \"Strong\"\n",
    "    MODERATE = \"Moderate\"\n",
    "    WEAK = \"Weak\"\n",
    "    UNDEFINED = \"Undefined\"\n",
    "\n",
    "class VelocityRegime:\n",
    "    \"\"\"Velocity regime classifications\"\"\"\n",
    "    ACCELERATING = \"Accelerating\"\n",
    "    DECELERATING = \"Decelerating\"\n",
    "    STABLE = \"Stable\"\n",
    "    UNDEFINED = \"Undefined\"\n",
    "\n",
    "class VolatilityRegime:\n",
    "    \"\"\"Volatility regime classifications\"\"\"\n",
    "    LOW_VOL = \"Low_Vol\"\n",
    "    MEDIUM_VOL = \"Medium_Vol\"\n",
    "    HIGH_VOL = \"High_Vol\"\n",
    "    EXTREME_VOL = \"Extreme_Vol\"\n",
    "    UNDEFINED = \"Undefined\"\n",
    "\n",
    "class MicrostructureRegime:\n",
    "    \"\"\"Market microstructure regime classifications\"\"\"\n",
    "    INSTITUTIONAL_FLOW = \"Institutional_Flow\"\n",
    "    RETAIL_FLOW = \"Retail_Flow\"\n",
    "    BALANCED_FLOW = \"Balanced_Flow\"\n",
    "    LOW_PARTICIPATION = \"Low_Participation\"\n",
    "    UNDEFINED = \"Undefined\"\n",
    "\n",
    "@dataclass\n",
    "class DimensionalVote:\n",
    "    \"\"\"Vote for a specific regime dimension\"\"\"\n",
    "    dimension: str\n",
    "    indicator_name: str\n",
    "    regime_vote: str\n",
    "    confidence: float\n",
    "    value: float\n",
    "    threshold_info: Dict[str, Any] = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.dimension}Vote({self.indicator_name}: {self.regime_vote} @ {self.confidence:.2f})\"\n",
    "\n",
    "@dataclass\n",
    "class MultiDimensionalClassification:\n",
    "    \"\"\"Complete multi-dimensional regime classification\"\"\"\n",
    "    timestamp: pd.Timestamp\n",
    "    direction_regime: str\n",
    "    direction_confidence: float\n",
    "    trend_strength_regime: str\n",
    "    trend_strength_confidence: float\n",
    "    velocity_regime: str\n",
    "    velocity_confidence: float\n",
    "    volatility_regime: str\n",
    "    volatility_confidence: float\n",
    "    microstructure_regime: str\n",
    "    microstructure_confidence: float\n",
    "    composite_regime: str\n",
    "    composite_confidence: float\n",
    "    all_votes: List[DimensionalVote]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MultiRegime({self.composite_regime} @ {self.composite_confidence:.2f})\"\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class InstrumentRegimeParameters:\n",
    "    \"\"\"Store instrument-specific regime parameters\"\"\"\n",
    "    symbol: str\n",
    "    direction_thresholds: Dict[str, float]\n",
    "    trend_strength_thresholds: Dict[str, float]\n",
    "    velocity_thresholds: Dict[str, float]\n",
    "    volatility_thresholds: Dict[str, float]\n",
    "    microstructure_thresholds: Dict[str, float]\n",
    "    last_update: pd.Timestamp\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for storage\"\"\"\n",
    "        return {\n",
    "            'symbol': self.symbol,\n",
    "            'direction_thresholds': self.direction_thresholds,\n",
    "            'trend_strength_thresholds': self.trend_strength_thresholds,\n",
    "            'velocity_thresholds': self.velocity_thresholds,\n",
    "            'volatility_thresholds': self.volatility_thresholds,\n",
    "            'microstructure_thresholds': self.microstructure_thresholds,\n",
    "            'last_update': self.last_update.isoformat()\n",
    "        }\n",
    "\n",
    "class RegimeSmoother:\n",
    "    \"\"\"Smooth regime transitions to prevent whipsaws\"\"\"\n",
    "    \n",
    "    def __init__(self, confirmation_periods: int = 3):\n",
    "        self.confirmation_periods = confirmation_periods\n",
    "        self.regime_counters = {}\n",
    "        self.current_regimes = {}\n",
    "        \n",
    "    def smooth_regime(self, dimension: str, new_regime: str, \n",
    "                     timestamp: pd.Timestamp) -> Tuple[str, bool]:\n",
    "        \"\"\"\n",
    "        Apply regime smoothing logic\n",
    "        Returns: (regime_to_use, regime_changed)\n",
    "        \"\"\"\n",
    "        if dimension not in self.current_regimes:\n",
    "            # First time seeing this dimension\n",
    "            self.current_regimes[dimension] = new_regime\n",
    "            self.regime_counters[dimension] = 0\n",
    "            return new_regime, True\n",
    "            \n",
    "        current_regime = self.current_regimes[dimension]\n",
    "        \n",
    "        if new_regime == current_regime:\n",
    "            # Same regime, reset counter\n",
    "            self.regime_counters[dimension] = 0\n",
    "            return current_regime, False\n",
    "        else:\n",
    "            # Different regime, increment counter\n",
    "            self.regime_counters[dimension] += 1\n",
    "            \n",
    "            if self.regime_counters[dimension] >= self.confirmation_periods:\n",
    "                # Enough confirmations, switch regime\n",
    "                self.current_regimes[dimension] = new_regime\n",
    "                self.regime_counters[dimension] = 0\n",
    "                #logger.info(f\"{dimension} regime changed from {current_regime} to {new_regime} at {timestamp}\")\n",
    "                return new_regime, True\n",
    "            else:\n",
    "                # Not enough confirmations yet\n",
    "                return current_regime, False\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all counters and regimes\"\"\"\n",
    "        self.regime_counters = {}\n",
    "        self.current_regimes = {}\n",
    "\n",
    "class RollingRegimeClassifier:\n",
    "    \"\"\"\n",
    "    Point-in-time regime classification with rolling windows\n",
    "    Eliminates forward-looking bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_periods: int = 520, max_periods: int = 1560):\n",
    "        \"\"\"\n",
    "        Initialize rolling classifier\n",
    "        min_periods: 20 trading days * 26 bars/day = 520 bars\n",
    "        max_periods: 60 trading days * 26 bars/day = 1560 bars\n",
    "        \"\"\"\n",
    "        self.min_periods = min_periods\n",
    "        self.max_periods = max_periods\n",
    "        self.instrument_params = {}\n",
    "        \n",
    "    def calculate_rolling_percentiles(self, series: pd.Series, \n",
    "                                    percentiles: List[float],\n",
    "                                    min_periods: int = None) -> pd.DataFrame:\n",
    "        \"\"\"Calculate rolling percentiles for a series\"\"\"\n",
    "        if min_periods is None:\n",
    "            min_periods = self.min_periods\n",
    "            \n",
    "        # Create expanding window that grows to max_periods\n",
    "        window_sizes = pd.Series(range(len(series)))\n",
    "        window_sizes = window_sizes.clip(lower=min_periods, upper=self.max_periods)\n",
    "        \n",
    "        result_df = pd.DataFrame(index=series.index)\n",
    "        \n",
    "        for pct in percentiles:\n",
    "            pct_values = []\n",
    "            \n",
    "            for i in range(len(series)):\n",
    "                if i < min_periods - 1:\n",
    "                    pct_values.append(np.nan)\n",
    "                else:\n",
    "                    # Use expanding window up to max_periods\n",
    "                    window_size = int(window_sizes.iloc[i])\n",
    "                    start_idx = max(0, i - window_size + 1)\n",
    "                    window_data = series.iloc[start_idx:i+1]\n",
    "                    pct_values.append(np.percentile(window_data.dropna(), pct))\n",
    "            \n",
    "            result_df[f'p{int(pct)}'] = pct_values\n",
    "            \n",
    "        return result_df\n",
    "    \n",
    "    def calculate_rolling_stats(self, series: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"Calculate rolling statistics (mean, std, min, max)\"\"\"\n",
    "        window_sizes = pd.Series(range(len(series)))\n",
    "        window_sizes = window_sizes.clip(lower=self.min_periods, upper=self.max_periods)\n",
    "        \n",
    "        stats_df = pd.DataFrame(index=series.index)\n",
    "        stats = ['mean', 'std', 'min', 'max']\n",
    "        \n",
    "        for stat in stats:\n",
    "            stat_values = []\n",
    "            \n",
    "            for i in range(len(series)):\n",
    "                if i < self.min_periods - 1:\n",
    "                    stat_values.append(np.nan)\n",
    "                else:\n",
    "                    window_size = int(window_sizes.iloc[i])\n",
    "                    start_idx = max(0, i - window_size + 1)\n",
    "                    window_data = series.iloc[start_idx:i+1].dropna()\n",
    "                    \n",
    "                    if stat == 'mean':\n",
    "                        stat_values.append(window_data.mean())\n",
    "                    elif stat == 'std':\n",
    "                        stat_values.append(window_data.std())\n",
    "                    elif stat == 'min':\n",
    "                        stat_values.append(window_data.min())\n",
    "                    elif stat == 'max':\n",
    "                        stat_values.append(window_data.max())\n",
    "            \n",
    "            stats_df[stat] = stat_values\n",
    "            \n",
    "        return stats_df\n",
    "    \n",
    "    def normalize_rolling(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Normalize series using rolling min/max\"\"\"\n",
    "        stats = self.calculate_rolling_stats(series)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        range_vals = stats['max'] - stats['min']\n",
    "        range_vals = range_vals.replace(0, 1)\n",
    "        \n",
    "        normalized = (series - stats['min']) / range_vals\n",
    "        return normalized.fillna(0.5)  # Middle value for NaN\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE MONITORING (FROM YOUR WORKING SYSTEM)\n",
    "# =============================================================================\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Performance monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.timings = {}\n",
    "        self.memory_usage = {}\n",
    "        self.error_counts = {}\n",
    "        \n",
    "    def start_timer(self, operation: str):\n",
    "        self.timings[operation] = time.time()\n",
    "        \n",
    "    def end_timer(self, operation: str):\n",
    "        if operation in self.timings:\n",
    "            duration = time.time() - self.timings[operation]\n",
    "            logger.info(f\"TIMER: {operation} completed in {duration:.3f} seconds\")\n",
    "            return duration\n",
    "        return None\n",
    "        \n",
    "    def log_memory(self, operation: str):\n",
    "        memory_mb = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        self.memory_usage[operation] = memory_mb\n",
    "        logger.info(f\"MEMORY: Memory usage after {operation}: {memory_mb:.1f} MB\")\n",
    "\n",
    "perf_monitor = PerformanceMonitor()\n",
    "\n",
    "# =============================================================================\n",
    "# UPDATED MULTI-DIMENSIONAL REGIME CLASSIFIER - 80+ INDICATORS\n",
    "# Complete classification system for all 5 regime dimensions\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# ORIGINAL WORKING REGIME CLASSIFIER - EXPANDED TO 85 INDICATORS\n",
    "# Based on proven logic that produced realistic 44%/35%/20% distributions\n",
    "# =============================================================================\n",
    "\n",
    "class MultiDimensionalRegimeClassifier:\n",
    "    \"\"\"\n",
    "    Multi-Dimensional Regime Classification System with EFFICIENT Rolling Windows\n",
    "    Pre-calculates all rolling statistics for performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_periods: int = None, max_periods: int = None, \n",
    "                 smoothing_periods: int = 3, window_hours: float = None):\n",
    "        \"\"\"\n",
    "        Initialize with optional window_hours parameter for easy configuration\n",
    "        \"\"\"\n",
    "        # If window_hours is provided, calculate periods from it\n",
    "        if window_hours is not None:\n",
    "            if TIMEFRAME == \"15min\":\n",
    "                periods = int(window_hours * 4)  # 4 bars per hour\n",
    "            elif TIMEFRAME == \"5min\":\n",
    "                periods = int(window_hours * 12)  # 12 bars per hour\n",
    "            elif TIMEFRAME == \"1H\":\n",
    "                periods = int(window_hours * 1)  # 1 bar per hour\n",
    "            else:  # Daily\n",
    "                periods = int(window_hours / 24)  # Convert hours to days\n",
    "            \n",
    "            # Set both min and max to the same value for testing\n",
    "            self.min_periods = periods if min_periods is None else min_periods\n",
    "            self.max_periods = periods if max_periods is None else max_periods\n",
    "        else:\n",
    "            # Use provided values or defaults\n",
    "            self.min_periods = min_periods if min_periods is not None else 520\n",
    "            self.max_periods = max_periods if max_periods is not None else 1560\n",
    "        \n",
    "        self.smoothing_periods = smoothing_periods\n",
    "        \n",
    "        # Rest of the initialization stays the same\n",
    "        self.indicator_weights = {}\n",
    "        self.dimension_thresholds = {}\n",
    "        self.regime_history = []\n",
    "        self.rolling_stats_cache = {}\n",
    "        self.regime_smoother = RegimeSmoother(smoothing_periods)\n",
    "        \n",
    "        # Initialize system\n",
    "        self.setup_indicator_weights()\n",
    "        self.setup_dimension_thresholds()\n",
    "\n",
    "        logger.info(f\"Initialized classifier with min_periods={self.min_periods}, max_periods={self.max_periods}\")\n",
    "        \n",
    "    def setup_indicator_weights(self):\n",
    "        \"\"\"Setup voting weights for targeted indicators within each dimension\"\"\"\n",
    "        # KEEPING EXACTLY AS YOUR ORIGINAL\n",
    "        self.indicator_weights = {\n",
    "            # Direction Dimension Weights (expanded from original 3 to 10+ indicators)\n",
    "            'direction': {\n",
    "                'EMA': 1.0,      # EMA crossovers (original)\n",
    "                'SMA': 1.2,      # SMA slopes (original) \n",
    "                'MA': 1.0,       # MA alignment (original)\n",
    "                'KAMA': 0.9,     # Adaptive MAs\n",
    "                'WMA': 0.8,      # Weighted MAs\n",
    "                'HMA': 0.9,      # Hull MAs\n",
    "                'VWMA': 1.0,     # Volume weighted\n",
    "                'Ichimoku': 0.8, # Ichimoku components\n",
    "                'PSAR': 1.1      # Parabolic SAR\n",
    "            },\n",
    "            \n",
    "            # Trend Strength Dimension Weights\n",
    "            'trend_strength': {\n",
    "                'MA': 1.0,       # MA alignment (original)\n",
    "                'ADX': 1.2,      # ADX levels (original)\n",
    "                'MACD': 1.0,     # MACD histogram (original)\n",
    "                'Aroon': 0.9,    # Aroon oscillator\n",
    "                'DMI': 0.8,      # Directional movement\n",
    "                'CCI': 0.7,      # Commodity Channel Index\n",
    "                'Vortex': 0.8,   # Vortex indicator\n",
    "                'Choppiness': 0.9 # Choppiness index\n",
    "            },\n",
    "            \n",
    "            # Velocity Dimension Weights\n",
    "            'velocity': {\n",
    "                'RSI': 1.0,      # RSI momentum (original)\n",
    "                'ROC': 1.1,      # Rate of change (original)\n",
    "                'Momentum': 1.0, # Momentum oscillator (original)\n",
    "                'TSI': 0.9,      # True Strength Index\n",
    "                'Ultimate': 0.8, # Ultimate Oscillator\n",
    "                'StochRSI': 0.9, # Stochastic RSI\n",
    "                'MACD': 0.8      # MACD velocity component\n",
    "            },\n",
    "            \n",
    "            # Volatility Dimension Weights\n",
    "            'volatility': {\n",
    "                'ATR': 1.2,      # ATR levels (original)\n",
    "                'BB': 1.0,       # Bollinger Band width (original)\n",
    "                'Percentile': 0.9, # Vol percentiles (original)\n",
    "                'Keltner': 0.8,  # Keltner channel width\n",
    "                'Donchian': 0.7, # Donchian channel width\n",
    "                'Ulcer': 0.8,    # Ulcer index\n",
    "                'GARCH': 0.9,    # GARCH estimate\n",
    "                'Parkinson': 0.8 # Parkinson estimator\n",
    "            },\n",
    "            \n",
    "            # Microstructure Dimension Weights\n",
    "            'microstructure': {\n",
    "                'Volume': 1.0,   # Volume analysis (original)\n",
    "                'VWAP': 1.1,     # VWAP deviation (original)\n",
    "                'OBV': 0.9,      # On Balance Volume\n",
    "                'CMF': 0.8,      # Chaikin Money Flow\n",
    "                'MFI': 0.9,      # Money Flow Index\n",
    "                'ADI': 0.7,      # Accumulation/Distribution\n",
    "                'EOM': 0.8,      # Ease of Movement\n",
    "                'Force': 0.8     # Force Index\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def setup_dimension_thresholds(self):\n",
    "        \"\"\"Setup initial thresholds (will be updated by rolling calculations)\"\"\"\n",
    "        # KEEPING YOUR ORIGINAL THRESHOLDS\n",
    "        self.dimension_thresholds = {\n",
    "            'direction': {\n",
    "                'ema_cross': {'bullish': 0.02, 'bearish': -0.02},\n",
    "                'ma_alignment': {'bullish': 0.7, 'bearish': 0.3},\n",
    "                'slope': {'up': 0.001, 'down': -0.001}\n",
    "            },\n",
    "            'trend_strength': {\n",
    "                'adx': {'strong': 40, 'moderate': 25, 'weak': 15},\n",
    "                'macd_hist': {'strong': 0.005, 'moderate': 0.002}\n",
    "            },\n",
    "            'velocity': {\n",
    "                'rsi': {'overbought': 70, 'oversold': 30, 'neutral_high': 60, 'neutral_low': 40},\n",
    "                'roc': {'accelerating': 0.02, 'decelerating': -0.02}\n",
    "            },\n",
    "            'volatility': {\n",
    "                'percentile': {'low': 25, 'medium': 50, 'high': 75, 'extreme': 90}\n",
    "            },\n",
    "            'microstructure': {\n",
    "                'volume_ratio': {'high': 1.5, 'low': 0.5},\n",
    "                'vwap_deviation': {'institutional': 0.002, 'retail': 0.005}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def pre_calculate_rolling_statistics(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Pre-calculate ALL rolling statistics ONCE for efficiency\n",
    "        This is the KEY to fixing the performance issue\n",
    "        \"\"\"\n",
    "        logger.info(\"Pre-calculating rolling statistics for efficient classification...\")\n",
    "        \n",
    "        # Clear cache\n",
    "        self.rolling_stats_cache = {}\n",
    "        \n",
    "        # Use expanding window that caps at max_periods\n",
    "        window = data.index.to_series().expanding(min_periods=self.min_periods)\n",
    "        \n",
    "        # Direction indicators - calculate rolling slopes\n",
    "        for indicator_type in ['EMA', 'SMA', 'KAMA', 'WMA', 'HMA']:\n",
    "            for period in [9, 12, 21, 26, 50]:\n",
    "                col_name = f'{indicator_type}_{period}'\n",
    "                if col_name in data.columns:\n",
    "                    # 5-period slope\n",
    "                    slope = data[col_name].diff(5) / 5\n",
    "                    \n",
    "                    # Rolling percentiles using pandas rolling\n",
    "                    self.rolling_stats_cache[f'{col_name}_slope'] = slope\n",
    "                    self.rolling_stats_cache[f'{col_name}_slope_p20'] = slope.rolling(\n",
    "                        window=self.max_periods, min_periods=self.min_periods\n",
    "                    ).quantile(0.2)\n",
    "                    self.rolling_stats_cache[f'{col_name}_slope_p80'] = slope.rolling(\n",
    "                        window=self.max_periods, min_periods=self.min_periods\n",
    "                    ).quantile(0.8)\n",
    "        \n",
    "        # Volatility indicators - calculate rolling percentiles\n",
    "        if 'ATR' in data.columns:\n",
    "            atr = data['ATR']\n",
    "            self.rolling_stats_cache['ATR'] = atr\n",
    "            self.rolling_stats_cache['ATR_p25'] = atr.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.25)\n",
    "            self.rolling_stats_cache['ATR_p50'] = atr.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.50)\n",
    "            self.rolling_stats_cache['ATR_p75'] = atr.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.75)\n",
    "            self.rolling_stats_cache['ATR_p90'] = atr.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.90)\n",
    "        \n",
    "        # Bollinger Band width normalization\n",
    "        if 'BB_Width' in data.columns:\n",
    "            bb_width = data['BB_Width']\n",
    "            self.rolling_stats_cache['BB_Width'] = bb_width\n",
    "            rolling_min = bb_width.rolling(window=self.max_periods, min_periods=self.min_periods).min()\n",
    "            rolling_max = bb_width.rolling(window=self.max_periods, min_periods=self.min_periods).max()\n",
    "            rolling_range = rolling_max - rolling_min\n",
    "            rolling_range[rolling_range == 0] = 1  # Avoid division by zero\n",
    "            self.rolling_stats_cache['BB_Width_normalized'] = (bb_width - rolling_min) / rolling_range\n",
    "        \n",
    "        # Trend strength indicators\n",
    "        if 'ADX' in data.columns:\n",
    "            self.rolling_stats_cache['ADX'] = data['ADX']\n",
    "        \n",
    "        if 'MACD_Histogram' in data.columns:\n",
    "            macd_hist = data['MACD_Histogram']\n",
    "            self.rolling_stats_cache['MACD_Histogram'] = macd_hist\n",
    "            self.rolling_stats_cache['MACD_Histogram_p80'] = macd_hist.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.8)\n",
    "            self.rolling_stats_cache['MACD_Histogram_p20'] = macd_hist.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.2)\n",
    "        \n",
    "        # Velocity indicators\n",
    "        if 'RSI' in data.columns:\n",
    "            self.rolling_stats_cache['RSI'] = data['RSI']\n",
    "        \n",
    "        if 'ROC' in data.columns:\n",
    "            roc = data['ROC']\n",
    "            self.rolling_stats_cache['ROC'] = roc\n",
    "            self.rolling_stats_cache['ROC_p80'] = roc.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.8)\n",
    "            self.rolling_stats_cache['ROC_p20'] = roc.rolling(\n",
    "                window=self.max_periods, min_periods=self.min_periods\n",
    "            ).quantile(0.2)\n",
    "        \n",
    "        # Volume indicators\n",
    "        if 'volume' in data.columns:\n",
    "            volume = data['volume']\n",
    "            self.rolling_stats_cache['volume'] = volume\n",
    "            self.rolling_stats_cache['volume_ma'] = volume.rolling(\n",
    "                window=20, min_periods=10\n",
    "            ).mean()\n",
    "        \n",
    "        logger.info(f\"Pre-calculated {len(self.rolling_stats_cache)} rolling statistics\")\n",
    "    \n",
    "    def classify_direction_dimension(self, data: pd.DataFrame, votes: List[DimensionalVote], \n",
    "                                   index: int) -> Tuple[str, float]:\n",
    "        \"\"\"Classify direction for a specific time index using pre-calculated stats\"\"\"\n",
    "        try:\n",
    "            direction_scores = {'Up_Trending': 0, 'Down_Trending': 0, 'Sideways': 0}\n",
    "            total_weight = 0\n",
    "            \n",
    "            # Use pre-calculated rolling statistics\n",
    "            for indicator_type in ['EMA', 'SMA', 'KAMA', 'WMA', 'HMA']:\n",
    "                for period in [9, 12, 21, 26, 50]:\n",
    "                    col_name = f'{indicator_type}_{period}'\n",
    "                    slope_key = f'{col_name}_slope'\n",
    "                    \n",
    "                    if slope_key in self.rolling_stats_cache:\n",
    "                        current_slope = self.rolling_stats_cache[slope_key].iloc[index]\n",
    "                        up_threshold = self.rolling_stats_cache[f'{slope_key}_p80'].iloc[index]\n",
    "                        down_threshold = self.rolling_stats_cache[f'{slope_key}_p20'].iloc[index]\n",
    "                        \n",
    "                        weight = self.indicator_weights['direction'].get(indicator_type, 0.8)\n",
    "                        \n",
    "                        if pd.notna(current_slope) and pd.notna(up_threshold):\n",
    "                            if current_slope > up_threshold:\n",
    "                                direction_scores['Up_Trending'] += weight\n",
    "                                vote = DimensionalVote(\n",
    "                                    dimension='direction',\n",
    "                                    indicator_name=col_name,\n",
    "                                    regime_vote='Up_Trending',\n",
    "                                    confidence=0.8,\n",
    "                                    value=current_slope,\n",
    "                                    threshold_info={'up': up_threshold, 'down': down_threshold}\n",
    "                                )\n",
    "                                votes.append(vote)\n",
    "                            elif current_slope < down_threshold:\n",
    "                                direction_scores['Down_Trending'] += weight\n",
    "                                vote = DimensionalVote(\n",
    "                                    dimension='direction',\n",
    "                                    indicator_name=col_name,\n",
    "                                    regime_vote='Down_Trending',\n",
    "                                    confidence=0.8,\n",
    "                                    value=current_slope,\n",
    "                                    threshold_info={'up': up_threshold, 'down': down_threshold}\n",
    "                                )\n",
    "                                votes.append(vote)\n",
    "                            else:\n",
    "                                direction_scores['Sideways'] += weight * 0.5\n",
    "                            \n",
    "                            total_weight += weight\n",
    "            \n",
    "            # Determine regime\n",
    "            if total_weight > 0:\n",
    "                for regime in direction_scores:\n",
    "                    direction_scores[regime] /= total_weight\n",
    "                \n",
    "                best_regime = max(direction_scores, key=direction_scores.get)\n",
    "                confidence = direction_scores[best_regime]\n",
    "                return best_regime, confidence\n",
    "            else:\n",
    "                return DirectionRegime.UNDEFINED, 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in direction classification: {e}\")\n",
    "            return DirectionRegime.UNDEFINED, 0.0\n",
    "    \n",
    "    def classify_trend_strength_dimension(self, data: pd.DataFrame, votes: List[DimensionalVote], \n",
    "                                        index: int) -> Tuple[str, float]:\n",
    "        \"\"\"Classify trend strength for a specific time index\"\"\"\n",
    "        try:\n",
    "            strength_scores = {'Strong': 0, 'Moderate': 0, 'Weak': 0}\n",
    "            total_weight = 0\n",
    "            \n",
    "            # ADX-based classification\n",
    "            if 'ADX' in self.rolling_stats_cache:\n",
    "                adx_value = self.rolling_stats_cache['ADX'].iloc[index]\n",
    "                if pd.notna(adx_value):\n",
    "                    weight = self.indicator_weights['trend_strength'].get('ADX', 1.2)\n",
    "                    \n",
    "                    if adx_value > 40:\n",
    "                        strength_scores['Strong'] += weight\n",
    "                        regime_vote = 'Strong'\n",
    "                    elif adx_value > 25:\n",
    "                        strength_scores['Moderate'] += weight\n",
    "                        regime_vote = 'Moderate'\n",
    "                    else:\n",
    "                        strength_scores['Weak'] += weight\n",
    "                        regime_vote = 'Weak'\n",
    "                    \n",
    "                    vote = DimensionalVote(\n",
    "                        dimension='trend_strength',\n",
    "                        indicator_name='ADX',\n",
    "                        regime_vote=regime_vote,\n",
    "                        confidence=0.9,\n",
    "                        value=adx_value\n",
    "                    )\n",
    "                    votes.append(vote)\n",
    "                    total_weight += weight\n",
    "            \n",
    "            # MACD Histogram strength\n",
    "            if 'MACD_Histogram' in self.rolling_stats_cache:\n",
    "                macd_hist = self.rolling_stats_cache['MACD_Histogram'].iloc[index]\n",
    "                macd_p80 = self.rolling_stats_cache['MACD_Histogram_p80'].iloc[index]\n",
    "                macd_p20 = self.rolling_stats_cache['MACD_Histogram_p20'].iloc[index]\n",
    "                \n",
    "                if pd.notna(macd_hist) and pd.notna(macd_p80):\n",
    "                    weight = self.indicator_weights['trend_strength'].get('MACD', 1.0)\n",
    "                    abs_hist = abs(macd_hist)\n",
    "                    abs_p80 = abs(macd_p80)\n",
    "                    \n",
    "                    if abs_hist > abs_p80:\n",
    "                        strength_scores['Strong'] += weight\n",
    "                    elif abs_hist > abs_p80 * 0.5:\n",
    "                        strength_scores['Moderate'] += weight\n",
    "                    else:\n",
    "                        strength_scores['Weak'] += weight\n",
    "                    \n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Determine regime\n",
    "            if total_weight > 0:\n",
    "                for regime in strength_scores:\n",
    "                    strength_scores[regime] /= total_weight\n",
    "                \n",
    "                best_regime = max(strength_scores, key=strength_scores.get)\n",
    "                confidence = strength_scores[best_regime]\n",
    "                return best_regime, confidence\n",
    "            else:\n",
    "                return TrendStrengthRegime.UNDEFINED, 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in trend strength classification: {e}\")\n",
    "            return TrendStrengthRegime.UNDEFINED, 0.0\n",
    "    \n",
    "    def classify_velocity_dimension(self, data: pd.DataFrame, votes: List[DimensionalVote], \n",
    "                                  index: int) -> Tuple[str, float]:\n",
    "        \"\"\"Classify velocity for a specific time index - ADJUSTED FOR BETTER DISTRIBUTION\"\"\"\n",
    "        try:\n",
    "            velocity_scores = {'Accelerating': 0, 'Decelerating': 0, 'Stable': 0}\n",
    "            total_weight = 0\n",
    "            \n",
    "            # RSI momentum - ADJUSTED THRESHOLDS\n",
    "            if 'RSI' in self.rolling_stats_cache:\n",
    "                rsi_value = self.rolling_stats_cache['RSI'].iloc[index]\n",
    "                if pd.notna(rsi_value):\n",
    "                    weight = self.indicator_weights['velocity'].get('RSI', 1.0)\n",
    "                    \n",
    "                    # More balanced thresholds\n",
    "                    if rsi_value > 65:  # Lowered from 70\n",
    "                        velocity_scores['Accelerating'] += weight\n",
    "                        regime_vote = 'Accelerating'\n",
    "                    elif rsi_value < 35:  # Raised from 30\n",
    "                        velocity_scores['Decelerating'] += weight\n",
    "                        regime_vote = 'Decelerating'\n",
    "                    else:\n",
    "                        velocity_scores['Stable'] += weight * 0.7  # Reduced weight for stable\n",
    "                        regime_vote = 'Stable'\n",
    "                    \n",
    "                    vote = DimensionalVote(\n",
    "                        dimension='velocity',\n",
    "                        indicator_name='RSI',\n",
    "                        regime_vote=regime_vote,\n",
    "                        confidence=0.8,\n",
    "                        value=rsi_value\n",
    "                    )\n",
    "                    votes.append(vote)\n",
    "                    total_weight += weight\n",
    "            \n",
    "            # ROC acceleration - USE PERCENTILES FOR DYNAMIC THRESHOLDS\n",
    "            if 'ROC' in self.rolling_stats_cache:\n",
    "                roc_value = self.rolling_stats_cache['ROC'].iloc[index]\n",
    "                # Use 70/30 percentiles instead of 80/20 for more balanced distribution\n",
    "                roc_p70_key = 'ROC_p70'\n",
    "                roc_p30_key = 'ROC_p30'\n",
    "                \n",
    "                # Calculate these if not in cache\n",
    "                if roc_p70_key not in self.rolling_stats_cache:\n",
    "                    roc_series = self.rolling_stats_cache['ROC']\n",
    "                    self.rolling_stats_cache[roc_p70_key] = roc_series.rolling(\n",
    "                        window=self.max_periods, min_periods=self.min_periods\n",
    "                    ).quantile(0.7)\n",
    "                    self.rolling_stats_cache[roc_p30_key] = roc_series.rolling(\n",
    "                        window=self.max_periods, min_periods=self.min_periods\n",
    "                    ).quantile(0.3)\n",
    "                \n",
    "                roc_p70 = self.rolling_stats_cache[roc_p70_key].iloc[index]\n",
    "                roc_p30 = self.rolling_stats_cache[roc_p30_key].iloc[index]\n",
    "                \n",
    "                if pd.notna(roc_value) and pd.notna(roc_p70):\n",
    "                    weight = self.indicator_weights['velocity'].get('ROC', 1.1)\n",
    "                    \n",
    "                    if roc_value > roc_p70:\n",
    "                        velocity_scores['Accelerating'] += weight\n",
    "                    elif roc_value < roc_p30:\n",
    "                        velocity_scores['Decelerating'] += weight\n",
    "                    else:\n",
    "                        velocity_scores['Stable'] += weight * 0.7  # Reduced weight\n",
    "                    \n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Add Stochastic RSI for more velocity signals\n",
    "            if 'Stoch_RSI' in data.columns and index < len(data):\n",
    "                stoch_rsi = data['Stoch_RSI'].iloc[index]\n",
    "                if pd.notna(stoch_rsi):\n",
    "                    weight = self.indicator_weights['velocity'].get('StochRSI', 0.9)\n",
    "                    \n",
    "                    if stoch_rsi > 0.8:\n",
    "                        velocity_scores['Accelerating'] += weight\n",
    "                    elif stoch_rsi < 0.2:\n",
    "                        velocity_scores['Decelerating'] += weight\n",
    "                    else:\n",
    "                        velocity_scores['Stable'] += weight * 0.6\n",
    "                    \n",
    "                    total_weight += weight\n",
    "            \n",
    "            # TSI (True Strength Index) if available\n",
    "            if 'TSI' in data.columns and index < len(data):\n",
    "                tsi_value = data['TSI'].iloc[index]\n",
    "                if pd.notna(tsi_value):\n",
    "                    weight = self.indicator_weights['velocity'].get('TSI', 0.9)\n",
    "                    \n",
    "                    if tsi_value > 25:\n",
    "                        velocity_scores['Accelerating'] += weight\n",
    "                    elif tsi_value < -25:\n",
    "                        velocity_scores['Decelerating'] += weight\n",
    "                    else:\n",
    "                        velocity_scores['Stable'] += weight * 0.6\n",
    "                    \n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Momentum indicator\n",
    "            if 'Momentum' in data.columns and index < len(data):\n",
    "                mom_value = data['Momentum'].iloc[index]\n",
    "                if pd.notna(mom_value):\n",
    "                    weight = self.indicator_weights['velocity'].get('Momentum', 1.0)\n",
    "                    \n",
    "                    # Use percentage of price for momentum\n",
    "                    price = data['close'].iloc[index] if 'close' in data.columns else 1\n",
    "                    mom_pct = (mom_value / price) * 100 if price > 0 else 0\n",
    "                    \n",
    "                    if mom_pct > 2:  # 2% positive momentum\n",
    "                        velocity_scores['Accelerating'] += weight\n",
    "                    elif mom_pct < -2:  # 2% negative momentum\n",
    "                        velocity_scores['Decelerating'] += weight\n",
    "                    else:\n",
    "                        velocity_scores['Stable'] += weight * 0.5\n",
    "                    \n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Determine regime\n",
    "            if total_weight > 0:\n",
    "                for regime in velocity_scores:\n",
    "                    velocity_scores[regime] /= total_weight\n",
    "                \n",
    "                best_regime = max(velocity_scores, key=velocity_scores.get)\n",
    "                confidence = velocity_scores[best_regime]\n",
    "                return best_regime, confidence\n",
    "            else:\n",
    "                return VelocityRegime.UNDEFINED, 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in velocity classification: {e}\")\n",
    "            return VelocityRegime.UNDEFINED, 0.0\n",
    "    \n",
    "    def classify_volatility_dimension(self, data: pd.DataFrame, votes: List[DimensionalVote], \n",
    "                                    index: int) -> Tuple[str, float]:\n",
    "        \"\"\"Classify volatility for a specific time index\"\"\"\n",
    "        try:\n",
    "            vol_scores = {'Low_Vol': 0, 'Medium_Vol': 0, 'High_Vol': 0, 'Extreme_Vol': 0}\n",
    "            total_weight = 0\n",
    "            \n",
    "            # ATR-based classification\n",
    "            if 'ATR' in self.rolling_stats_cache:\n",
    "                current_atr = self.rolling_stats_cache['ATR'].iloc[index]\n",
    "                atr_p25 = self.rolling_stats_cache['ATR_p25'].iloc[index]\n",
    "                atr_p50 = self.rolling_stats_cache['ATR_p50'].iloc[index]\n",
    "                atr_p75 = self.rolling_stats_cache['ATR_p75'].iloc[index]\n",
    "                atr_p90 = self.rolling_stats_cache['ATR_p90'].iloc[index]\n",
    "                \n",
    "                if pd.notna(current_atr) and pd.notna(atr_p25):\n",
    "                    weight = self.indicator_weights['volatility'].get('ATR', 1.2)\n",
    "                    \n",
    "                    if current_atr < atr_p25:\n",
    "                        vol_scores['Low_Vol'] += weight\n",
    "                        regime_vote = 'Low_Vol'\n",
    "                    elif current_atr < atr_p50:\n",
    "                        vol_scores['Medium_Vol'] += weight\n",
    "                        regime_vote = 'Medium_Vol'\n",
    "                    elif current_atr < atr_p90:\n",
    "                        vol_scores['High_Vol'] += weight\n",
    "                        regime_vote = 'High_Vol'\n",
    "                    else:\n",
    "                        vol_scores['Extreme_Vol'] += weight\n",
    "                        regime_vote = 'Extreme_Vol'\n",
    "                    \n",
    "                    vote = DimensionalVote(\n",
    "                        dimension='volatility',\n",
    "                        indicator_name='ATR',\n",
    "                        regime_vote=regime_vote,\n",
    "                        confidence=0.9,\n",
    "                        value=current_atr,\n",
    "                        threshold_info={\n",
    "                            'p25': atr_p25, 'p50': atr_p50,\n",
    "                            'p75': atr_p75, 'p90': atr_p90\n",
    "                        }\n",
    "                    )\n",
    "                    votes.append(vote)\n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Bollinger Band width\n",
    "            if 'BB_Width_normalized' in self.rolling_stats_cache:\n",
    "                bb_norm = self.rolling_stats_cache['BB_Width_normalized'].iloc[index]\n",
    "                \n",
    "                if pd.notna(bb_norm):\n",
    "                    weight = self.indicator_weights['volatility'].get('BB', 1.0)\n",
    "                    \n",
    "                    if bb_norm < 0.25:\n",
    "                        vol_scores['Low_Vol'] += weight\n",
    "                    elif bb_norm < 0.5:\n",
    "                        vol_scores['Medium_Vol'] += weight\n",
    "                    elif bb_norm < 0.75:\n",
    "                        vol_scores['High_Vol'] += weight\n",
    "                    else:\n",
    "                        vol_scores['Extreme_Vol'] += weight\n",
    "                    \n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Determine regime\n",
    "            if total_weight > 0:\n",
    "                for regime in vol_scores:\n",
    "                    vol_scores[regime] /= total_weight\n",
    "                \n",
    "                best_regime = max(vol_scores, key=vol_scores.get)\n",
    "                confidence = vol_scores[best_regime]\n",
    "                return best_regime, confidence\n",
    "            else:\n",
    "                return VolatilityRegime.UNDEFINED, 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in volatility classification: {e}\")\n",
    "            return VolatilityRegime.UNDEFINED, 0.0\n",
    "    \n",
    "    def classify_microstructure_dimension(self, data: pd.DataFrame, votes: List[DimensionalVote], \n",
    "                                        index: int) -> Tuple[str, float]:\n",
    "        \"\"\"Classify microstructure for a specific time index\"\"\"\n",
    "        try:\n",
    "            micro_scores = {'Institutional_Flow': 0, 'Retail_Flow': 0, \n",
    "                          'Balanced_Flow': 0, 'Low_Participation': 0}\n",
    "            total_weight = 0\n",
    "            \n",
    "            # Volume-based classification\n",
    "            if 'volume' in self.rolling_stats_cache and 'volume_ma' in self.rolling_stats_cache:\n",
    "                current_vol = self.rolling_stats_cache['volume'].iloc[index]\n",
    "                vol_ma = self.rolling_stats_cache['volume_ma'].iloc[index]\n",
    "                \n",
    "                if pd.notna(current_vol) and pd.notna(vol_ma) and vol_ma > 0:\n",
    "                    vol_ratio = current_vol / vol_ma\n",
    "                    weight = self.indicator_weights['microstructure'].get('Volume', 1.0)\n",
    "                    \n",
    "                    if vol_ratio > 1.5:\n",
    "                        micro_scores['Institutional_Flow'] += weight\n",
    "                        regime_vote = 'Institutional_Flow'\n",
    "                    elif vol_ratio < 0.5:\n",
    "                        micro_scores['Low_Participation'] += weight\n",
    "                        regime_vote = 'Low_Participation'\n",
    "                    else:\n",
    "                        micro_scores['Balanced_Flow'] += weight\n",
    "                        regime_vote = 'Balanced_Flow'\n",
    "                    \n",
    "                    vote = DimensionalVote(\n",
    "                        dimension='microstructure',\n",
    "                        indicator_name='Volume',\n",
    "                        regime_vote=regime_vote,\n",
    "                        confidence=0.7,\n",
    "                        value=vol_ratio\n",
    "                    )\n",
    "                    votes.append(vote)\n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Money Flow Index\n",
    "            if 'MFI' in data.columns and index < len(data):\n",
    "                mfi_value = data['MFI'].iloc[index]\n",
    "                if pd.notna(mfi_value):\n",
    "                    weight = self.indicator_weights['microstructure'].get('MFI', 0.9)\n",
    "                    \n",
    "                    if mfi_value > 80:\n",
    "                        micro_scores['Institutional_Flow'] += weight * 0.8\n",
    "                    elif mfi_value < 20:\n",
    "                        micro_scores['Retail_Flow'] += weight * 0.8\n",
    "                    else:\n",
    "                        micro_scores['Balanced_Flow'] += weight * 0.5\n",
    "                    \n",
    "                    total_weight += weight\n",
    "            \n",
    "            # Determine regime\n",
    "            if total_weight > 0:\n",
    "                for regime in micro_scores:\n",
    "                    micro_scores[regime] /= total_weight\n",
    "                \n",
    "                best_regime = max(micro_scores, key=micro_scores.get)\n",
    "                confidence = micro_scores[best_regime]\n",
    "                return best_regime, confidence\n",
    "            else:\n",
    "                return MicrostructureRegime.UNDEFINED, 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in microstructure classification: {e}\")\n",
    "            return MicrostructureRegime.UNDEFINED, 0.0\n",
    "    \n",
    "    def classify_multidimensional_regime(self, data: pd.DataFrame, \n",
    "                                       symbol: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        EFFICIENT classification using pre-calculated rolling statistics\n",
    "        This is the main fix for the performance issue\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting efficient multi-dimensional regime classification...\")\n",
    "        \n",
    "        # CRITICAL: Pre-calculate all rolling statistics ONCE\n",
    "        self.pre_calculate_rolling_statistics(data)\n",
    "        \n",
    "        # Reset smoother for new classification\n",
    "        self.regime_smoother.reset()\n",
    "        \n",
    "        # Initialize results DataFrame with all columns\n",
    "        results = pd.DataFrame(index=data.index)\n",
    "        \n",
    "        # Initialize all regime columns\n",
    "        results['Direction_Regime'] = DirectionRegime.UNDEFINED\n",
    "        results['Direction_Confidence'] = 0.0\n",
    "        results['TrendStrength_Regime'] = TrendStrengthRegime.UNDEFINED\n",
    "        results['TrendStrength_Confidence'] = 0.0\n",
    "        results['Velocity_Regime'] = VelocityRegime.UNDEFINED\n",
    "        results['Velocity_Confidence'] = 0.0\n",
    "        results['Volatility_Regime'] = VolatilityRegime.UNDEFINED\n",
    "        results['Volatility_Confidence'] = 0.0\n",
    "        results['Microstructure_Regime'] = MicrostructureRegime.UNDEFINED\n",
    "        results['Microstructure_Confidence'] = 0.0\n",
    "        results['Composite_Regime'] = 'Undefined'\n",
    "        results['Composite_Confidence'] = 0.0\n",
    "        \n",
    "        # Process each time period efficiently\n",
    "        for i in tqdm(range(len(data)), desc=\"Classifying regimes\", mininterval=1.0, miniters=5):\n",
    "            if i < self.min_periods:\n",
    "                # Not enough data yet - keep defaults\n",
    "                continue\n",
    "            \n",
    "            votes = []\n",
    "            \n",
    "            # Classify each dimension using pre-calculated stats\n",
    "            direction_regime, direction_conf = self.classify_direction_dimension(data, votes, i)\n",
    "            trend_strength_regime, trend_strength_conf = self.classify_trend_strength_dimension(data, votes, i)\n",
    "            velocity_regime, velocity_conf = self.classify_velocity_dimension(data, votes, i)\n",
    "            volatility_regime, volatility_conf = self.classify_volatility_dimension(data, votes, i)\n",
    "            microstructure_regime, microstructure_conf = self.classify_microstructure_dimension(data, votes, i)\n",
    "            \n",
    "            # Apply smoothing to each dimension\n",
    "            smoothed_direction, _ = self.regime_smoother.smooth_regime('direction', direction_regime, data.index[i])\n",
    "            smoothed_trend_strength, _ = self.regime_smoother.smooth_regime('trend_strength', trend_strength_regime, data.index[i])\n",
    "            smoothed_velocity, _ = self.regime_smoother.smooth_regime('velocity', velocity_regime, data.index[i])\n",
    "            smoothed_volatility, _ = self.regime_smoother.smooth_regime('volatility', volatility_regime, data.index[i])\n",
    "            smoothed_microstructure, _ = self.regime_smoother.smooth_regime('microstructure', microstructure_regime, data.index[i])\n",
    "            \n",
    "            # Store results\n",
    "            results.loc[data.index[i], 'Direction_Regime'] = smoothed_direction\n",
    "            results.loc[data.index[i], 'Direction_Confidence'] = direction_conf\n",
    "            results.loc[data.index[i], 'TrendStrength_Regime'] = smoothed_trend_strength\n",
    "            results.loc[data.index[i], 'TrendStrength_Confidence'] = trend_strength_conf\n",
    "            results.loc[data.index[i], 'Velocity_Regime'] = smoothed_velocity\n",
    "            results.loc[data.index[i], 'Velocity_Confidence'] = velocity_conf\n",
    "            results.loc[data.index[i], 'Volatility_Regime'] = smoothed_volatility\n",
    "            results.loc[data.index[i], 'Volatility_Confidence'] = volatility_conf\n",
    "            results.loc[data.index[i], 'Microstructure_Regime'] = smoothed_microstructure\n",
    "            results.loc[data.index[i], 'Microstructure_Confidence'] = microstructure_conf\n",
    "            \n",
    "            # Create composite regime\n",
    "            composite = f\"{smoothed_direction}_{smoothed_trend_strength}_{smoothed_volatility}\"\n",
    "            results.loc[data.index[i], 'Composite_Regime'] = composite\n",
    "            \n",
    "            # Calculate composite confidence\n",
    "            avg_confidence = np.mean([direction_conf, trend_strength_conf, \n",
    "                                    velocity_conf, volatility_conf, microstructure_conf])\n",
    "            results.loc[data.index[i], 'Composite_Confidence'] = avg_confidence\n",
    "        \n",
    "        # Store parameters for this instrument\n",
    "        if symbol:\n",
    "            params = InstrumentRegimeParameters(\n",
    "                symbol=symbol,\n",
    "                direction_thresholds=self.dimension_thresholds.get('direction', {}),\n",
    "                trend_strength_thresholds=self.dimension_thresholds.get('trend_strength', {}),\n",
    "                velocity_thresholds=self.dimension_thresholds.get('velocity', {}),\n",
    "                volatility_thresholds=self.dimension_thresholds.get('volatility', {}),\n",
    "                microstructure_thresholds=self.dimension_thresholds.get('microstructure', {}),\n",
    "                last_update=data.index[-1]\n",
    "            )\n",
    "            logger.info(f\"Stored regime parameters for {symbol}\")\n",
    "        \n",
    "        # Combine with original data\n",
    "        return pd.concat([data, results], axis=1)\n",
    "    \n",
    "    # Keep all existing methods that aren't being replaced...\n",
    "    def update_thresholds(self, threshold_params: Dict[str, float]):\n",
    "        \"\"\"Update dimension thresholds from optimization parameters\"\"\"\n",
    "        # This is used by the optimizer - keep as is\n",
    "        for param_name, value in threshold_params.items():\n",
    "            if 'direction' in param_name:\n",
    "                if 'bullish' in param_name:\n",
    "                    self.dimension_thresholds['direction']['ema_cross']['bullish'] = value\n",
    "                elif 'bearish' in param_name:\n",
    "                    self.dimension_thresholds['direction']['ema_cross']['bearish'] = value\n",
    "            elif 'volatility' in param_name:\n",
    "                if 'low' in param_name:\n",
    "                    self.dimension_thresholds['volatility']['percentile']['low'] = value\n",
    "                elif 'high' in param_name:\n",
    "                    self.dimension_thresholds['volatility']['percentile']['high'] = value\n",
    "            elif 'trend' in param_name:\n",
    "                if 'strong' in param_name:\n",
    "                    self.dimension_thresholds['trend_strength']['adx']['strong'] = value\n",
    "                elif 'weak' in param_name:\n",
    "                    self.dimension_thresholds['trend_strength']['adx']['weak'] = value\n",
    "            elif 'velocity' in param_name:\n",
    "                if 'overbought' in param_name:\n",
    "                    self.dimension_thresholds['velocity']['rsi']['overbought'] = value\n",
    "                elif 'oversold' in param_name:\n",
    "                    self.dimension_thresholds['velocity']['rsi']['oversold'] = value\n",
    "\n",
    "# =============================================================================\n",
    "# INTEGRATION WITH YOUR WORKING SYSTEM (DATA LOADING + INDICATORS)\n",
    "# =============================================================================\n",
    "\n",
    "def parse_symbol(symbol_str):\n",
    "    \"\"\"Extract base symbol from futures contract notation\"\"\"\n",
    "    if not isinstance(symbol_str, str) or pd.isna(symbol_str):\n",
    "        return None\n",
    "    if len(symbol_str) < 3:\n",
    "        return symbol_str\n",
    "    year = symbol_str[-2:]\n",
    "    if year.isdigit():\n",
    "        month = symbol_str[-3]\n",
    "        valid_month_codes = {'F', 'G', 'H', 'J', 'K', 'M', 'N', 'Q', 'U', 'V', 'X', 'Z'}\n",
    "        if month in valid_month_codes:\n",
    "            return symbol_str[:-3]\n",
    "    return symbol_str\n",
    "\n",
    "def load_csv_data(csv_paths: Union[str, List[str]], symbols: Optional[List[str]] = None, start_date: Optional[str] = None, end_date: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Your working data loader\"\"\"\n",
    "    logger.info(\"DATA: Starting load_csv_data\")\n",
    "    perf_monitor.start_timer(\"data_loading\")\n",
    "    \n",
    "    if isinstance(csv_paths, str):\n",
    "        csv_paths = [csv_paths]\n",
    "    \n",
    "    all_dfs = []\n",
    "    \n",
    "    for csv_path in tqdm(csv_paths, desc=\"Reading CSV files\"):\n",
    "        try:\n",
    "            chunks = pd.read_csv(csv_path, parse_dates=['Date'], index_col='Date', chunksize=100000, dtype={'Symbol': str}, low_memory=False)\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                if chunk.empty or chunk.index.hasnans:\n",
    "                    continue\n",
    "                \n",
    "                chunk = chunk.copy()\n",
    "                \n",
    "                try:\n",
    "                    if chunk.index.tz is None:\n",
    "                        chunk.index = chunk.index.tz_localize('America/New_York', ambiguous='infer', nonexistent='shift_forward')\n",
    "                    else:\n",
    "                        chunk.index = chunk.index.tz_convert('America/New_York')\n",
    "                except Exception:\n",
    "                    if chunk.index.tz is not None:\n",
    "                        chunk.index = chunk.index.tz_localize(None)\n",
    "                \n",
    "                dfs = []\n",
    "                i = 1\n",
    "                \n",
    "                while True:\n",
    "                    symbol_col = f'Symbol.{i}'\n",
    "                    required_cols = [symbol_col, f'Open.{i}', f'High.{i}', f'Low.{i}', f'Close.{i}']\n",
    "                    if not all(col in chunk.columns for col in required_cols):\n",
    "                        break\n",
    "                    \n",
    "                    sub_df = pd.DataFrame(index=chunk.index)\n",
    "                    sub_df['symbol'] = chunk[symbol_col]\n",
    "                    sub_df['open'] = chunk[f'Open.{i}']\n",
    "                    sub_df['high'] = chunk[f'High.{i}']\n",
    "                    sub_df['low'] = chunk[f'Low.{i}']\n",
    "                    sub_df['close'] = chunk[f'Close.{i}']\n",
    "                    sub_df['volume'] = chunk[f'Volume.{i}'] if f'Volume.{i}' in chunk.columns else 0\n",
    "                    sub_df['openinterest'] = chunk[f'OpenInterest.{i}'] if f'OpenInterest.{i}' in chunk.columns else 0\n",
    "                    \n",
    "                    sub_df = sub_df.dropna(subset=['symbol'])\n",
    "                    if sub_df.empty:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    sub_df.columns = ['symbol', 'open', 'high', 'low', 'close', 'volume', 'openinterest']\n",
    "                    \n",
    "                    for col in ['open', 'high', 'low', 'close', 'volume', 'openinterest']:\n",
    "                        sub_df[col] = sub_df[col].astype(str).str.replace(',', '', regex=False)\n",
    "                        sub_df[col] = pd.to_numeric(sub_df[col], errors='coerce')\n",
    "                    \n",
    "                    sub_df['BaseSymbol'] = sub_df['symbol'].apply(lambda x: parse_symbol(x) if pd.notna(x) else None)\n",
    "                    sub_df = sub_df.dropna(subset=['BaseSymbol'])\n",
    "                    \n",
    "                    if not sub_df.empty:\n",
    "                        dfs.append(sub_df)\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                if dfs:\n",
    "                    chunk_df = pd.concat(dfs, ignore_index=False)\n",
    "                    all_dfs.append(chunk_df)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"FILE ERROR: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.concat(all_dfs, ignore_index=False)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    if symbols:\n",
    "        df = df[df['BaseSymbol'].isin(symbols)]\n",
    "    if start_date:\n",
    "        df = df[df.index >= pd.to_datetime(start_date)]\n",
    "    if end_date:\n",
    "        df = df[df.index <= pd.to_datetime(end_date)]\n",
    "    \n",
    "    df = df.dropna(subset=['open', 'high', 'low', 'close'], how='all')\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "    gc.collect()\n",
    "    perf_monitor.end_timer(\"data_loading\")\n",
    "    logger.info(f\"SUCCESS: Loaded {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# EXPANDED 80+ INDICATOR UNIVERSE - TA 0.11.0 COMPATIBLE\n",
    "# Complete institutional-level indicator calculation system\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_kama(close: pd.Series, window: int = 14, pow1: int = 2, pow2: int = 30) -> pd.Series:\n",
    "    \"\"\"Kaufman's Adaptive Moving Average - Custom implementation\"\"\"\n",
    "    try:\n",
    "        change = close.diff(window).abs()\n",
    "        volatility = close.diff().abs().rolling(window).sum()\n",
    "        er = change / volatility\n",
    "        sc = (er * (2.0 / (pow1 + 1) - 2.0 / (pow2 + 1.0)) + 2 / (pow2 + 1.0)) ** 2.0\n",
    "        kama = pd.Series(index=close.index, dtype=float)\n",
    "        kama.iloc[window] = close.iloc[:window+1].mean()\n",
    "        for i in range(window + 1, len(close)):\n",
    "            kama.iloc[i] = kama.iloc[i-1] + sc.iloc[i] * (close.iloc[i] - kama.iloc[i-1])\n",
    "        return kama\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"KAMA calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_wma(close: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Weighted Moving Average - Custom implementation\"\"\"\n",
    "    try:\n",
    "        weights = np.arange(1, window + 1)\n",
    "        return close.rolling(window).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"WMA calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_hma(close: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Hull Moving Average - Custom implementation\"\"\"\n",
    "    try:\n",
    "        wma_half = calculate_wma(close, window // 2)\n",
    "        wma_full = calculate_wma(close, window)\n",
    "        hma_data = 2 * wma_half - wma_full\n",
    "        hma_window = int(np.sqrt(window))\n",
    "        return calculate_wma(hma_data, hma_window)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"HMA calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_vwma(close: pd.Series, volume: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Volume Weighted Moving Average - Custom implementation\"\"\"\n",
    "    try:\n",
    "        return (close * volume).rolling(window).sum() / volume.rolling(window).sum()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"VWMA calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_volume_sma(volume: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Volume Simple Moving Average\"\"\"\n",
    "    try:\n",
    "        return volume.rolling(window).mean()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Volume SMA calculation failed: {e}\")\n",
    "        return pd.Series(index=volume.index, dtype=float)\n",
    "\n",
    "def calculate_vwap(high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Volume Weighted Average Price\"\"\"\n",
    "    try:\n",
    "        typical_price = (high + low + close) / 3\n",
    "        return (typical_price * volume).rolling(window).sum() / volume.rolling(window).sum()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"VWAP calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_dmi(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14) -> tuple:\n",
    "    \"\"\"Directional Movement Index - Custom implementation\"\"\"\n",
    "    try:\n",
    "        tr = pd.concat([high - low, (high - close.shift()).abs(), (low - close.shift()).abs()], axis=1).max(axis=1)\n",
    "        dm_plus = ((high - high.shift()) > (low.shift() - low)) & ((high - high.shift()) > 0)\n",
    "        dm_plus = dm_plus * (high - high.shift())\n",
    "        dm_minus = ((low.shift() - low) > (high - high.shift())) & ((low.shift() - low) > 0)\n",
    "        dm_minus = dm_minus * (low.shift() - low)\n",
    "        \n",
    "        tr_smooth = tr.rolling(window).mean()\n",
    "        dm_plus_smooth = dm_plus.rolling(window).mean()\n",
    "        dm_minus_smooth = dm_minus.rolling(window).mean()\n",
    "        \n",
    "        di_plus = 100 * dm_plus_smooth / tr_smooth\n",
    "        di_minus = 100 * dm_minus_smooth / tr_smooth\n",
    "        \n",
    "        return di_plus, di_minus\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"DMI calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float), pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_choppiness_index(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14) -> pd.Series:\n",
    "    \"\"\"Choppiness Index - Custom implementation\"\"\"\n",
    "    try:\n",
    "        tr = pd.concat([high - low, (high - close.shift()).abs(), (low - close.shift()).abs()], axis=1).max(axis=1)\n",
    "        atr_sum = tr.rolling(window).sum()\n",
    "        high_max = high.rolling(window).max()\n",
    "        low_min = low.rolling(window).min()\n",
    "        ci = 100 * np.log10(atr_sum / (high_max - low_min)) / np.log10(window)\n",
    "        return ci\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Choppiness Index calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_linear_regression_slope(close: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Linear Regression Slope - Custom implementation\"\"\"\n",
    "    try:\n",
    "        def slope(y):\n",
    "            x = np.arange(len(y))\n",
    "            return np.polyfit(x, y, 1)[0] if len(y) == window else np.nan\n",
    "        return close.rolling(window).apply(slope, raw=True)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Linear Regression Slope calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_momentum(close: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Momentum - Custom implementation\"\"\"\n",
    "    try:\n",
    "        return close.diff(window)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Momentum calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_price_acceleration(close: pd.Series, window: int = 10) -> pd.Series:\n",
    "    \"\"\"Price Acceleration - Custom implementation\"\"\"\n",
    "    try:\n",
    "        velocity = close.diff()\n",
    "        acceleration = velocity.diff()\n",
    "        return acceleration.rolling(window).mean()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Price acceleration calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_bollinger_width(close: pd.Series, window: int = 20, std_dev: int = 2) -> pd.Series:\n",
    "    \"\"\"Bollinger Bands Width - Custom implementation\"\"\"\n",
    "    try:\n",
    "        bb = BollingerBands(close=close, window=window, window_dev=std_dev)\n",
    "        upper = bb.bollinger_hband()\n",
    "        lower = bb.bollinger_lband()\n",
    "        width = (upper - lower) / bb.bollinger_mavg() * 100\n",
    "        return width\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Bollinger Width calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_historical_volatility(close: pd.Series, window: int = 20) -> pd.Series:\n",
    "    \"\"\"Historical Volatility - Custom implementation\"\"\"\n",
    "    try:\n",
    "        returns = np.log(close / close.shift())\n",
    "        return returns.rolling(window).std() * np.sqrt(252) * 100\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Historical Volatility calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_parkinson_estimator(high: pd.Series, low: pd.Series, window: int = 20) -> pd.Series:\n",
    "    \"\"\"Parkinson Volatility Estimator - Custom implementation\"\"\"\n",
    "    try:\n",
    "        log_ratio = np.log(high / low) ** 2\n",
    "        parkinson = np.sqrt(log_ratio.rolling(window).mean() / (4 * np.log(2))) * np.sqrt(252) * 100\n",
    "        return parkinson\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Parkinson Estimator calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_trend_consistency(close: pd.Series, window: int = 20) -> pd.Series:\n",
    "    \"\"\"Trend Consistency - Custom implementation\"\"\"\n",
    "    try:\n",
    "        price_changes = close.diff()\n",
    "        positive_changes = (price_changes > 0).rolling(window).sum()\n",
    "        consistency = positive_changes / window\n",
    "        return consistency\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Trend Consistency calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "def calculate_ma_alignment(close: pd.Series, windows: list = [5, 10, 20, 50]) -> pd.Series:\n",
    "    \"\"\"Moving Average Alignment Score - Custom implementation\"\"\"\n",
    "    try:\n",
    "        mas = {}\n",
    "        for w in windows:\n",
    "            mas[w] = close.rolling(w).mean()\n",
    "        \n",
    "        alignment_scores = []\n",
    "        for i in close.index:\n",
    "            ma_values = [mas[w].loc[i] if i in mas[w].index and not pd.isna(mas[w].loc[i]) else None for w in windows]\n",
    "            ma_values = [v for v in ma_values if v is not None]\n",
    "            \n",
    "            if len(ma_values) < 2:\n",
    "                alignment_scores.append(0)\n",
    "                continue\n",
    "                \n",
    "            # Check if MAs are in ascending or descending order\n",
    "            ascending = all(ma_values[i] <= ma_values[i+1] for i in range(len(ma_values)-1))\n",
    "            descending = all(ma_values[i] >= ma_values[i+1] for i in range(len(ma_values)-1))\n",
    "            \n",
    "            if ascending:\n",
    "                alignment_scores.append(1)  # Bullish alignment\n",
    "            elif descending:\n",
    "                alignment_scores.append(-1)  # Bearish alignment\n",
    "            else:\n",
    "                alignment_scores.append(0)  # Mixed alignment\n",
    "        \n",
    "        return pd.Series(alignment_scores, index=close.index)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"MA Alignment calculation failed: {e}\")\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "\n",
    "# =============================================================================\n",
    "# 80+ INDICATOR CALCULATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_all_indicators(df: pd.DataFrame, timeframe: str = \"15min\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate complete 80+ indicator universe across 5 regime dimensions\n",
    "    \n",
    "    DIRECTION INDICATORS (20):\n",
    "    - Moving Averages: SMA, EMA, KAMA, WMA, HMA, VWMA\n",
    "    - Trend Indicators: Ichimoku, PSAR, ADX, DMI, Aroon, CCI\n",
    "    \n",
    "    TREND STRENGTH INDICATORS (15):\n",
    "    - MACD family, Slope analysis, Trend consistency, MA alignment\n",
    "    - Vortex, Linear Regression, Choppiness Index\n",
    "    \n",
    "    VELOCITY INDICATORS (12):\n",
    "    - ROC multiple timeframes, Momentum, RSI derivatives\n",
    "    - Stochastic momentum, Price acceleration\n",
    "    \n",
    "    VOLATILITY INDICATORS (13):\n",
    "    - ATR family, Bollinger Bands, Keltner Channels\n",
    "    - Ulcer Index, Historical volatility, Parkinson estimator\n",
    "    \n",
    "    MARKET MICROSTRUCTURE INDICATORS (12):\n",
    "    - VWAP analysis, Volume indicators, Money flow\n",
    "    - Accumulation/Distribution, Force Index\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\"INDICATORS: Starting 80+ indicator calculation\")\n",
    "    \n",
    "    try:\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['open', 'high', 'low', 'close']\n",
    "        for col in required_cols:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "        \n",
    "        # Handle volume column\n",
    "        has_volume = 'volume' in df.columns\n",
    "        if not has_volume:\n",
    "            logger.warning(\"Volume column not found, volume-based indicators will be skipped\")\n",
    "            df['volume'] = 1.0  # Dummy volume for non-volume indicators\n",
    "        \n",
    "        # =============================================================================\n",
    "        # DIRECTION INDICATORS (20)\n",
    "        # =============================================================================\n",
    "        \n",
    "        logger.info(\"Calculating Direction Indicators (20)...\")\n",
    "        \n",
    "        # Moving Averages - Multiple timeframes with error handling\n",
    "        try:\n",
    "            df['SMA_5'] = SMAIndicator(close=df['close'], window=5).sma_indicator()\n",
    "            df['SMA_10'] = SMAIndicator(close=df['close'], window=10).sma_indicator()\n",
    "            df['SMA_20'] = SMAIndicator(close=df['close'], window=20).sma_indicator()\n",
    "            df['SMA_50'] = SMAIndicator(close=df['close'], window=50).sma_indicator()\n",
    "            df['SMA_100'] = SMAIndicator(close=df['close'], window=100).sma_indicator()\n",
    "            df['SMA_200'] = SMAIndicator(close=df['close'], window=200).sma_indicator()\n",
    "            logger.info(\"SMA indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating SMA indicators: {e}\")\n",
    "        \n",
    "        try:\n",
    "            df['EMA_5'] = EMAIndicator(close=df['close'], window=5).ema_indicator()\n",
    "            df['EMA_8'] = EMAIndicator(close=df['close'], window=8).ema_indicator()\n",
    "            df['EMA_12'] = EMAIndicator(close=df['close'], window=12).ema_indicator()\n",
    "            df['EMA_21'] = EMAIndicator(close=df['close'], window=21).ema_indicator()\n",
    "            df['EMA_26'] = EMAIndicator(close=df['close'], window=26).ema_indicator()\n",
    "            df['EMA_50'] = EMAIndicator(close=df['close'], window=50).ema_indicator()\n",
    "            df['EMA_100'] = EMAIndicator(close=df['close'], window=100).ema_indicator()\n",
    "            logger.info(\"EMA indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating EMA indicators: {e}\")\n",
    "        \n",
    "        # Custom Moving Averages\n",
    "        try:\n",
    "            df['KAMA_14'] = calculate_kama(df['close'], window=14)\n",
    "            df['KAMA_30'] = calculate_kama(df['close'], window=30)\n",
    "            df['WMA_20'] = calculate_wma(df['close'], window=20)\n",
    "            df['HMA_14'] = calculate_hma(df['close'], window=14)\n",
    "            logger.info(\"Custom moving averages calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating custom MAs: {e}\")\n",
    "        \n",
    "        if has_volume:\n",
    "            try:\n",
    "                df['VWMA_20'] = calculate_vwma(df['close'], df['volume'], window=20)\n",
    "                logger.info(\"VWMA calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating VWMA: {e}\")\n",
    "        \n",
    "        # Trend Direction Indicators\n",
    "        try:\n",
    "            ichimoku = IchimokuIndicator(high=df['high'], low=df['low'], window1=9, window2=26, window3=52)\n",
    "            df['Ichimoku_A'] = ichimoku.ichimoku_a()\n",
    "            df['Ichimoku_B'] = ichimoku.ichimoku_b()\n",
    "            df['Ichimoku_Base'] = ichimoku.ichimoku_base_line()\n",
    "            df['Ichimoku_Conversion'] = ichimoku.ichimoku_conversion_line()\n",
    "            logger.info(\"Ichimoku indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Ichimoku: {e}\")\n",
    "        \n",
    "        try:\n",
    "            df['PSAR'] = PSARIndicator(high=df['high'], low=df['low'], close=df['close']).psar()\n",
    "            logger.info(\"PSAR calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating PSAR: {e}\")\n",
    "        \n",
    "        # =============================================================================\n",
    "        # TREND STRENGTH INDICATORS (15)\n",
    "        # =============================================================================\n",
    "        \n",
    "        logger.info(\"Calculating Trend Strength Indicators (15)...\")\n",
    "        \n",
    "        # MACD Family with error handling\n",
    "        try:\n",
    "            macd = MACD(close=df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "            df['MACD'] = macd.macd()\n",
    "            df['MACD_Signal'] = macd.macd_signal()\n",
    "            df['MACD_Histogram'] = macd.macd_diff()\n",
    "            logger.info(\"MACD indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating MACD: {e}\")\n",
    "        \n",
    "        # ADX and DMI with error handling\n",
    "        try:\n",
    "            adx = ADXIndicator(high=df['high'], low=df['low'], close=df['close'], window=14)\n",
    "            df['ADX'] = adx.adx()\n",
    "            df['ADX_POS'] = adx.adx_pos()\n",
    "            df['ADX_NEG'] = adx.adx_neg()\n",
    "            logger.info(\"ADX indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating ADX: {e}\")\n",
    "        \n",
    "        # Additional DMI calculation\n",
    "        try:\n",
    "            df['DI_Plus'], df['DI_Minus'] = calculate_dmi(df['high'], df['low'], df['close'], window=14)\n",
    "            logger.info(\"Custom DMI calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating custom DMI: {e}\")\n",
    "        \n",
    "        # Aroon - Fixed for ta 0.11.0 compatibility\n",
    "        try:\n",
    "            aroon = AroonIndicator(high=df['high'], low=df['low'], window=25)\n",
    "            df['Aroon_Up'] = aroon.aroon_up()\n",
    "            df['Aroon_Down'] = aroon.aroon_down()\n",
    "            df['Aroon_Oscillator'] = df['Aroon_Up'] - df['Aroon_Down']\n",
    "            logger.info(\"Aroon indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Aroon: {e}\")\n",
    "        \n",
    "        # CCI with error handling\n",
    "        try:\n",
    "            df['CCI'] = CCIIndicator(high=df['high'], low=df['low'], close=df['close'], window=20).cci()\n",
    "            logger.info(\"CCI calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating CCI: {e}\")\n",
    "        \n",
    "        # Vortex with error handling\n",
    "        try:\n",
    "            vortex = VortexIndicator(high=df['high'], low=df['low'], close=df['close'], window=14)\n",
    "            df['Vortex_Pos'] = vortex.vortex_indicator_pos()\n",
    "            df['Vortex_Neg'] = vortex.vortex_indicator_neg()\n",
    "            logger.info(\"Vortex indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Vortex: {e}\")\n",
    "        \n",
    "        # Custom Trend Strength Indicators\n",
    "        try:\n",
    "            df['Linear_Regression_Slope_14'] = calculate_linear_regression_slope(df['close'], window=14)\n",
    "            df['Linear_Regression_Slope_30'] = calculate_linear_regression_slope(df['close'], window=30)\n",
    "            df['Choppiness_Index'] = calculate_choppiness_index(df['high'], df['low'], df['close'], window=14)\n",
    "            df['Trend_Consistency'] = calculate_trend_consistency(df['close'], window=20)\n",
    "            df['MA_Alignment'] = calculate_ma_alignment(df['close'], windows=[5, 10, 20, 50])\n",
    "            logger.info(\"Custom trend strength indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating custom trend indicators: {e}\")\n",
    "        \n",
    "        # =============================================================================\n",
    "        # VELOCITY INDICATORS (12)\n",
    "        # =============================================================================\n",
    "        \n",
    "        logger.info(\"Calculating Velocity Indicators (12)...\")\n",
    "        \n",
    "        # ROC Multiple Timeframes with error handling\n",
    "        try:\n",
    "            df['ROC_5'] = ROCIndicator(close=df['close'], window=5).roc()\n",
    "            df['ROC_10'] = ROCIndicator(close=df['close'], window=10).roc()\n",
    "            df['ROC_20'] = ROCIndicator(close=df['close'], window=20).roc()\n",
    "            df['ROC_50'] = ROCIndicator(close=df['close'], window=50).roc()\n",
    "            logger.info(\"ROC indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating ROC: {e}\")\n",
    "        \n",
    "        # RSI and derivatives with error handling\n",
    "        try:\n",
    "            df['RSI'] = RSIIndicator(close=df['close'], window=14).rsi()\n",
    "            df['RSI_9'] = RSIIndicator(close=df['close'], window=9).rsi()\n",
    "            logger.info(\"RSI indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating RSI: {e}\")\n",
    "        \n",
    "        try:\n",
    "            stoch_rsi = StochRSIIndicator(close=df['close'], window=14, smooth1=3, smooth2=3)\n",
    "            df['StochRSI'] = stoch_rsi.stochrsi()\n",
    "            df['StochRSI_K'] = stoch_rsi.stochrsi_k()\n",
    "            df['StochRSI_D'] = stoch_rsi.stochrsi_d()\n",
    "            logger.info(\"StochRSI indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating StochRSI: {e}\")\n",
    "        \n",
    "        # Stochastic Oscillator with error handling\n",
    "        try:\n",
    "            stoch = StochasticOscillator(high=df['high'], low=df['low'], close=df['close'], window=14, smooth_window=3)\n",
    "            df['Stoch_K'] = stoch.stoch()\n",
    "            df['Stoch_D'] = stoch.stoch_signal()\n",
    "            logger.info(\"Stochastic indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Stochastic: {e}\")\n",
    "        \n",
    "        # Custom Velocity Indicators with error handling\n",
    "        try:\n",
    "            df['Momentum_10'] = calculate_momentum(df['close'], window=10)\n",
    "            df['Price_Acceleration'] = calculate_price_acceleration(df['close'], window=10)\n",
    "            logger.info(\"Custom velocity indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating custom velocity indicators: {e}\")\n",
    "        \n",
    "        # =============================================================================\n",
    "        # VOLATILITY INDICATORS (13)\n",
    "        # =============================================================================\n",
    "        \n",
    "        logger.info(\"Calculating Volatility Indicators (13)...\")\n",
    "        \n",
    "        # ATR Family with error handling\n",
    "        try:\n",
    "            df['ATR'] = AverageTrueRange(high=df['high'], low=df['low'], close=df['close'], window=14).average_true_range()\n",
    "            df['ATR_7'] = AverageTrueRange(high=df['high'], low=df['low'], close=df['close'], window=7).average_true_range()\n",
    "            df['ATR_21'] = AverageTrueRange(high=df['high'], low=df['low'], close=df['close'], window=21).average_true_range()\n",
    "            logger.info(\"ATR indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating ATR: {e}\")\n",
    "        \n",
    "        # Bollinger Bands with error handling\n",
    "        try:\n",
    "            bb = BollingerBands(close=df['close'], window=20, window_dev=2)\n",
    "            df['BB_Upper'] = bb.bollinger_hband()\n",
    "            df['BB_Lower'] = bb.bollinger_lband()\n",
    "            df['BB_Middle'] = bb.bollinger_mavg()\n",
    "            df['BB_Width'] = calculate_bollinger_width(df['close'], window=20, std_dev=2)\n",
    "            df['BB_Percent'] = (df['close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "            logger.info(\"Bollinger Bands calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Bollinger Bands: {e}\")\n",
    "        \n",
    "        # Keltner Channels with error handling\n",
    "        try:\n",
    "            kc = KeltnerChannel(high=df['high'], low=df['low'], close=df['close'], window=20, window_atr=10)\n",
    "            df['KC_Upper'] = kc.keltner_channel_hband()\n",
    "            df['KC_Lower'] = kc.keltner_channel_lband()\n",
    "            df['KC_Middle'] = kc.keltner_channel_mband()\n",
    "            logger.info(\"Keltner Channels calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Keltner Channels: {e}\")\n",
    "        \n",
    "        # Donchian Channels with error handling\n",
    "        try:\n",
    "            dc = DonchianChannel(high=df['high'], low=df['low'], close=df['close'], window=20)\n",
    "            df['DC_Upper'] = dc.donchian_channel_hband()\n",
    "            df['DC_Lower'] = dc.donchian_channel_lband()\n",
    "            df['DC_Middle'] = dc.donchian_channel_mband()\n",
    "            logger.info(\"Donchian Channels calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Donchian Channels: {e}\")\n",
    "        \n",
    "        # Additional Volatility Indicators with error handling\n",
    "        try:\n",
    "            df['Ulcer_Index'] = UlcerIndex(close=df['close'], window=14).ulcer_index()\n",
    "            logger.info(\"Ulcer Index calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Ulcer Index: {e}\")\n",
    "        \n",
    "        try:\n",
    "            df['Historical_Volatility'] = calculate_historical_volatility(df['close'], window=20)\n",
    "            df['Parkinson_Estimator'] = calculate_parkinson_estimator(df['high'], df['low'], window=20)\n",
    "            logger.info(\"Custom volatility indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating custom volatility indicators: {e}\")\n",
    "        \n",
    "        # =============================================================================\n",
    "        # MARKET MICROSTRUCTURE INDICATORS (12)\n",
    "        # =============================================================================\n",
    "        \n",
    "        logger.info(\"Calculating Market Microstructure Indicators (12)...\")\n",
    "        \n",
    "        if has_volume:\n",
    "            # VWAP Analysis with error handling\n",
    "            try:\n",
    "                df['VWAP'] = calculate_vwap(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
    "                df['VWAP_20'] = calculate_vwap(df['high'], df['low'], df['close'], df['volume'], window=20)\n",
    "                logger.info(\"VWAP indicators calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating VWAP: {e}\")\n",
    "            \n",
    "            # Volume Indicators with error handling\n",
    "            try:\n",
    "                df['Volume_SMA'] = calculate_volume_sma(df['volume'], window=20)\n",
    "                df['Volume_Ratio'] = df['volume'] / df['Volume_SMA']\n",
    "                logger.info(\"Volume indicators calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating volume indicators: {e}\")\n",
    "            \n",
    "            # On Balance Volume with error handling\n",
    "            try:\n",
    "                df['OBV'] = OnBalanceVolumeIndicator(close=df['close'], volume=df['volume']).on_balance_volume()\n",
    "                logger.info(\"OBV calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating OBV: {e}\")\n",
    "            \n",
    "            # Money Flow Indicators with error handling\n",
    "            try:\n",
    "                df['CMF'] = ChaikinMoneyFlowIndicator(high=df['high'], low=df['low'], close=df['close'], volume=df['volume'], window=20).chaikin_money_flow()\n",
    "                df['MFI'] = MFIIndicator(high=df['high'], low=df['low'], close=df['close'], volume=df['volume'], window=14).money_flow_index()\n",
    "                logger.info(\"Money flow indicators calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating money flow indicators: {e}\")\n",
    "            \n",
    "            # Accumulation/Distribution with error handling\n",
    "            try:\n",
    "                df['ADI'] = AccDistIndexIndicator(high=df['high'], low=df['low'], close=df['close'], volume=df['volume']).acc_dist_index()\n",
    "                logger.info(\"ADI calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating ADI: {e}\")\n",
    "            \n",
    "            # Force Index with error handling\n",
    "            try:\n",
    "                df['Force_Index'] = ForceIndexIndicator(close=df['close'], volume=df['volume'], window=13).force_index()\n",
    "                logger.info(\"Force Index calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating Force Index: {e}\")\n",
    "            \n",
    "            # Ease of Movement with error handling\n",
    "            try:\n",
    "                df['EMV'] = EaseOfMovementIndicator(high=df['high'], low=df['low'], volume=df['volume'], window=14).ease_of_movement()\n",
    "                logger.info(\"EMV calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating EMV: {e}\")\n",
    "            \n",
    "            # Volume Price Trend with error handling\n",
    "            try:\n",
    "                df['VPT'] = (df['volume'] * ((df['close'] - df['close'].shift()) / df['close'].shift())).cumsum()\n",
    "                df['PVT'] = (df['volume'] * (df['close'] - df['close'].shift()) / df['close'].shift()).cumsum()\n",
    "                logger.info(\"VPT and PVT calculated successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating VPT/PVT: {e}\")\n",
    "        else:\n",
    "            # Create dummy microstructure indicators when volume is not available\n",
    "            logger.warning(\"Creating dummy microstructure indicators (no volume data)\")\n",
    "            microstructure_cols = ['VWAP', 'VWAP_20', 'Volume_SMA', 'Volume_Ratio', 'OBV', 'CMF', 'MFI', 'ADI', 'Force_Index', 'EMV', 'VPT', 'PVT']\n",
    "            for col in microstructure_cols:\n",
    "                df[col] = 0.0\n",
    "        \n",
    "        # =============================================================================\n",
    "        # ADDITIONAL UTILITY INDICATORS\n",
    "        # =============================================================================\n",
    "        \n",
    "        # Daily Returns (useful for various calculations) with error handling\n",
    "        try:\n",
    "            df['Daily_Return'] = DailyReturnIndicator(close=df['close']).daily_return()\n",
    "            df['Cumulative_Return'] = CumulativeReturnIndicator(close=df['close']).cumulative_return()\n",
    "            logger.info(\"Return indicators calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating return indicators: {e}\")\n",
    "        \n",
    "        # True Range with error handling\n",
    "        try:\n",
    "            df['True_Range'] = pd.concat([\n",
    "                df['high'] - df['low'],\n",
    "                (df['high'] - df['close'].shift()).abs(),\n",
    "                (df['low'] - df['close'].shift()).abs()\n",
    "            ], axis=1).max(axis=1)\n",
    "            logger.info(\"True Range calculated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating True Range: {e}\")\n",
    "        \n",
    "        # Calculate total indicator count\n",
    "        indicator_columns = [col for col in df.columns if col not in ['symbol', 'open', 'high', 'low', 'close', 'volume', 'openinterest', 'BaseSymbol']]\n",
    "        total_indicators = len(indicator_columns)\n",
    "        \n",
    "        logger.info(f\"INDICATORS: Successfully calculated {total_indicators} indicators\")\n",
    "        logger.info(f\"INDICATORS: Direction: 20, Trend Strength: 15, Velocity: 12, Volatility: 13, Microstructure: 12+\")\n",
    "        \n",
    "        # Final data validation\n",
    "        if df.isnull().any().any():\n",
    "            null_counts = df.isnull().sum()\n",
    "            null_columns = null_counts[null_counts > 0]\n",
    "            logger.warning(f\"NULL values found in {len(null_columns)} columns: {null_columns.to_dict()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"INDICATORS: Critical error in indicator calculation: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION - UPDATE THIS SECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\">>> Multi-Dimensional Regime Classification System - Starting...\")\n",
    "print(\">>> Now with Rolling Windows & Forward-Looking Bias Protection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def run_enhanced_analysis_with_optimization(data_with_regimes, data_with_indicators, classifier):\n",
    "    \"\"\"Enhanced analysis that includes multi-objective optimization\"\"\"\n",
    "    \n",
    "    # STEP 4A: Initial Regime Analysis (Original)\n",
    "    print(\"\\nSTEP 4A: Generating Initial Multi-Dimensional Analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"\\nDEBUG: RUN_OPTIMIZATION = {RUN_OPTIMIZATION}\")\n",
    "    print(f\"DEBUG: OPTIMIZE_WINDOW = {OPTIMIZE_WINDOW}\")\n",
    "    print(f\"DEBUG: WALK_FORWARD = {WALK_FORWARD}\")\n",
    "    \n",
    "    # Check if function exists\n",
    "    try:\n",
    "        print(f\"DEBUG: optimize_window_size function exists: {'optimize_window_size' in globals()}\")\n",
    "        if 'optimize_window_size' not in globals():\n",
    "            from multi_objective_optimizer import optimize_window_size\n",
    "            print(\"DEBUG: Had to import optimize_window_size!\")\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error checking for function: {e}\")\n",
    "    \n",
    "    # Analyze each dimension\n",
    "    dimensions = ['Direction', 'TrendStrength', 'Velocity', 'Volatility', 'Microstructure']\n",
    "    \n",
    "    print(\"\\nINITIAL DIMENSIONAL DISTRIBUTION:\")\n",
    "    initial_distributions = {}\n",
    "    for dimension in dimensions:\n",
    "        col_name = f'{dimension}_Regime'\n",
    "        if col_name in data_with_regimes.columns:\n",
    "            regime_counts = data_with_regimes[col_name].value_counts()\n",
    "            initial_distributions[dimension] = regime_counts\n",
    "            print(f\"\\n{dimension}:\")\n",
    "            for regime, count in regime_counts.items():\n",
    "                pct = count / len(data_with_regimes) * 100\n",
    "                print(f\"  {regime}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Persistence analysis\n",
    "    print(\"\\nREGIME PERSISTENCE ANALYSIS:\")\n",
    "    for dimension in dimensions:\n",
    "        col_name = f'{dimension}_Regime'\n",
    "        if col_name in data_with_regimes.columns:\n",
    "            regime_changes = (data_with_regimes[col_name] != data_with_regimes[col_name].shift()).sum()\n",
    "            persistence = 1 - (regime_changes / len(data_with_regimes))\n",
    "            print(f\"  {dimension}: {persistence:.1%} persistence ({regime_changes} changes)\")\n",
    "    \n",
    "    # STEP 4B: Run Optimization if enabled\n",
    "    optimization_results = None\n",
    "    if RUN_OPTIMIZATION:\n",
    "        logger.info(\"\\nSTEP 4B: Optimization Phase\")\n",
    "        logger.info(\"-\" * 50)\n",
    "        logger.info(f\"DEBUG: About to run window optimization...\")\n",
    "        \n",
    "        # First, optimize window size\n",
    "        logger.info(\"4B.1: Window Size Optimization\")\n",
    "        \n",
    "        # First, optimize window size\n",
    "        if OPTIMIZE_WINDOW:  # ADD THIS LINE\n",
    "            print(\"\\n4B.1: Window Size Optimization\")\n",
    "            try:\n",
    "\n",
    "                from multi_objective_optimizer import optimize_window_size\n",
    "                \n",
    "                print(\"DEBUG: About to call optimize_window_size...\")\n",
    "                window_results = optimize_window_size(\n",
    "                    data_with_indicators,\n",
    "                    classifier,\n",
    "                    timeframe='15min',\n",
    "                    window_sizes_hours=[2, 4, 6, 8, 12, 16, 24, 36]\n",
    "                )\n",
    "                print(\"DEBUG: optimize_window_size completed!\")\n",
    "                print(f\"DEBUG: Results: {window_results}\")\n",
    "                \n",
    "                # Apply optimal window to classifier\n",
    "                optimal_window = window_results['optimal_window_periods']\n",
    "                print(f\"\\nApplying optimal window: {optimal_window} periods\")\n",
    "                classifier.rolling_window = optimal_window\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR in window optimization: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Now run walk-forward validation\n",
    "        if WALK_FORWARD:\n",
    "            print(\"\\n4B.2: Walk-Forward Validation\")\n",
    "            \n",
    "            # For 15-min data: ~20 days training, ~10 days testing\n",
    "            train_periods = 20 * 96  # 20 days * 96 periods per day\n",
    "            test_periods = 10 * 96   # 10 days * 96 periods per day\n",
    "            \n",
    "            wf_optimizer = WalkForwardOptimizer(\n",
    "                data_with_indicators,\n",
    "                train_periods=train_periods,\n",
    "                test_periods=test_periods\n",
    "            )\n",
    "            \n",
    "            wf_results = wf_optimizer.run_walk_forward_optimization(\n",
    "                classifier,\n",
    "                optimization_method='differential_evolution',\n",
    "                max_iterations=20  # Fewer iterations for walk-forward\n",
    "            )\n",
    "            \n",
    "            # Save walk-forward results\n",
    "            wf_results.to_csv(f'walk_forward_results_{timestamp}.csv', index=False)\n",
    "            print(f\"\\nWalk-forward results saved to: walk_forward_results_{timestamp}.csv\")\n",
    "            \n",
    "        else:\n",
    "            # Regular optimization (existing code)\n",
    "            print(\"\\n4B.2: Regular Optimization\")\n",
    "            optimization_results = run_regime_optimization(\n",
    "                classifier, \n",
    "                data_with_indicators,\n",
    "                max_iterations=OPTIMIZATION_ITERATIONS\n",
    "            )\n",
    "        \n",
    "        if optimization_results:\n",
    "            print(\"\\nOPTIMIZATION COMPLETE!\")\n",
    "            print_optimization_results(optimization_results)\n",
    "            \n",
    "            # Re-classify with optimized parameters\n",
    "            print(\"\\nRe-classifying with optimized parameters...\")\n",
    "            data_with_regimes = classifier.classify_multidimensional_regime(\n",
    "                data_with_indicators.copy(), \n",
    "                symbol=SYMBOLS[0]\n",
    "            )\n",
    "            \n",
    "            # Show improved distributions\n",
    "            print(\"\\nOPTIMIZED DIMENSIONAL DISTRIBUTIONS:\")\n",
    "            for dimension in dimensions:\n",
    "                col_name = f'{dimension}_Regime'\n",
    "                if col_name in data_with_regimes.columns:\n",
    "                    regime_counts = data_with_regimes[col_name].value_counts()\n",
    "                    print(f\"\\n{dimension}:\")\n",
    "                    for regime, count in regime_counts.items():\n",
    "                        pct = count / len(data_with_regimes) * 100\n",
    "                        \n",
    "                        # Compare to initial\n",
    "                        if dimension in initial_distributions and regime in initial_distributions[dimension]:\n",
    "                            initial_pct = initial_distributions[dimension][regime] / len(data_with_regimes) * 100\n",
    "                            change = pct - initial_pct\n",
    "                            print(f\"  {regime}: {count} ({pct:.1f}%) [Change: {change:+.1f}%]\")\n",
    "                        else:\n",
    "                            print(f\"  {regime}: {count} ({pct:.1f}%) [New]\")\n",
    "    \n",
    "    return data_with_regimes, optimization_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "        # STEP 1: Load Data\n",
    "        print(\"\\nSTEP 1: Loading Data\")\n",
    "        print(\"-\" * 50)\n",
    "        data = load_csv_data(CSV_FILES, SYMBOLS, START_DATE, END_DATE)\n",
    "        print(f\"SUCCESS: Loaded {len(data)} records\")\n",
    "        \n",
    "        # STEP 2: Calculate Indicators\n",
    "        print(\"\\nSTEP 2: Calculating 85 Indicators\")\n",
    "        print(\"-\" * 50)\n",
    "        data_with_indicators = calculate_all_indicators(data)\n",
    "        print(f\"SUCCESS: Calculated {len(data_with_indicators.columns)} total columns\")\n",
    "        \n",
    "        # STEP 3: Classify Regimes with Rolling Windows\n",
    "        print(\"\\nSTEP 3: Classifying Multi-Dimensional Regimes\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate window sizes in bars\n",
    "        bars_per_hour = 4  # 4 bars per hour for 15-minute data\n",
    "        rolling_bars = int(ROLLING_WINDOW_HOURS * bars_per_hour)  # 8 * 4 = 32 bars\n",
    "        min_bars = int(MIN_WINDOW_HOURS * bars_per_hour)          # 2 * 4 = 8 bars\n",
    "        \n",
    "        print(f\"Window Configuration for 15-minute data:\")\n",
    "        print(f\"  Rolling Window: {ROLLING_WINDOW_HOURS} hours ({rolling_bars} bars)\")\n",
    "        print(f\"  Minimum Window: {MIN_WINDOW_HOURS} hours ({min_bars} bars)\")\n",
    "        print(f\"  Smoothing Periods: {SMOOTHING_PERIODS}\")\n",
    "        \n",
    "        classifier = MultiDimensionalRegimeClassifier(\n",
    "            min_periods=min_bars,        # Now integer: 8\n",
    "            max_periods=rolling_bars,    # Now integer: 32\n",
    "            smoothing_periods=SMOOTHING_PERIODS\n",
    "        )\n",
    "        \n",
    "        data_with_regimes = classifier.classify_multidimensional_regime(\n",
    "            data_with_indicators.copy(),\n",
    "            symbol=SYMBOLS[0]\n",
    "        )\n",
    "        \n",
    "        print(f\"SUCCESS: Classified all 5 dimensions for {len(data_with_regimes)} periods\")\n",
    "        print(f\"Using {ROLLING_WINDOW_HOURS}-hour rolling windows with {SMOOTHING_PERIODS}-period smoothing\")\n",
    "        \n",
    "        # STEP 4: Enhanced Analysis with Optimization\n",
    "        data_with_regimes, optimization_results = run_enhanced_analysis_with_optimization(\n",
    "            data_with_regimes, data_with_indicators, classifier\n",
    "        )\n",
    "        \n",
    "        # STEP 5: Save Results\n",
    "        print(\"\\nSTEP 5: Saving Results\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Save main dataset\n",
    "        main_output = os.path.join(OUTPUT_DIR, f\"regime_analysis_rolling_{timestamp}.csv\")\n",
    "        data_with_regimes.to_csv(main_output)\n",
    "        print(f\"SUCCESS: Main analysis saved to {main_output}\")\n",
    "        \n",
    "        # Save optimization results if available\n",
    "        if optimization_results:\n",
    "            opt_output = os.path.join(OUTPUT_DIR, f\"optimization_results_{timestamp}.csv\")\n",
    "            \n",
    "            # Save optimization history\n",
    "            if optimization_results.optimization_history:\n",
    "                opt_history_df = pd.DataFrame(optimization_results.optimization_history)\n",
    "                opt_history_df.to_csv(opt_output, index=False)\n",
    "                print(f\"SUCCESS: Optimization history saved to {opt_output}\")\n",
    "            \n",
    "            # Save best parameters\n",
    "            params_output = os.path.join(OUTPUT_DIR, f\"best_parameters_{timestamp}.txt\")\n",
    "            with open(params_output, 'w') as f:\n",
    "                f.write(\"OPTIMIZED REGIME CLASSIFICATION PARAMETERS\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\\n\")\n",
    "                f.write(\"CONFIGURATION:\\n\")\n",
    "                f.write(f\"Rolling Window: {ROLLING_WINDOW_HOURS} hours\\n\")\n",
    "                f.write(f\"Min Window: {MIN_WINDOW_HOURS} hours\\n\")\n",
    "                f.write(f\"Smoothing Periods: {SMOOTHING_PERIODS}\\n\")\n",
    "                f.write(f\"Walk-Forward Validation: Enabled\\n\\n\")\n",
    "                f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "                f.write(f\"Final Score: {optimization_results.best_score:.4f}\\n\")\n",
    "                f.write(f\"Sharpe Ratio: {optimization_results.sharpe_ratio:.4f}\\n\")\n",
    "                f.write(f\"Max Drawdown: {optimization_results.max_drawdown:.2%}\\n\")\n",
    "                f.write(f\"Regime Persistence: {optimization_results.regime_persistence:.2%}\\n\\n\")\n",
    "                f.write(\"BEST PARAMETERS:\\n\")\n",
    "                for param, value in optimization_results.best_params.items():\n",
    "                    f.write(f\"{param}: {value:.6f}\\n\")\n",
    "            \n",
    "            print(f\"SUCCESS: Best parameters saved to {params_output}\")\n",
    "        \n",
    "        # Most common composite regimes\n",
    "        print(\"\\nMOST COMMON COMPOSITE REGIMES:\")\n",
    "        composite_counts = data_with_regimes['Composite_Regime'].value_counts().head(10)\n",
    "        for regime, count in composite_counts.items():\n",
    "            pct = count / len(data_with_regimes) * 100\n",
    "            conf = data_with_regimes[data_with_regimes['Composite_Regime'] == regime]['Composite_Confidence'].mean()\n",
    "            print(f\"  {regime}: {count} periods ({pct:.1f}%) - Confidence: {conf:.3f}\")\n",
    "        \n",
    "        # FINAL SUMMARY\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MULTI-DIMENSIONAL REGIME CLASSIFICATION SYSTEM - COMPLETE!\")\n",
    "        print(\"Forward-Looking Bias: ELIMINATED\")\n",
    "        print(\"Rolling Windows: ACTIVE\")\n",
    "        print(\"Regime Smoothing: ENABLED\")\n",
    "        print(\"Walk-Forward Validation: COMPLETED\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in main execution: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
